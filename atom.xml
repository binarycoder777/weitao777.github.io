<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-06-17T12:43:17.962Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spring编程式事务</title>
    <link href="http://example.com/2022/06/17/Spring%E7%BC%96%E7%A8%8B%E5%BC%8F%E4%BA%8B%E5%8A%A1/"/>
    <id>http://example.com/2022/06/17/Spring%E7%BC%96%E7%A8%8B%E5%BC%8F%E4%BA%8B%E5%8A%A1/</id>
    <published>2022-06-17T08:55:00.000Z</published>
    <updated>2022-06-17T12:43:17.962Z</updated>
    
    <content type="html"><![CDATA[<p>描述：今天在做项目实现活动领取功能模块时，考虑到在系统中，用户的领取次数表和领取活动表的后续开销问题，在设计时通过自研的分库分表db-router-Spring-boot-starter组件对该功能涉及的两张表进行了分库操作。即让不同uid的用户产生的领取活动信息和活动次数信息打到不同的数据库上，减轻数据库的压力。但在后续开发时，发现涉及到两张表的修改，而这里又用到了分库操作，对数据源进行了切换。导致常用的@trancation注解在这里并不适用。为此，去温习了一下编程式事务，用于解决此处的问题。在这里记录一下。</p><p>编程式事务主要有2种用法</p><ul><li>方式1：通过PlatformTransactionManager控制事务</li><li>方式2：通过TransactionTemplate控制事务</li></ul><h3 id="方式1：PlatformTransactionManager"><a href="#方式1：PlatformTransactionManager" class="headerlink" title="方式1：PlatformTransactionManager"></a>方式1：PlatformTransactionManager</h3><ol><li>定义事务管理器PlatformTransactionManager<br>（事务管理器相当于一个管理员，这个管理员就是用来帮你控制事务的，比如开启事务，提交事务，回滚事务等等。spring中使用PlatformTransactionManager这个接口来表示事务管理器，PlatformTransactionManager多个实现类，用来应对不同的环境<br>）</li></ol><ul><li><p>JpaTransactionManager：如果你用jpa来操作db，那么需要用这个管理器来帮你控制事务。</p></li><li><p>DataSourceTransactionManager：如果你用是指定数据源的方式，比如操作数据库用的是：JdbcTemplate、mybatis、ibatis，那么需要用这个管理器来帮你控制事务。</p></li><li><p>HibernateTransactionManager：如果你用hibernate来操作db，那么需要用这个管理器来帮你控制事务。</p></li><li><p>JtaTransactionManager：如果你用的是java中的jta来操作db，这种通常是分布式事务，此时需要用这种管理器来控制事务。<br><img src="/images/pasted-234.png" alt="upload successful"></p></li></ul><ol start="2"><li><p>定义事务属性TransactionDefinition：定义事务属性，比如事务隔离级别、事务超时时间、事务传播方式、是否是只读事务等等。spring中使用TransactionDefinition接口来表示事务的定义信息，有个子类比较常用：DefaultTransactionDefinition。</p></li><li><p>开启事务：调用事务管理器的getTransaction方法，即可以开启一个事务，这个方法会返回一个TransactionStatus表示事务状态的一个对象，通过TransactionStatus提供的一些方法可以用来控制事务的一些状态，比如事务最终是需要回滚还是需要提交。（执行了getTransaction后，spring内部会执行一些操作。将数据源datasource和connection映射起来放在了ThreadLocal中。通过resources这个ThreadLocal获取datasource其对应的connection对象）</p></li><li><p>执行业务操作：用同一个dataSource，而事务管理器开启事务的时候，会创建一个连接，将datasource和connection映射之后丢在了ThreadLocal中，而JdbcTemplate内部执行db操作的时候，也需要获取连接，JdbcTemplate会以自己内部的datasource去上面的threadlocal中找有没有关联的连接，如果有直接拿来用，若没找到将重新创建一个连接，而此时是可以找到的，那么JdbcTemplate就参与到spring的事务中了。</p></li><li><p>提交或回滚</p></li></ol><p>分析：</p><p>TransactionTemplate，主要有2个方法：</p><p>executeWithoutResult：无返回值场景</p><p>executeWithoutResult(Consumer<TransactionStatus> action)：没有返回值的，需传递一个Consumer对象，在accept方法中做业务操作</p><p>execute：有返回值场景</p><p><T> T execute(TransactionCallback<T> action)：有返回值的，需要传递一个TransactionCallback对象，在doInTransaction方法中做业务操作</p><p>  通过上面2个方法，事务管理器会自动提交事务或者回滚事务。</p><p>什么时候事务会回滚，有2种方式</p><p>方式1</p><p>在execute或者executeWithoutResult内部执行transactionStatus.setRollbackOnly();将事务状态标注为回滚状态，spring会自动让事务回滚</p><p>方式2</p><p>execute方法或者executeWithoutResult方法内部抛出任意异常即可回滚。</p><p>总结：平时我们用的最多的是声明式事务，声明式事务的底层还是使用上面这种方式来控制事务的，只不过对其进行了封装，让我们用起来更容易些。了解不同的方法有助于我们在不同的场景的应用。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;描述：今天在做项目实现活动领取功能模块时，考虑到在系统中，用户的领取次数表和领取活动表的后续开销问题，在设计时通过自研的分库分表db-router-Spring-boot-starter组件对该功能涉及的两张表进行了分库操作。即让不同uid的用户产生的领取活动信息和活动次数</summary>
      
    
    
    
    <category term="Spring" scheme="http://example.com/categories/Spring/"/>
    
    
    <category term="事务" scheme="http://example.com/tags/%E4%BA%8B%E5%8A%A1/"/>
    
    <category term="Spring" scheme="http://example.com/tags/Spring/"/>
    
  </entry>
  
  <entry>
    <title>误删数据库后除了跑路，还能怎么办？</title>
    <link href="http://example.com/2022/06/15/%E8%AF%AF%E5%88%A0%E6%95%B0%E6%8D%AE%E5%BA%93%E5%90%8E%E9%99%A4%E4%BA%86%E8%B7%91%E8%B7%AF%EF%BC%8C%E8%BF%98%E8%83%BD%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9F/"/>
    <id>http://example.com/2022/06/15/%E8%AF%AF%E5%88%A0%E6%95%B0%E6%8D%AE%E5%BA%93%E5%90%8E%E9%99%A4%E4%BA%86%E8%B7%91%E8%B7%AF%EF%BC%8C%E8%BF%98%E8%83%BD%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9F/</id>
    <published>2022-06-15T14:23:00.000Z</published>
    <updated>2022-06-15T15:43:59.602Z</updated>
    
    <content type="html"><![CDATA[<p>传统的高可用架构是不能预防误删除数据的，因为主库的一个drop table命令，会通过binlog传给所有从库和级联从库，进而导致整个集群的实例都会执行这个命令。</p><p>为了找到解决误删数据库的更高效的方法，我们需要先对和MySQL相关的误删数据，做下分类：</p><ol><li>使用delete语句误删数据行</li><li>使用drop table或者truncate table语句误删数据库</li><li>使用drop database语句误删数据库</li><li>使用rm命令误删整个MySQL实例</li></ol><h3 id="误删行"><a href="#误删行" class="headerlink" title="误删行"></a>误删行</h3><p>方案：使用delete语句误删了数据行，可以用Flashback工具通过闪回把数据恢复回来。</p><p>原理：修改binlog的内容，拿回原库重放。而能够使用这个方案的前提是，需要确保binlog_format=row和binlog_row_image=FULL。</p><p>具体：</p><ol><li>对于insert语句，对应的binlog event类型是Write_rows event，把它改成Delete_rows event即可。</li><li>同理，对于delete语句，也是将Delete_rows event改为Write_rows event。</li><li>而如果是Update_rows的话,binlog里面记录了数据行修改前和修改后的值,对调这两行的位置即可。</li></ol><p>而对于误删数据涉及到了多个事务的话，需要将事务的顺序调过来再执行。</p><p>当然恢复过程并不建议在主库上进行，而是恢复出一个备份，或者找一个从库作为临时库，在临时库上执行恢复操作，确认过数据无误后，在恢复回主库。（原因：一个在执行线上逻辑的主库，数据状态往往是有关联的。可能由于发现数据问题的实际晚了一点，就导致已经在之前误删的基础上，业务代码逻辑又继续修改了其他数据，如果这时进行数据恢复，未经确认，会导致出现对数据的二次破坏）</p><pre><code>    注意：不止要了解误删数据的事后处理方法，更重要的是做到事前预防。    建议：    1. 把sql_safe_updates参数设置成on，在delete或者update语句中写where，或者where条件不包含索引字段，这条语句就会报错。    2. 代码上线前，必须经过SQL审计</code></pre><h3 id="误删库-表"><a href="#误删库-表" class="headerlink" title="误删库/表"></a>误删库/表</h3><p>这种情况下，想要恢复数据就需要使用全量备份了，加增量日志的方式。这个方案要求线上定期的全量备份，并且实时备份binlog（因为对于drop/turncate table/database binlog记录的是statement格式，没法通过flasback进行恢复）</p><p>恢复数据的流程：</p><ol><li>取最近一次全量备份（假设是一天一备，上次备份是当天0点）</li><li>用备份恢复出一个临时库</li><li>从日志备份里面，取出凌晨0点之后的日志</li><li>把这些日志，除了误删除数据的语句外，全部应用到临时库</li></ol><p>说明：为了加速数据恢复，可以在mysqlbinlog加上-database，用来指定误删除表所在的库，但是这样还是不够快（1.误删表，不能指定解析一个表的日志；2.用mysqlbinlog解析出日志应用，应用日志的过程就只能是单线程）</p><p>误删库或表后，恢复数据的思路主要就是通过备份，再加上应用binlog的方式。最好把数据恢复功能做成自动化工具。</p><h3 id="延迟复制备库"><a href="#延迟复制备库" class="headerlink" title="延迟复制备库"></a>延迟复制备库</h3><p>延迟复制备库是一种特殊的备库，通过CHANGE MASTER TO_DELAY = N命令，可以指定这个备库持续保持主库有N秒的延迟。</p><h3 id="预防误删库表的方法"><a href="#预防误删库表的方法" class="headerlink" title="预防误删库表的方法"></a>预防误删库表的方法</h3><ul><li>账号分离，权限控制</li><li>制定操作规范</li><li>定期给开发进行培训</li><li>搭建延迟备库</li><li>做好sql审计</li><li>做好备份</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;传统的高可用架构是不能预防误删除数据的，因为主库的一个drop table命令，会通过binlog传给所有从库和级联从库，进而导致整个集群的实例都会执行这个命令。&lt;/p&gt;
&lt;p&gt;为了找到解决误删数据库的更高效的方法，我们需要先对和MySQL相关的误删数据，做下分类：&lt;/p&gt;</summary>
      
    
    
    
    <category term="MySQL" scheme="http://example.com/categories/MySQL/"/>
    
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>关于ZAB协议</title>
    <link href="http://example.com/2022/05/25/%E5%85%B3%E4%BA%8EZAB%E5%8D%8F%E8%AE%AE/"/>
    <id>http://example.com/2022/05/25/%E5%85%B3%E4%BA%8EZAB%E5%8D%8F%E8%AE%AE/</id>
    <published>2022-05-25T14:02:00.000Z</published>
    <updated>2022-05-25T14:19:02.540Z</updated>
    
    <content type="html"><![CDATA[<p>Zab 借鉴了 Paxos 算法，是特别为 Zookeeper 设计的支持崩溃恢复的原子广播协议。基于该协议，Zookeeper 设计为只有一台客户端（Leader）负责处理外部的写事务请求，然后Leader 客户端将数据同步到其他 Follower 节点。Zookeeper 只有一个 Leader 可以发起提案</p><p>Zab协议包括两种基本的模式：消息广播、崩溃恢复。</p><h3 id="消息广播"><a href="#消息广播" class="headerlink" title="消息广播"></a>消息广播</h3><p>ZAB协议针对事务请求的处理过程类似于一个两阶段提交过程</p><p>（1）广播事务阶段</p><p>（2）广播提交操作<br>这两阶段提交模型如下，有可能因为Leader宕机带来数据不一致，比如</p><p>（1） Leader 发 起 一 个 事 务Proposal1 后 就 宕 机 ， Follower 都 没有Proposal1 </p><p>（2）Leader收到半数ACK宕 机，没来得及向Follower发送Commit<br>怎么解决呢？ZAB引入了崩溃恢复模式。</p><p> <img src="/images/pasted-231.png" alt="upload successful"></p><p>（1）客户端发起一个写操作请求。 </p><p>（2）Leader服务器将客户端的请求转化为事务Proposal 提案，同时为每个Proposal 分配一个全局的ID，即zxid。 </p><p>（3）Leader服务器为每个Follower服务器分配一个单独的队列，然后将需要广播的 Proposal依次放到队列中去，并且根据FIFO策略进行消息发送。 </p><p>（4）Follower接收到Proposal后，会首先将其以事务日志的方式写入本地磁盘中，写入成功后向Leader反馈一个Ack响应消息。 </p><p>（5）Leader接收到超过半数以上Follower的Ack响应消息后，即认为消息发送成功，可以发送commit消息。 </p><p>（6）Leader向所有Follower广播commit消息，同时自身也会完成事务提交。Follower 接收到commit消息后，会将上一条事务提交。 </p><p>（7）Zookeeper采用Zab协议的核心，就是只要有一台服务器提交了Proposal，就要确保所有的服务器最终都能正确提交Proposal。</p><h3 id="崩溃恢复——异常假设"><a href="#崩溃恢复——异常假设" class="headerlink" title="崩溃恢复——异常假设"></a>崩溃恢复——异常假设</h3><p> 一旦Leader服务器出现崩溃或者由于网络原因导致Leader服务器失去了与过半 Follower的联系，那么就会进入崩溃恢复模式。</p><p> <img src="/images/pasted-232.png" alt="upload successful"></p><p> 假设两种服务器异常情况</p><ol><li>假设一个事务在Leader提出之后，Leader挂了</li><li>一个事务在Leader上提交了，并且过半的Follower都响应ACK了，但是Leader在Commit消息发出之前挂了。</li></ol><p> Zab协议崩溃恢复要满足以下两个要求</p><p> 1、 确保已经被Leader提交的提案Proposal，必须最终被所有的Follower服务器提交。 （已经产生的提案，Follower必须执行） </p><p> 2、 确保丢弃已经被Leader提出的，但是没有被提交的Proposal。（丢弃胎死腹中的提案）</p><p> 崩溃恢复主要包括两部分：Leader选举和数据恢复。</p><h3 id="崩溃恢复—Leader选举"><a href="#崩溃恢复—Leader选举" class="headerlink" title="崩溃恢复—Leader选举"></a>崩溃恢复—Leader选举</h3><p> <img src="/images/pasted-233.png" alt="upload successful"></p><p>Leader选举：根据上述要求，Zab协议需要保证选举出来的Leader需要满足以下条件：</p><p>（1）新选举出来的Leader不能包含未提交的Proposal。即新Leader必须都是已经提交了Proposal的Follower服务器节点。</p><p>（2）新选举的Leader节点中含有最大的zxid。这样做的好处是可以避免Leader服务器检查Proposal的提交和丢弃工作。</p><h3 id="崩溃恢复——数据恢复"><a href="#崩溃恢复——数据恢复" class="headerlink" title="崩溃恢复——数据恢复"></a>崩溃恢复——数据恢复</h3><p>Zab如何数据同步： </p><p>（1）完成Leader选举后，在正式开始工作之前（接收事务请求，然后提出新的Proposal），Leader服务器会首先确认事务日志中的所有的Proposal 是否已经被集群中过半的服务器Commit。</p><p>（2）Leader服务器需要确保所有的Follower服务器能够接收到每一条事务的Proposal，并且能将所有已经提交的事务Proposal应用到内存数据中。等到Follower将所有尚未同步的事务Proposal都从Leader服务器上同步过，并且应用到内存数据中以后，<br>Leader才会把该Follower加入到真正可用的Follower列表中。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Zab 借鉴了 Paxos 算法，是特别为 Zookeeper 设计的支持崩溃恢复的原子广播协议。基于该协议，Zookeeper 设计为只有一台客户端（Leader）负责处理外部的写事务请求，然后Leader 客户端将数据同步到其他 Follower 节点。Zookeepe</summary>
      
    
    
    
    <category term="分布式" scheme="http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    
    <category term="ZAB协议" scheme="http://example.com/tags/ZAB%E5%8D%8F%E8%AE%AE/"/>
    
    <category term="数据一致性" scheme="http://example.com/tags/%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7/"/>
    
  </entry>
  
  <entry>
    <title>关于Paxos算法</title>
    <link href="http://example.com/2022/05/25/%E5%85%B3%E4%BA%8EPaxos%E7%AE%97%E6%B3%95/"/>
    <id>http://example.com/2022/05/25/%E5%85%B3%E4%BA%8EPaxos%E7%AE%97%E6%B3%95/</id>
    <published>2022-05-25T13:38:00.000Z</published>
    <updated>2022-05-25T13:59:20.676Z</updated>
    
    <content type="html"><![CDATA[<p>Paxos算法是一种基于消息传递且具有高度容错特性的一致性算法。其通常用于快速正确的在一个分布式系统中对某个数据值达成一致，并且保证无论发送任何异常都不会破坏整个系统的一致性。</p><p> <img src="/images/pasted-226.png" alt="upload successful"></p><p> 在一个Paxos系统中，首先将所有节点划分为Proposer（提议者）、Acceptor（接受者）和Learner（学习者）</p><pre><code>     注意：每个节点可以身兼数职    </code></pre><p>一个完整的Paxos算法流程分为三个阶段</p><ol><li><p>Prepare准备阶段</p><ol><li>Proposer向多个Acceptor发出Propose请求Promise（承诺）</li><li>Acceptor针对收到的Propose请求进行Promise（承诺）</li></ol></li><li><p>Accept接受阶段</p><ol><li>Proposer收到多数Acceptor承诺的Promise后，向Acceptor发出Propose请求</li><li>Acceptor针对收到的Propose请求进行Accept处理</li></ol></li><li><p>Learn学习阶段</p><ol><li>Proposer将形成的决议发送给所有Learners</li></ol></li></ol><p> <img src="/images/pasted-227.png" alt="upload successful"></p><p> 1、 Prepare: Proposer生成全局唯一且递增的Proposal ID，向所有Acceptor发送Propose请求，这里无需携带提案内容，只携带Proposal ID即可。</p><p> 2、 Promise: Acceptor收到Propose请求后，做出“两个承诺，一个应答”。<br>    - 不再接受Proposal ID小于等于（注意：这里是&lt;= ）当前请求的Propose请求。<br>    - 不再接受Proposal ID小于（注意：这里是&lt; ）当前请求的Accept请求。<br>    - 不违背以前做出的承诺下，回复已经Accept过的提案中Proposal ID最大的那个提案的Value和Proposal ID，没有则返回空值。</p><p>3、 Propose: Proposer收到多数Acceptor的Promise应答后，从应答中选择Proposal ID最大的提案的Value，作为本次要发起的提案。如果所有应答的提案Value均为空值，则可以自己随意决定提案Value。然后携带当前Proposal ID，向所有Acceptor发送Propose请求。</p><p>4、 Accept: Acceptor收到Propose请求后，在不违背自己之前做出的承诺下，接受并持久化当前Proposal ID和提案Value。</p><p>5、 Learn: Proposer收到多数Acceptor的Accept后，决议形成，将形成的决议发送给所有Learner。</p><p>下面举例以说明：</p><ol><li>情况1：</li></ol><p> <img src="/images/pasted-228.png" alt="upload successful"></p><ul><li>有A1, A2, A3, A4, A5 5位议员，就税率问题进行决议。</li><li>A1发起1号Proposal的Propose，等待Promise承诺； </li><li>A2-A5回应Promise； </li><li>A1在收到两份回复时就会发起税率10%的Proposal； </li><li>A2-A5回应Accept； </li><li>通过Proposal，税率10%。</li></ul><ol start="2"><li>情况2：</li></ol><p> <img src="/images/pasted-229.png" alt="upload successful"><br>     - 现在我们假设在A1提出提案的同时, A5决定将税率定为20%<br>     - A2承诺A1，A4承诺A5，A3行为成为关键<br>     - 情况1：A3先收到A1消息，承诺A1。<br>     - A1发起Proposal（1，10%），A2，A3接受。<br>     - 之后A3又收到A5消息，回复A1：（1，10%），并承诺A5<br>     - A5发起Proposal（2，20%），A3，A4接受。之后A1，A5同时广播决议。</p><pre><code>    由此可见Paxos 算法缺陷：在网络复杂的情况下，一个应用 Paxos 算法的分布式系统，可能很久无法收敛，甚至陷入活锁的情况。</code></pre><ol start="3"><li>情况3：</li></ol><p> <img src="/images/pasted-230.png" alt="upload successful"></p><pre><code> - 现在我们假设在A1提出提案的同时, A5决定将税率定为20% - A1，A5同时发起Propose（序号分别为1，2） - A2承诺A1，A4承诺A5，A3行为成为关键 - 情况2：A3先收到A1消息，承诺A1。之后立刻收到A5消息，承诺A5。 - A1发起Proposal（1，10%），无足够响应，A1重新Propose （序号3），A3再次承诺A1。 - A5发起Proposal（2，20%），无足够相应。A5重新Propose （序号4），A3再次承诺A5。</code></pre><p> 造成这种情况的原因是系统中有一个以上的 Proposer，多个 Proposers 相互争夺 Acceptor，造成迟迟无法达成一致的情况。针对这种情况，一种改进的 Paxos 算法被提出：从系统中选出一个节点作为 Leader，只有 Leader 能够发起提案。这样，一次 Paxos 流程中只有一个Proposer，不会出现活锁的情况，此时只会出现例子中第一种情况。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Paxos算法是一种基于消息传递且具有高度容错特性的一致性算法。其通常用于快速正确的在一个分布式系统中对某个数据值达成一致，并且保证无论发送任何异常都不会破坏整个系统的一致性。&lt;/p&gt;
&lt;p&gt; &lt;img src=&quot;/images/pasted-226.png&quot; alt=&quot;u</summary>
      
    
    
    
    <category term="分布式" scheme="http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    
    <category term="Paxos" scheme="http://example.com/tags/Paxos/"/>
    
    <category term="一致性算法" scheme="http://example.com/tags/%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper选举机制</title>
    <link href="http://example.com/2022/05/25/Zookeeper%E9%80%89%E4%B8%BE%E6%9C%BA%E5%88%B6/"/>
    <id>http://example.com/2022/05/25/Zookeeper%E9%80%89%E4%B8%BE%E6%9C%BA%E5%88%B6/</id>
    <published>2022-05-25T12:24:00.000Z</published>
    <updated>2022-05-25T12:40:27.471Z</updated>
    
    <content type="html"><![CDATA[<p>在了解Zookeeper的选举机制之前，首先需要了解</p><ul><li>SID(服务器ID，用于唯一标识一台Zookeeper集群中的机器，每台机器不能重复，和myid一致)</li><li>ZXID（事务ID。用来标识一次服务器状态的变更。在某一时刻，集群中的每台机器的ZXID值不一定完全一致，这和Zookeeper服务器对于客户端“更新请求”的处理逻辑有关）</li><li>Epoch(每个Leader任期的代号。没有Leader时同一轮投票过程中的逻辑时钟是相同的。每投完一次票，这个数据会增加)</li></ul><p>了解上述三个参数之后，后续Zookeeper的选举机制便和其有关。关于其选举机制可以分为两种情况。</p><p>第一种情况是第一次启动集群时，Leader的选举是根据其myid的大小进行选举的。</p><pre><code>    例如：目前有一个五台Zookeeper的集群。当启动id为1的Zookeeper时，它投自己一票。此时不满足Leader成立的条件。票数半数以上。因此进入looking状态。然后id为2的服务器启动，id1和id2的服务器都投自己一票，然后交换选票信息。此时服务器1发现服务器2的id比自己大，因此服务器1转投服务器2一票。服务器1只有0票，而服务器2有2票。但此时还是不满足半数以上，因此两者都进入looking状态。然后服务器3启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数，服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING；服务器4启动，发起一次选举。此时服务器1，2，3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为 1票。此时服务器4服从多数，更改选票信息为服务器3，并更改状态为FOLLOWING；服务器5启动，同4一样当小弟。    </code></pre><p>而第二种情况是非第一次启动，而是在集群运行中，Leader宕机了，需要重新选举Leader。</p><pre><code>    例如：假设ZooKeeper由5台服务器组成，SID分别为1、2、3、4、5，ZXID分别为8、8、8、7、7，并且此时SID为3的服务器是Leader。某一时刻，3和5服务器出现故障，因此开始进行Leader选举。</code></pre><p> <img src="/images/pasted-225.png" alt="upload successful"></p><p> 选举Leader规则： ①EPOCH大的直接胜出 ②EPOCH相同，事务id大的胜出 ③事务id相同，服务器id大的胜出</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在了解Zookeeper的选举机制之前，首先需要了解&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SID(服务器ID，用于唯一标识一台Zookeeper集群中的机器，每台机器不能重复，和myid一致)&lt;/li&gt;
&lt;li&gt;ZXID（事务ID。用来标识一次服务器状态的变更。在某一时刻，集群中的每</summary>
      
    
    
    
    <category term="Zookeeper" scheme="http://example.com/categories/Zookeeper/"/>
    
    
    <category term="Zookeeper" scheme="http://example.com/tags/Zookeeper/"/>
    
    <category term="选举机制" scheme="http://example.com/tags/%E9%80%89%E4%B8%BE%E6%9C%BA%E5%88%B6/"/>
    
  </entry>
  
  <entry>
    <title>为什么要使用泛型程序设计？</title>
    <link href="http://example.com/2022/05/15/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BD%BF%E7%94%A8%E6%B3%9B%E5%9E%8B%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%EF%BC%9F/"/>
    <id>http://example.com/2022/05/15/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BD%BF%E7%94%A8%E6%B3%9B%E5%9E%8B%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%EF%BC%9F/</id>
    <published>2022-05-15T13:49:00.000Z</published>
    <updated>2022-05-18T11:13:31.973Z</updated>
    
    <content type="html"><![CDATA[<p>最近一直忙于编译原理的学习，难得空闲下来，重新巩固了一下泛型的知识。在此小记一下。</p><p>泛型程序设计意味着编写的代码可以被很多不同类型的对象重用</p><h2 id="泛型类"><a href="#泛型类" class="headerlink" title="泛型类"></a>泛型类</h2><p>在java中增加泛型类前，泛型程序设计时通过继承等方式实现的。例如：ArrayList维护一个Object数组的引用。 这种方式存在着两个问题：</p><ol><li>当获取一个值时，必须进行强制类型转换。</li><li>没有进行任何检测，可以向其中添加任何类的对象。</li></ol><p>因此，对于调用，编译和运行都不会出错。但在某些情况下进行了错误的强制类型转换使用。就会报错。</p><p>对此，泛型提供了更好的解决方案：类型参数。这不仅使得代码更具可读性，也使得代码更加安全。因为编译器可以根据这个信息推断出get时的类型，不需要进行强制类型转换。在编译期间检查出类型错误，而不是在运行时才检测出。</p><p>一个泛型类就是具有一个或多个类型变量的类。（使用大写形式，且比较短。在java中，E表示集合类型，K和V则是键值对，T表示任意类型）</p><h2 id="泛型方法"><a href="#泛型方法" class="headerlink" title="泛型方法"></a>泛型方法</h2><p>泛型方法的类型变量放在修饰符后，返回值前面。泛型方法可以定义在普通类中，也可以定义在泛型类中。</p><p>当调用一个泛型方法时，在方法名前的尖括号中放入具体的类型。当然在大多数情况下，编译器可以推导出类型，意味着我们可以省去。（在某些情况下，编译器无法推导出，此时需要指明）</p><h2 id="类型变量的限定"><a href="#类型变量的限定" class="headerlink" title="类型变量的限定"></a>类型变量的限定</h2><p>有时候类或方法需要对类型变量加以约束，&lt; T extends Object &gt; T</p><p>当做出这样的限定后，泛型的变量类型便被约束了。当然一个类型变量或通配符可以有多个限定，只需要用&amp;隔开即可。&lt; T extends Object1 &amp; Object2 &gt; T<br>值得注意的是，在java中可以根据需要拥有多个接口超类型，但限定中至多有一个类。如果用一个类作为限定，它必须位于限定列表的第一个</p><p>extends决定了泛型变量的上限。</p><h2 id="泛型代码和虚拟机"><a href="#泛型代码和虚拟机" class="headerlink" title="泛型代码和虚拟机"></a>泛型代码和虚拟机</h2><p>虚拟机没有泛型类对象，所有对象都是属于普通对象。</p><ul><li><p>类型擦除：无论何时定义一个泛型类型，都自动提供了一个相应的原始类型。原始类型的名字就是删去类型参数后的泛型类型名。擦除类型变量，并替换为限定类型类型。（无限定的变量用Object）</p></li><li><p>翻译泛型表达式：当程序调用泛型方法时，如果擦除返回类型，编译器会插入强制类型转换。</p></li><li><p>翻译泛型方法：类型擦除也会出现在泛型方法中，只留下限定类型。但这可能会导致类型擦除和多态发生冲突。</p></li></ul><h2 id="约束与局限性"><a href="#约束与局限性" class="headerlink" title="约束与局限性"></a>约束与局限性</h2><ul><li><p>不能用基本数据类型实例化类型参数，即不能有&lt; double &gt;，但可以有&lt; Double &gt;,原因是类型擦除。</p></li><li><p>运行时类型查询只适用于原始类型，而不适用于泛型类型。当试图查询一个对象是否属于某个泛型类型时，倘若使用instanceof会得到一个编译器错误。同样的道理，getClass方法总是返回原始类型。</p></li><li><p>不能创建参数化类型的数组，例如：Pair&lt; String &gt;[] table = new Pair&lt; String &gt; [10];类型擦除之后，table类型时Pair[],，可以把它转化为Object[],数组会记住其元素类型，如果试图存其他类型，则会报错。不过对于泛型，这种机制会使之无效。不过仍会导致一个类型错误。因此，不能创建参数化类型的数组。当然可以声明通配类型的数组，然后进行类型转换。</p></li><li><p>不能构造泛型数组，因为数组本身也有类型，用来监控存在虚拟机中的数组。</p></li><li><p>泛型类的静态上下文中类型变量无效：不能在静态域或方法中使用类型变量。</p></li><li><p>不能抛出或捕获泛型类的实例：可以消除对受查异常的检查</p></li><li><p>注意擦除后的冲突：要想支持擦除的转换，就需要强制限制一个类或类型变量不能同时成为两个接口类型的子类，而这两个接口时同一接口的不同参数。</p></li></ul><h2 id="泛型类型的继承规则"><a href="#泛型类型的继承规则" class="headerlink" title="泛型类型的继承规则"></a>泛型类型的继承规则</h2><ul><li>无论S和T有什么关系，通常calssName &lt; S &gt; 和 calssName &lt; T &gt; 没有任何关系</li></ul><p>待续…</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近一直忙于编译原理的学习，难得空闲下来，重新巩固了一下泛型的知识。在此小记一下。&lt;/p&gt;
&lt;p&gt;泛型程序设计意味着编写的代码可以被很多不同类型的对象重用&lt;/p&gt;
&lt;h2 id=&quot;泛型类&quot;&gt;&lt;a href=&quot;#泛型类&quot; class=&quot;headerlink&quot; title=&quot;泛</summary>
      
    
    
    
    <category term="Java" scheme="http://example.com/categories/Java/"/>
    
    
    <category term="Java" scheme="http://example.com/tags/Java/"/>
    
    <category term="泛型" scheme="http://example.com/tags/%E6%B3%9B%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>关于lambda表达式</title>
    <link href="http://example.com/2022/05/02/%E5%85%B3%E4%BA%8Elambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    <id>http://example.com/2022/05/02/%E5%85%B3%E4%BA%8Elambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/</id>
    <published>2022-05-02T14:46:00.000Z</published>
    <updated>2022-05-02T15:19:37.907Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简单介绍"><a href="#简单介绍" class="headerlink" title="简单介绍"></a>简单介绍</h3><p>lambda表达式是一个可传递的代码块，可以在以后执行一次或多次。</p><p>表达形式：(参数)-&gt;{表达式}</p><ul><li><p>如果可以推导出lambda表达式的参数类型，则可以忽略</p></li><li><p>如果只有一个参数，且参数类型可以推断，则（）可以省略</p></li><li><p>无需指定其返回类型，lambda表达式的返回类型可以根据上下文推断得出</p><pre><code>  注意：如果一个lambda表达式在某些分支返回值，而在另一些分支不返还，这是错误的</code></pre></li></ul><h3 id="函数式接口"><a href="#函数式接口" class="headerlink" title="函数式接口"></a>函数式接口</h3><p>对于只有一个抽象方法的接口，需要这种接口的对象时，就可以通过提供一个lambda表达式。这种接口称为函数式接口。</p><p>java.util.function包中定义了不少非常通用的函数式接口。例如其中一个接口BiFunction&lt;T,U,R&gt;描述了参数类型为T和U且返回类型为R的函数。（比如可以把比较的lambda表达式保存在这个类型的变量中。当然，没多少人喜欢在sort的时候接收一个BiFunction。）</p><h3 id="方法引用"><a href="#方法引用" class="headerlink" title="方法引用"></a>方法引用</h3><p>对象或类型::方法名</p><ul><li>object::instanceMethod</li><li>Class::staticMethod</li><li>Class::instanceMethod</li></ul><p>前两种方法引用等价于提供方法参数的lambda表达式，而对于第三种，第一个参数会成为方法的目标</p><p>例子：Arrays.sort(strings,String::compareToIgnoreCase)不考虑字母的大小写进行排序</p><h3 id="构造器引用"><a href="#构造器引用" class="headerlink" title="构造器引用"></a>构造器引用</h3><p>构造器引用同方法引用类似，不过方法名为new。如：String:new</p><h3 id="变量的作用域"><a href="#变量的作用域" class="headerlink" title="变量的作用域"></a>变量的作用域</h3><p>lambda表达式的三个部分：</p><ol><li>代码块</li><li>参数</li><li>自由变量的值（非参数，且不在代码块中定义的变量）<pre><code> 注意：即对于自由变量，lambda表达式需要数据结构对其进行存储，而为了明确其捕获到的自由变量的值，lambda表达式中只能引用值不会改变的变量（常量） 补充：关于代码块和自由变量值有一个术语：闭包</code></pre>在lambda表达式中使用this关键字，值创建这个lambda表达式的方法的this参数<h3 id="处理lambda表达式"><a href="#处理lambda表达式" class="headerlink" title="处理lambda表达式"></a>处理lambda表达式</h3>使用lambda表达式的重点是延迟执行。而希望一个代码延迟执行的原因有很多：</li><li>在一个单独的线程中运行的代码</li><li>多次运行代码</li><li>在算法的适当位置运行代码（如排序比较）</li><li>发生某种事件时执行代码（如点击一个按钮之类）<br>…</li></ol><h3 id="常用函数式接口"><a href="#常用函数式接口" class="headerlink" title="常用函数式接口"></a>常用函数式接口</h3><p> <img src="/images/pasted-223.png" alt="upload successful"></p><h3 id="基本类型函数式接口"><a href="#基本类型函数式接口" class="headerlink" title="基本类型函数式接口"></a>基本类型函数式接口</h3><p><img src="/images/pasted-224.png" alt="upload successful"><br>可以使用这些来减少装箱拆箱</p><pre><code>    补充：如果自己设计接口，其中只有一个抽象方法，可以使用@Functionallnterface注解来标记这个接口（好处：一方面无意增加了另一个抽象方法编译器会提示报错，另一方面javadoc页会指出这是一个函数式接口）</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;简单介绍&quot;&gt;&lt;a href=&quot;#简单介绍&quot; class=&quot;headerlink&quot; title=&quot;简单介绍&quot;&gt;&lt;/a&gt;简单介绍&lt;/h3&gt;&lt;p&gt;lambda表达式是一个可传递的代码块，可以在以后执行一次或多次。&lt;/p&gt;
&lt;p&gt;表达形式：(参数)-&amp;gt;{表达式}&lt;/</summary>
      
    
    
    
    <category term="Java" scheme="http://example.com/categories/Java/"/>
    
    
    <category term="Java" scheme="http://example.com/tags/Java/"/>
    
    <category term="lambda表达式" scheme="http://example.com/tags/lambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>SLF4J那些事</title>
    <link href="http://example.com/2022/04/29/SLF4J%E9%82%A3%E4%BA%9B%E4%BA%8B/"/>
    <id>http://example.com/2022/04/29/SLF4J%E9%82%A3%E4%BA%9B%E4%BA%8B/</id>
    <published>2022-04-29T07:21:00.000Z</published>
    <updated>2022-04-29T08:49:21.775Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Java中的日志框架"><a href="#Java中的日志框架" class="headerlink" title="Java中的日志框架"></a>Java中的日志框架</h3><p> <img src="/images/pasted-216.png" alt="upload successful"></p><h3 id="门面日志框架"><a href="#门面日志框架" class="headerlink" title="门面日志框架"></a>门面日志框架</h3><p> <img src="/images/pasted-217.png" alt="upload successful"><br>不同的日志框架，有着不同的api和配置文件，为了统一应用中不同的日志框架所带来的统一管理问题，引入了门面日志框架，向上提供了统一的接口管理，向下对接不同的日志框架实现。<br> <img src="/images/pasted-218.png" alt="upload successful"></p><h3 id="具体流程"><a href="#具体流程" class="headerlink" title="具体流程"></a>具体流程</h3><p> <img src="/images/pasted-219.png" alt="upload successful"></p><h3 id="关于SL4J的适配"><a href="#关于SL4J的适配" class="headerlink" title="关于SL4J的适配"></a>关于SL4J的适配</h3><p> <img src="/images/pasted-220.png" alt="upload successful"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol><li>引入jar包slf4-api.jar</li><li>引入适配层jar包（如果需要的话）</li><li>引入底层日志框架的jar包</li><li>确认是否版本安全<h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><img src="/images/pasted-221.png" alt="upload successful"></li></ol><p> <img src="/images/pasted-222.png" alt="upload successful"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Java中的日志框架&quot;&gt;&lt;a href=&quot;#Java中的日志框架&quot; class=&quot;headerlink&quot; title=&quot;Java中的日志框架&quot;&gt;&lt;/a&gt;Java中的日志框架&lt;/h3&gt;&lt;p&gt; &lt;img src=&quot;/images/pasted-216.png&quot; alt</summary>
      
    
    
    
    <category term="Java" scheme="http://example.com/categories/Java/"/>
    
    
    <category term="Java" scheme="http://example.com/tags/Java/"/>
    
    <category term="日志" scheme="http://example.com/tags/%E6%97%A5%E5%BF%97/"/>
    
    <category term="SL4J" scheme="http://example.com/tags/SL4J/"/>
    
  </entry>
  
  <entry>
    <title>Java SPI机制</title>
    <link href="http://example.com/2022/04/29/Java-SPI%E6%9C%BA%E5%88%B6/"/>
    <id>http://example.com/2022/04/29/Java-SPI%E6%9C%BA%E5%88%B6/</id>
    <published>2022-04-29T01:06:00.000Z</published>
    <updated>2022-04-29T01:35:01.943Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>SPI（Serivce Provider Interface）：它是从java6开始引入的，一种基于classloader来发现并加载服务的机制。</p><p>一个标准的SPI有三个组件构成：<br>    - Service：是一个公开的接口或抽象类，定义了一个抽象的功能模块<br>    - Service Provider：service的一个实现类<br>    - ServiceLoader：核心组件，负责在运行时发现并加载service provider</p><h2 id="SPI运行流程"><a href="#SPI运行流程" class="headerlink" title="SPI运行流程"></a>SPI运行流程</h2><p> <img src="/images/pasted-211.png" alt="upload successful"></p><p> Application无需关注service的具体实现，只需面向接口编程</p><h2 id="Java-SPI在JDBC中的应用"><a href="#Java-SPI在JDBC中的应用" class="headerlink" title="Java SPI在JDBC中的应用"></a>Java SPI在JDBC中的应用</h2><p> <img src="/images/pasted-212.png" alt="upload successful"></p><ul><li>在Java SPI前，我们需要编码去注册驱动Class.forName(“com.mysql.jdbc.Driver”)</li><li>在引入Java SPI后，我们只需要日引入对应的依赖jar包即可</li></ul><h3 id="Java-SPI的三大规范要素"><a href="#Java-SPI的三大规范要素" class="headerlink" title="Java SPI的三大规范要素"></a>Java SPI的三大规范要素</h3><p> <img src="/images/pasted-214.png" alt="upload successful"></p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ul><li>作用：提供了一种组件发现和注册的方式，可以用于实现各种插件，或者灵活替换所使用的组件</li><li>优点：面向接口编程，优雅的实现模块之间的解耦</li><li>设计思想：面向接口+配置文件+反射技术</li><li>应用场景：JDBC、SLF4J等</li></ul><h3 id="补充：Java-SPI和SPringBoot自动装配"><a href="#补充：Java-SPI和SPringBoot自动装配" class="headerlink" title="补充：Java SPI和SPringBoot自动装配"></a>补充：Java SPI和SPringBoot自动装配</h3><p> <img src="/images/pasted-215.png" alt="upload successful"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h2&gt;&lt;p&gt;SPI（Serivce Provider Interface）：它是从java6开始引入的，一种基于classloader来</summary>
      
    
    
    
    <category term="Java" scheme="http://example.com/categories/Java/"/>
    
    
    <category term="Java" scheme="http://example.com/tags/Java/"/>
    
    <category term="SPI" scheme="http://example.com/tags/SPI/"/>
    
  </entry>
  
  <entry>
    <title>MySQL行转列，列转行</title>
    <link href="http://example.com/2022/04/26/MySQL%E8%A1%8C%E8%BD%AC%E5%88%97%EF%BC%8C%E5%88%97%E8%BD%AC%E8%A1%8C/"/>
    <id>http://example.com/2022/04/26/MySQL%E8%A1%8C%E8%BD%AC%E5%88%97%EF%BC%8C%E5%88%97%E8%BD%AC%E8%A1%8C/</id>
    <published>2022-04-26T12:09:00.000Z</published>
    <updated>2022-04-26T12:41:41.561Z</updated>
    
    <content type="html"><![CDATA[<h2 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h2><ul><li><p>建表</p><pre><code>CREATE TABLE tb_score(    id INT(11) NOT NULL auto_increment,    userid VARCHAR(20) NOT NULL COMMENT &#39;用户id&#39;,    subject VARCHAR(20) COMMENT &#39;科目&#39;,    score DOUBLE COMMENT &#39;成绩&#39;,    PRIMARY KEY(id))ENGINE = INNODB DEFAULT CHARSET = utf8;</code></pre></li><li><p>插入数据</p><pre><code>INSERT INTO tb_score(userid,subject,score) VALUES (&#39;001&#39;,&#39;语文&#39;,90);INSERT INTO tb_score(userid,subject,score) VALUES (&#39;001&#39;,&#39;数学&#39;,92);INSERT INTO tb_score(userid,subject,score) VALUES (&#39;001&#39;,&#39;英语&#39;,80);INSERT INTO tb_score(userid,subject,score) VALUES (&#39;002&#39;,&#39;语文&#39;,88);INSERT INTO tb_score(userid,subject,score) VALUES (&#39;002&#39;,&#39;数学&#39;,90);INSERT INTO tb_score(userid,subject,score) VALUES (&#39;002&#39;,&#39;英语&#39;,75.5);INSERT INTO tb_score(userid,subject,score) VALUES (&#39;003&#39;,&#39;语文&#39;,70);INSERT INTO tb_score(userid,subject,score) VALUES (&#39;003&#39;,&#39;数学&#39;,85);INSERT INTO tb_score(userid,subject,score) VALUES (&#39;003&#39;,&#39;英语&#39;,90);INSERT INTO tb_score(userid,subject,score) VALUES (&#39;003&#39;,&#39;政治&#39;,82);</code></pre></li></ul><p> 查询数据表中的内容（即转换前的结果）</p><pre><code>    select * from tb_score;</code></pre><p> <img src="/images/pasted-206.png" alt="upload successful"></p><ol><li>使用case…when….then 进行行转列<pre><code> SELECT userid,SUM(CASE `subject` WHEN &#39;语文&#39; THEN score ELSE 0 END) AS &#39;语文&#39;,SUM(CASE `subject` WHEN &#39;数学&#39; THEN score ELSE 0 END) AS &#39;数学&#39;,SUM(CASE `subject` WHEN &#39;英语&#39; THEN score ELSE 0 END) AS &#39;英语&#39;,SUM(CASE `subject` WHEN &#39;政治&#39; THEN score ELSE 0 END) AS &#39;政治&#39;FROM tb_scoreGROUP BY userid</code></pre></li></ol><p> <img src="/images/pasted-207.png" alt="upload successful"></p><ol start="2"><li>使用IF() 进行行转列：<pre><code>SELECT userid,SUM(IF(`subject`=&#39;语文&#39;,score,0)) AS &#39;语文&#39;,SUM(IF(`subject`=&#39;数学&#39;,score,0)) AS &#39;数学&#39;,SUM(IF(`subject`=&#39;英语&#39;,score,0)) AS &#39;英语&#39;,SUM(IF(`subject`=&#39;政治&#39;,score,0)) AS &#39;政治&#39;FROM tb_scoreGROUP BY userid</code></pre></li></ol><p> <img src="/images/pasted-208.png" alt="upload successful"></p><blockquote><p>注意点：</p></blockquote><blockquote><blockquote><p>（1）SUM() 是为了能够使用GROUP BY根据userid进行分组，因为每一个userid对应的subject=”语文”的记录只有一条，所以SUM() 的值就等于对应那一条记录的score的值。假如userid =’001’ and subject=’语文’ 的记录有两条，则此时SUM() 的值将会是这两条记录的和，同理，使用Max()的值将会是这两条记录里面值最大的一个。但是正常情况下，一个user对应一个subject只有一个分数，因此可以使用SUM()、MAX()、MIN()、AVG()等聚合函数都可以达到行转列的效果。</p></blockquote></blockquote><blockquote><blockquote><p>（2）IF(<code>subject</code>=’语文’,score,0) 作为条件，即对所有subject=’语文’的记录的score字段进行SUM()、MAX()、MIN()、AVG()操作，如果score没有值则默认为0。</p></blockquote></blockquote><ol start="3"><li>计算行列和<pre><code> SELECT IFNULL(userid,&#39;TOTAL&#39;) AS userid, SUM(IF(`subject`=&#39;语文&#39;,score,0)) AS 语文, SUM(IF(`subject`=&#39;数学&#39;,score,0)) AS 数学, SUM(IF(`subject`=&#39;英语&#39;,score,0)) AS 英语, SUM(IF(`subject`=&#39;政治&#39;,score,0)) AS 政治, SUM(score) AS TOTAL  FROM tb_score GROUP BY userid WITH ROLLUP;</code></pre></li></ol><p> <img src="/images/pasted-209.png" alt="upload successful"></p><ol start="4"><li><p>合并字段显示：利用group_concat()</p><pre><code>  SELECT userid,GROUP_CONCAT(`subject`,&quot;:&quot;,score)AS 成绩 FROM tb_score  GROUP BY userid</code></pre></li></ol><p> <img src="/images/pasted-210.png" alt="upload successful"></p><h3 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h3><p>将每个userid对应的多个科目的成绩查出来，通过UNION ALL将结果集加起来</p><pre><code>附：UNION与UNION ALL的区别（摘）：1.对重复结果的处理：UNION会去掉重复记录，UNION ALL不会；2.对排序的处理：UNION会排序，UNION ALL只是简单地将两个结果集合并；3.效率方面的区别：因为UNION 会做去重和排序处理，因此效率比UNION ALL慢很多；</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;行转列&quot;&gt;&lt;a href=&quot;#行转列&quot; class=&quot;headerlink&quot; title=&quot;行转列&quot;&gt;&lt;/a&gt;行转列&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;建表&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;CREATE TABLE tb_score(
    id INT(11) N</summary>
      
    
    
    
    <category term="MySQL" scheme="http://example.com/categories/MySQL/"/>
    
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
    <category term="查询" scheme="http://example.com/tags/%E6%9F%A5%E8%AF%A2/"/>
    
    <category term="行列转换" scheme="http://example.com/tags/%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2/"/>
    
  </entry>
  
  <entry>
    <title>记一次安装Zookeeper启动失败的坑</title>
    <link href="http://example.com/2022/04/21/%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%AE%89%E8%A3%85Zookeeper%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5%E7%9A%84%E5%9D%91/"/>
    <id>http://example.com/2022/04/21/%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%AE%89%E8%A3%85Zookeeper%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5%E7%9A%84%E5%9D%91/</id>
    <published>2022-04-21T13:36:00.000Z</published>
    <updated>2022-04-21T13:43:12.434Z</updated>
    
    <content type="html"><![CDATA[<p>今天在阿里云上安装ZooKeeper，然后启动时一直报: Starting zookeeper … FAILED TO START。折腾了一会，经查阅资料发现，原来和版本有莫大关系…</p><h3 id="1-下载的版本问题（-gt-3-5-5）"><a href="#1-下载的版本问题（-gt-3-5-5）" class="headerlink" title="1. 下载的版本问题（&gt;= 3.5.5）"></a>1. 下载的版本问题（&gt;= 3.5.5）</h3><p>实际上只要 &gt;= 3.5.5 版本都会出现这种问题。</p><p>问题原因：下载了错误的版本文件，Zookeeper 从3.5.5后开始拆分为两个版本，而且他们的结构还很类似。</p><ul><li>标准版本（Apache ZooKeeper x.y.z ），下载的文件名为：apache-zookeeper-x.y.z-bin.tar.gz</li><li>另一个是源码版本（Apache ZooKeeper x.y.z Source Release），下载的文件名为：apache-zookeeper-x.y.z.tar.gz</li></ul><p> <img src="/images/pasted-204.png" alt="upload successful"></p><p> <img src="/images/pasted-205.png" alt="upload successful"><br> 所以下载 Zookeeper 的时候要注意，应该下载第一个(本人头铁，下载了第二个)。</p><h3 id="2-端口冲突问题（-gt-3-5-0）"><a href="#2-端口冲突问题（-gt-3-5-0）" class="headerlink" title="2. 端口冲突问题（&gt;=3.5.0）"></a>2. 端口冲突问题（&gt;=3.5.0）</h3><p> 在3.5.5版本及以上，Zookeeper 提供了一个内嵌的Jetty容器来运行 AdminServer，默认占用的是 8080端口，AdminServer 主要是来查看 Zookeeper 的一些状态，如果机器上有其他程序（比如：Tomcat）占用了 8080 端口，也会导致 Starting zookeeper … FAILED TO START 的问题。</p><p>可以通过以下几种方式去解决：</p><ul><li><p>禁用 AdminServer 服务</p><pre><code>  admin.enableServer=false</code></pre></li><li><p>修改器端口号：</p><pre><code>  admin.serverPort=9000</code></pre></li></ul><p>转载自：<a href="https://blog.csdn.net/peng2hui1314/article/details/107255142">https://blog.csdn.net/peng2hui1314/article/details/107255142</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;今天在阿里云上安装ZooKeeper，然后启动时一直报: Starting zookeeper … FAILED TO START。折腾了一会，经查阅资料发现，原来和版本有莫大关系…&lt;/p&gt;
&lt;h3 id=&quot;1-下载的版本问题（-gt-3-5-5）&quot;&gt;&lt;a href=&quot;#1</summary>
      
    
    
    
    <category term="Zookeeper" scheme="http://example.com/categories/Zookeeper/"/>
    
    
    <category term="Zookeeper" scheme="http://example.com/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>重排序</title>
    <link href="http://example.com/2022/04/17/%E9%87%8D%E6%8E%92%E5%BA%8F/"/>
    <id>http://example.com/2022/04/17/%E9%87%8D%E6%8E%92%E5%BA%8F/</id>
    <published>2022-04-17T02:01:00.000Z</published>
    <updated>2022-04-17T02:13:06.463Z</updated>
    
    <content type="html"><![CDATA[<p>重排序是指编译器和处理器为了优化程序性能而对指令序列进行重新排序的一种手段。</p><h3 id="数据依赖性"><a href="#数据依赖性" class="headerlink" title="数据依赖性"></a>数据依赖性</h3><p>如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间 就存在数据依赖性。数据依赖分为下列3种类型</p><p> <img src="/images/pasted-203.png" alt="upload successful"></p><p> 上面3种情况，只要重排序两个操作的执行顺序，程序的执行结果就会被改变。 而编译器和处理器可能会对操作做重排序。编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序。 这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作， 不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑。</p><h3 id="as-if-serial语义"><a href="#as-if-serial语义" class="headerlink" title="as-if-serial语义"></a>as-if-serial语义</h3><p> as-if-serial语义的意思是：不管怎么重排序（编译器和处理器为了提高并行度），（单线程） 程序的执行结果不能被改变。编译器、runtime和处理器都必须遵守as-if-serial语义。</p><p> 为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因 为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作就可能被 编译器和处理器重排序。</p><p> as-if-serial语义把单线程程序保护了起来，遵守as-if-serial语义的编译器、runtime和处理器 共同为编写单线程程序的程序员创建了一个幻觉：单线程程序是按程序的顺序来执行的。as- if-serial语义使单线程程序员无需担心重排序会干扰他们，也无需担心内存可见性问题。</p><h3 id="程序顺序规则"><a href="#程序顺序规则" class="headerlink" title="程序顺序规则"></a>程序顺序规则</h3><p> 1）A happens-before B。 </p><p> 2）B happens-before C。 </p><p> 3）A happens-before C。</p><p> 这里的第3个happens-before关系，是根据happens-before的传递性推导出来的。 这里A happens-before B，但实际执行时B却可以排在A之前执行（看上面的重排序后的执 行顺序）。如果A happens-before B，JMM并不要求A一定要在B之前执行。JMM仅仅要求前一个 操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前。这里操作A 的执行结果不需要对操作B可见；而且重排序操作A和操作B后的执行结果，与操作A和操作B 按happens-before顺序执行的结果一致。在这种情况下，JMM会认为这种重排序并不非法（not illegal），JMM允许这种重排序。</p><p> 在计算机中，软件技术和硬件技术有一个共同的目标：在不改变程序执行结果的前提下， 尽可能提高并行度。编译器和处理器遵从这一目标，从happens-before的定义我们可以看出， JMM同样遵从这一目标。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;重排序是指编译器和处理器为了优化程序性能而对指令序列进行重新排序的一种手段。&lt;/p&gt;
&lt;h3 id=&quot;数据依赖性&quot;&gt;&lt;a href=&quot;#数据依赖性&quot; class=&quot;headerlink&quot; title=&quot;数据依赖性&quot;&gt;&lt;/a&gt;数据依赖性&lt;/h3&gt;&lt;p&gt;如果两个操作访问同一个变</summary>
      
    
    
    
    <category term="并发编程" scheme="http://example.com/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"/>
    
    
    <category term="JMM" scheme="http://example.com/tags/JMM/"/>
    
    <category term="并发编程" scheme="http://example.com/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"/>
    
    <category term="重排序" scheme="http://example.com/tags/%E9%87%8D%E6%8E%92%E5%BA%8F/"/>
    
  </entry>
  
  <entry>
    <title>Java内存模式的基础</title>
    <link href="http://example.com/2022/04/17/Java%E5%86%85%E5%AD%98%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%9F%BA%E7%A1%80/"/>
    <id>http://example.com/2022/04/17/Java%E5%86%85%E5%AD%98%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%9F%BA%E7%A1%80/</id>
    <published>2022-04-17T01:11:00.000Z</published>
    <updated>2022-04-17T01:52:43.054Z</updated>
    
    <content type="html"><![CDATA[<h3 id="并发编程模型的两个关键问题"><a href="#并发编程模型的两个关键问题" class="headerlink" title="并发编程模型的两个关键问题"></a>并发编程模型的两个关键问题</h3><p>在并发编程中，需要处理两个关键问题：线程之间如何通信及线程之间如何同步（这里的线程是指并发执行的活动实体）。</p><p>通信是指线程之间以何种机制来交换信息。在命令式编程中，线程之间的通信机制有两种：共享内存和消息传递。</p><ul><li>在共享内存的并发模型里，线程之间共享程序的公共状态，通过写-读内存中的公共状态 进行隐式通信。</li><li>在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过发送消 息来显式进行通信。</li></ul><p>同步是指程序中用于控制不同线程间操作发生相对顺序的机制。</p><ul><li>在共享内存并发模型里，同步是显式进行的。程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行。 </li><li>在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。</li></ul><p>Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对 程序员完全透明。如果编写多线程程序的Java程序员不理解隐式进行的线程之间通信的工作 机制，很可能会遇到各种奇怪的内存可见性问题。</p><h3 id="Java内存模型的抽象结构"><a href="#Java内存模型的抽象结构" class="headerlink" title="Java内存模型的抽象结构"></a>Java内存模型的抽象结构</h3><p>在Java中，所有实例域、静态域和数组元素都存储在堆内存中，堆内存在线程之间共享 （本章用“共享变量”这个术语代指实例域，静态域和数组元素）。局部变量（Local Variables），方 法定义参数（Java语言规范称之为Formal Method Parameters）和异常处理器参数（Exception Handler Parameters）不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影 响。</p><p>Java线程之间的通信由Java内存模型（本文简称为JMM）控制，JMM决定一个线程对共享 变量的写入何时对另一个线程可见。从抽象的角度来看，JMM定义了线程和主内存之间的抽 象关系：线程之间的共享变量存储在主内存（Main Memory）中，每个线程都有一个私有的本地 内存（Local Memory），本地内存中存储了该线程以读/写共享变量的副本。本地内存是JMM的 一个抽象概念，并不真实存在。它涵盖了缓存、写缓冲区、寄存器以及其他的硬件和编译器优化。</p><h3 id="从源代码到指令序列的重排序"><a href="#从源代码到指令序列的重排序" class="headerlink" title="从源代码到指令序列的重排序"></a>从源代码到指令序列的重排序</h3><p>在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序。重排序分3种类型。</p><ol><li>编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句 的执行顺序。 </li><li>指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism，ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应 机器指令的执行顺序。 </li><li>内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上 去可能是在乱序执行。</li></ol><p>这些重排序可能会导致多线程程序 出现内存可见性问题。对于编译器，JMM的编译器重排序规则会禁止特定类型的编译器重排 序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM的处理器重排序规则会要 求Java编译器在生成指令序列时，插入特定类型的内存屏障（Memory Barriers，Intel称之为 Memory Fence）指令，通过内存屏障指令来禁止特定类型的处理器重排序。</p><p>JMM属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。</p><h3 id="并发编程模型的分类"><a href="#并发编程模型的分类" class="headerlink" title="并发编程模型的分类"></a>并发编程模型的分类</h3><p>现代的处理器使用写缓冲区临时保存向内存写入的数据。写缓冲区可以保证指令流水线 持续运行，它可以避免由于处理器停顿下来等待向内存写入数据而产生的延迟。同时，通过以 批处理的方式刷新写缓冲区，以及合并写缓冲区中对同一内存地址的多次写，减少对内存总 线的占用。虽然写缓冲区有这么多好处，但每个处理器上的写缓冲区，仅仅对它所在的处理器 可见。这个特性会对内存操作的执行顺序产生重要的影响：处理器对内存的读/写操作的执行 顺序，不一定与内存实际发生的读/写操作顺序一致！</p><p> <img src="/images/pasted-200.png" alt="upload successful"><br> 这里处理器A和处理器B可以同时把共享变量写入自己的写缓冲区（A1，B1），然后从内存 中读取另一个共享变量（A2，B2），最后才把自己写缓存区中保存的脏数据刷新到内存中（A3， B3）。当以这种时序执行时，程序就可以得到x=y=0的结果。 从内存操作实际发生的顺序来看，直到处理器A执行A3来刷新自己的写缓存区，写操作 A1才算真正执行了。虽然处理器A执行内存操作的顺序为：A1→A2，但内存操作实际发生的顺 序却是A2→A1。此时，处理器A的内存操作顺序被重排序了（处理器B的情况和处理器A一样， 这里就不赘述了）。</p><p>这里的关键是，由于写缓冲区仅对自己的处理器可见，它会导致处理器执行内存操作的 顺序可能会与内存实际的操作执行顺序不一致。由于现代的处理器都会使用写缓冲区，因此现代的处理器都会允许对写-读操作进行重排序。</p><pre><code>常见的处理器都允许Store-Load重排序；常见的处理器都不允许对 存在数据依赖的操作做重排序。sparc-TSO和X86拥有相对较强的处理器内存模型，它们仅允 许对写-读操作做重排序（因为它们都使用了写缓冲区）。</code></pre><p>为了保证内存可见性，Java编译器在生成指令序列的适当位置会插入内存屏障指令来禁 止特定类型的处理器重排序。JMM把内存屏障指令分为4类</p><p> <img src="/images/pasted-201.png" alt="upload successful"><br> StoreLoad Barriers是一个“全能型”的屏障，它同时具有其他3个屏障的效果。现代的多处 理器大多支持该屏障（其他类型的屏障不一定被所有处理器支持）。执行该屏障开销会很昂 贵，因为当前处理器通常要把写缓冲区中的数据全部刷新到内存中（Buffer Fully Flush）。</p><h3 id="happens-before简介"><a href="#happens-before简介" class="headerlink" title="happens-before简介"></a>happens-before简介</h3><p> 从JDK 5开始，Java使用新的JSR-133内存模型（除非特别说明，本文针对的都是JSR-133内 存模型）。JSR-133使用happens-before的概念来阐述操作之间的内存可见性。在JMM中，如果一 个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须要存在happens-before关 系。这里提到的两个操作既可以是在一个线程之内，也可以是在不同线程之间。</p><p>与程序员密切相关的happens-before规则如下。 </p><ul><li><p>程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。  </p></li><li><p>监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。 </p></li><li><p>volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。 </p></li><li><p>传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。</p><pre><code>  注意：两个操作之间具有happens-before关系，并不意味着前一个操作必须要在后一个 操作之前执行！happens-before仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一 个操作按顺序排在第二个操作之前</code></pre></li></ul><p> <img src="/images/pasted-202.png" alt="upload successful"></p><p>一个happens-before规则对应于一个或多个编译器和处理器重排序规则。对 于Java程序员来说，happens-before规则简单易懂，它避免Java程序员为了理解JMM提供的内存 可见性保证而去学习复杂的重排序规则以及这些规则的具体实现方法。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;并发编程模型的两个关键问题&quot;&gt;&lt;a href=&quot;#并发编程模型的两个关键问题&quot; class=&quot;headerlink&quot; title=&quot;并发编程模型的两个关键问题&quot;&gt;&lt;/a&gt;并发编程模型的两个关键问题&lt;/h3&gt;&lt;p&gt;在并发编程中，需要处理两个关键问题：线程之间如何通信</summary>
      
    
    
    
    <category term="JMM" scheme="http://example.com/categories/JMM/"/>
    
    <category term="并发编程" scheme="http://example.com/categories/JMM/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"/>
    
    
    <category term="JMM" scheme="http://example.com/tags/JMM/"/>
    
    <category term="并发编程" scheme="http://example.com/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>First集和Follow集的构造</title>
    <link href="http://example.com/2022/04/14/First%E9%9B%86%E5%92%8CFollow%E9%9B%86%E7%9A%84%E6%9E%84%E9%80%A0/"/>
    <id>http://example.com/2022/04/14/First%E9%9B%86%E5%92%8CFollow%E9%9B%86%E7%9A%84%E6%9E%84%E9%80%A0/</id>
    <published>2022-04-14T10:57:00.000Z</published>
    <updated>2022-04-14T11:11:37.751Z</updated>
    
    <content type="html"><![CDATA[<p>在编译原理语法分析学习中，关于First集和Follow集的构造令我比较费解（课件上的白话太晦涩了，恕我看得一知半解），消化了好一会才明白，特此记录一下。</p><h3 id="关于First集"><a href="#关于First集" class="headerlink" title="关于First集"></a>关于First集</h3><p>对于 X -&gt; … 这条产生式而言：</p><ol><li><p>若右边第一个符号是终结符或 ε  ，则直接将其加入 First（X）</p></li><li><p>若右边第一个符号是非终结符，则将其 First 集的的非 ε  元素加入 First（X）</p></li><li><p>若右边第一个符号是非终结符而且紧随其后的是很多个非终结符，这个时候就要注意是否有 ε  。</p><p> 【3.1】若第 i 个非终结符的 First 集有 ε  ，则可将第 i+1 个非终结符去除 ε  的 First 集加入 First（X）。<br> 【3.2】若所有的非终结符都能够推导出 ε ，则将  ε  也加入到 First（X）</p><p>　　　<br> E.G. G[S]:</p></li></ol><p>　　　　　　S -&gt; ABCD</p><p>　　　　　　A -&gt; a |  ε  </p><p>　　　　　　B -&gt; b |  ε  </p><p>　　　　　　C -&gt; c</p><p>　　　　　　D -&gt; d</p><p>　　　　　　解：</p><p>　　　　　　　　First(S) = {a, b, c}，其中 c 是由上面所说的第二、三条规则所推得出来的，因为此时 A 和 B 都可以等于空串（ ε  ），所以非终结符 C 的 first 集合就被加入 G[S] 了。</p><p>　　　　　　　　如果这里 C，D 也能够产生 ε  的话，根据第三条规则中的第二小点，此时 First（S） = {a, b, c, d,  ε}</p><h3 id="关于Follow集"><a href="#关于Follow集" class="headerlink" title="关于Follow集"></a>关于Follow集</h3><ol><li><p>将所有产生式的候选式（即产生式右部）的非终结符都找到，定位到你想要求解 Follow 集的非终结符的位置，从当前位置往后挨个检查。设 A -&gt; aBC 是一个产生式，在这个产生式中， B 和 C 是非终结符，a 是终结符</p></li><li><p>先检验这个非终结符的右边还有没有别的符号（终结符或非终结符都可以），在例子中 B 是需要检查的第一个非终结符，它的右边是有非终结符 C 的。</p><ul><li><p>若右边有符号 -&gt; 将 First（右侧第一个符号）的非 ε 集合加入到 Follow（当前符号）中，如果 First（右侧第一个符号）含有 ε  ，即有 … -&gt; ε ，则将 Follow（产生式左部符号）加入 Follow（当前符号）中。</p></li><li><p>E.G.  用 A -&gt; aBC 来说就是，当前扫描到 B 了，而 B 的右侧有非终结符 C，则将去掉 ε  的 First（C）加入 Follow（B）中。若存在 C -&gt; ε  ，则将 Follow（A）也加入 Follow（B）中。</p></li><li><p>若右边没有符号了，例如这里的 C，那么可以将 Follow（A）中的元素全部加入到 Follow（C）中。</p></li></ul></li><li><p>判断当前符号是不是文法的开始符号，比如 G[A] 中的非终结符 A 就是 G[A] 文法的开始符号，如果是的话就将“#”也加入到 Follow（当前符号）中去</p></li></ol><h3 id="预测表的构造"><a href="#预测表的构造" class="headerlink" title="预测表的构造"></a>预测表的构造</h3><p>首先构造出预测分析表的第一行与第一列，第一行为文法出现的所有终结符以及‘#’（注意：没有 ε ，因为 Follow 集不含 ε），第一列为文法出现的所有终结符。</p><p>　　然后对文法 G 的每个产生式 A -&gt; ab 都执行如下操作：</p><p>　　　　【1】对于每个属于 First(ab) 的终结符 m ，都把 A -&gt; ab 添加到预测表中的 [A, m] 中去</p><p>　　　　【2】如果 ε 也属于 First(ab)，那么对于任何属于 Follow(A) 的字符 x，都把 A -&gt;  ε  加入到 [A, x] 中去</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在编译原理语法分析学习中，关于First集和Follow集的构造令我比较费解（课件上的白话太晦涩了，恕我看得一知半解），消化了好一会才明白，特此记录一下。&lt;/p&gt;
&lt;h3 id=&quot;关于First集&quot;&gt;&lt;a href=&quot;#关于First集&quot; class=&quot;headerlink</summary>
      
    
    
    
    <category term="编译原理" scheme="http://example.com/categories/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"/>
    
    
    <category term="上下文无关文法" scheme="http://example.com/tags/%E4%B8%8A%E4%B8%8B%E6%96%87%E6%97%A0%E5%85%B3%E6%96%87%E6%B3%95/"/>
    
    <category term="First" scheme="http://example.com/tags/First/"/>
    
    <category term="Follow" scheme="http://example.com/tags/Follow/"/>
    
  </entry>
  
  <entry>
    <title>初识LSM树</title>
    <link href="http://example.com/2022/04/14/%E5%88%9D%E8%AF%86LSM%E6%A0%91/"/>
    <id>http://example.com/2022/04/14/%E5%88%9D%E8%AF%86LSM%E6%A0%91/</id>
    <published>2022-04-14T01:09:00.000Z</published>
    <updated>2022-04-14T01:37:31.362Z</updated>
    
    <content type="html"><![CDATA[<p>今天在看MYSQL45讲的时候，看见一个新词LSM树，抱着好奇和求学的心态，上网查询了一下。对LMS树有了一个简单的认识。特以此篇来记录一下。摘抄自：<a href="https://zhuanlan.zhihu.com/p/181498475">https://zhuanlan.zhihu.com/p/181498475</a></p><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>LSM树并不像B+树、红黑树一样是一颗严格的树状数据结构，它其实是一种存储结构，目前HBase,LevelDB,RocksDB这些NoSQL存储都是采用的LSM树。</p><p>对于传统关系型数据库使用btree或一些变体作为存储结构，其能高效进行查找。但保存在磁盘中时它也有一个明显的缺陷，那就是逻辑上相离很近但物理却可能相隔很远，这就可能造成大量的磁盘随机读写。随机读写比顺序读写慢很多，为了提升IO性能，我们需要一种能将随机操作变为顺序操作的机制，于是便有了LSM树。LSM树能让我们进行顺序写磁盘，从而大幅提升写操作，作为代价的是牺牲了一些读性能。</p><h3 id="LMS树的核心思想"><a href="#LMS树的核心思想" class="headerlink" title="LMS树的核心思想"></a>LMS树的核心思想</h3><p> <img src="/images/pasted-194.png" alt="upload successful"><br>由上图可知，LMS主要由三个部分组成：</p><ol><li>MemTable<br>MemTable是在内存中的数据结构，用于保存最近更新的数据，会按照Key有序地组织这些数据，LSM树对于具体如何组织有序地组织数据并没有明确的数据结构定义，例如Hbase使跳跃表来保证内存中key的有序。</li></ol><p>因为数据暂时保存在内存中，内存并不是可靠存储，如果断电会丢失数据，因此通常会通过WAL(Write-ahead logging，预写式日志)的方式来保证数据的可靠性。</p><ol start="2"><li>Immutable MemTable</li></ol><p>当 MemTable达到一定大小后，会转化成Immutable MemTable。Immutable MemTable是将转MemTable变为SSTable的一种中间状态。写操作由新的MemTable处理，在转存过程中不阻塞数据更新操作。</p><ol start="3"><li>SSTable(Sorted String Table)</li></ol><p>有序键值对集合，是LSM树组在磁盘中的数据结构。为了加快SSTable的读取，可以通过建立key的索引以及布隆过滤器来加快key的查找。</p><ul><li><p>这里需要关注一个重点，LSM树(Log-Structured-Merge-Tree)正如它的名字一样，LSM树会将所有的数据插入、修改、删除等操作记录(注意是操作记录)保存在内存之中，当此类操作达到一定的数据量后，再批量地顺序写入到磁盘当中。这与B+树不同，B+树数据的更新会直接在原数据所在处修改对应的值，但是LSM数的数据更新是日志式的，当一条数据更新是直接append一条更新记录完成的。这样设计的目的就是为了顺序写，不断地将Immutable MemTable flush到持久化存储即可，而不用去修改之前的SSTable中的key，保证了顺序写。</p></li><li><p>因此当MemTable达到一定大小flush到持久化存储变成SSTable后，在不同的SSTable中，可能存在相同Key的记录，当然最新的那条记录才是准确的。这样设计的虽然大大提高了写性能，但同时也会带来一些问题：</p><pre><code>1）冗余存储，对于某个key，实际上除了最新的那条记录外，其他的记录都是冗余无用的，但是仍然占用了存储空间。因此需要进行Compact操作(合并多个SSTable)来清除冗余的记录。2）读取时需要从最新的倒着查询，直到找到某个key的记录。最坏情况需要查询完所有的SSTable，这里可以通过前面提到的索引/布隆过滤器来优化查找速度。</code></pre></li></ul><h3 id="LSM树的Compact策略"><a href="#LSM树的Compact策略" class="headerlink" title="LSM树的Compact策略"></a>LSM树的Compact策略</h3><p>从上面可以看出，Compact操作是十分关键的操作，否则SSTable数量会不断膨胀。在Compact策略上，主要介绍两种基本策略：size-tiered和leveled。</p><p>不过在介绍这两种策略之前，先介绍三个比较重要的概念，事实上不同的策略就是围绕这三个概念之间做出权衡和取舍。</p><ol><li>读放大:读取数据时实际读取的数据量大于真正的数据量。例如在LSM树中需要先在MemTable查看当前key是否存在，不存在继续从SSTable中寻找。</li><li>写放大:写入数据时实际写入的数据量大于真正的数据量。例如在LSM树中写入时可能触发Compact操作，导致实际写入的数据量远大于该key的数据量。</li><li>空间放大:数据实际占用的磁盘空间比数据的真正大小更多。上面提到的冗余存储，对于一个key来说，只有最新的那条记录是有效的，而之前的记录都是可以被清理回收的。</li></ol><ol><li>size-tiered 策略</li></ol><p> <img src="/images/pasted-195.png" alt="upload successful"></p><p>size-tiered策略保证每层SSTable的大小相近，同时限制每一层SSTable的数量。如上图，每层限制SSTable为N，当每层SSTable达到N后，则触发Compact操作合并这些SSTable，并将合并后的结果写入到下一层成为一个更大的sstable。</p><p>由此可以看出，当层数达到一定数量时，最底层的单个SSTable的大小会变得非常大。并且size-tiered策略会导致空间放大比较严重。即使对于同一层的SSTable，每个key的记录是可能存在多份的，只有当该层的SSTable执行compact操作才会消除这些key的冗余记录。</p><ol start="2"><li>leveled策略</li></ol><p> <img src="/images/pasted-196.png" alt="upload successful"></p><p>leveled策略也是采用分层的思想，每一层限制总文件的大小。</p><p>但是跟size-tiered策略不同的是，leveled会将每一层切分成多个大小相近的SSTable。这些SSTable是这一层是全局有序的，意味着一个key在每一层至多只有1条记录，不存在冗余记录。之所以可以保证全局有序，是因为合并策略和size-tiered不同，接下来会详细提到。</p><p>假设存在以下这样的场景:</p><ol><li>L1的总大小超过L1本身大小限制：</li></ol><p> <img src="/images/pasted-197.png" alt="upload successful"></p><ol start="2"><li>此时会从L1中选择至少一个文件，然后把它跟L2有交集的部分(非常关键)进行合并。生成的文件会放在L2:</li></ol><p> <img src="/images/pasted-198.png" alt="upload successful"></p><p> 如上图所示，此时L1第二SSTable的key的范围覆盖了L2中前三个SSTable，那么就需要将L1中第二个SSTable与L2中前三个SSTable执行Compact操作。</p><ol start="3"><li>如果L2合并后的结果仍旧超出L5的阈值大小，需要重复之前的操作 —— 选至少一个文件然后把它合并到下一层:</li></ol><p> <img src="/images/pasted-199.png" alt="upload successful"></p><pre><code> 需要注意的是，多个不相干的合并是可以并发进行的</code></pre><p>leveled策略相较于size-tiered策略来说，每层内key是不会重复的，即使是最坏的情况，除开最底层外，其余层都是重复key，按照相邻层大小比例为10来算，冗余占比也很小。因此空间放大问题得到缓解。但是写放大问题会更加突出。举一个最坏场景，如果LevelN层某个SSTable的key的范围跨度非常大，覆盖了LevelN+1层所有key的范围，那么进行Compact时将涉及LevelN+1层的全部数据。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>从Btree到LSM，其设计思想都和底层息息相关，了解学习不同的思想和底层。有助于我们对不同的框架的学习和思考。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;今天在看MYSQL45讲的时候，看见一个新词LSM树，抱着好奇和求学的心态，上网查询了一下。对LMS树有了一个简单的认识。特以此篇来记录一下。摘抄自：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/181498475&quot;&gt;https://zhuan</summary>
      
    
    
    
    <category term="LSM" scheme="http://example.com/categories/LSM/"/>
    
    
    <category term="LSM" scheme="http://example.com/tags/LSM/"/>
    
  </entry>
  
  <entry>
    <title>MySQL EXPLAIN 详解</title>
    <link href="http://example.com/2022/04/10/MySQL-EXPLAIN-%E8%AF%A6%E8%A7%A3/"/>
    <id>http://example.com/2022/04/10/MySQL-EXPLAIN-%E8%AF%A6%E8%A7%A3/</id>
    <published>2022-04-10T08:23:00.000Z</published>
    <updated>2022-04-10T08:44:36.967Z</updated>
    
    <content type="html"><![CDATA[<p>MySQL 提供了一个EXPALIN 命令，可以用于对SQL语句的执行计划进行分析，并详细的输出分析结果，供开发人员进行针对性的优化。</p><p>我们想要查询一条sql有没有用上索引，有没有全表查询，这些都可以通过explain这个命令来查看。</p><p>通过explain命令，我们可以深入了解到MySQL的基于开销的优化器，还可以获得很多被优化器考虑到的访问策略的细节以及运行sql语句时哪种策略预计会被优化器采用。</p><p>xplain的使用十分简单，通过在查询语句前面加一个explain关键字即可。</p><h2 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h2><p>explain 命令一共返回12列信息，分别是：</p><pre><code>id、select_type、table、partitions、type、possible_keys、key、key_len、ref、rows、filtered、Extra</code></pre><h3 id="id-列"><a href="#id-列" class="headerlink" title="id 列"></a>id 列</h3><ul><li>每个select语句都会自动分配的一个唯一标识符</li><li>表示查询中，操作表的顺序，有三种情况<pre><code>id相同，执行顺序从上到下id不同，如果是子查询，id号会自增，id越大，优先级越高id相同的不相同的同时存在</code></pre></li><li>id列为null表示为结果集，不需要使用这个语句来查询</li></ul><h3 id="select-type-列（很重要）"><a href="#select-type-列（很重要）" class="headerlink" title="select_type 列（很重要）"></a>select_type 列（很重要）</h3><p>查询类型，主要用于区别 普通查询、联合查询（union、union all）、子查询等复杂查询。</p><ul><li><p>simple：表示不需要union操作或者不包含子查询的简单select查询。有连接查询时，外层的查询为simple，且只有一个。</p></li><li><p>primary：一个需要使用union的操作或者含有子查询的select，位于最外层的单位查询的select_type即为primary。且只有一个</p></li><li><p>subquery:除了from子句中包含的子查询外，其它地方出现的子查询都可能时subquery</p></li><li><p>dependent subquery:子查询的结果受到外层的影响</p></li><li><p>union、union result:union 连接的多表查询，第一个查询primary，后面的是union, 结果集是 union result</p></li><li><p>dependent union:和union一样，出现在union或者union all中，但是这个查询要受到外部查询的影响</p></li><li><p>derived:在from子句后面的子查询，也叫派生表,注意，在MySql5.6 对于此查询没有优化，所以查询类型是derived.在mysql 5.7 使用了 Merge Derived table 优化，查询类型变为SIMPLE。通过控制参数: optimizer_switch=’derived=on|off’ 决定开始还是优化。默认开启。</p></li></ul><h2 id="table列"><a href="#table列" class="headerlink" title="table列"></a>table列</h2><ul><li><p>显示的查询表名，如果查询使用了别名，那么这里显示的就是别名</p></li><li><p>如果不涉及对数据表的操作，那么这里就是null</p></li><li><p>如果显示为尖括号括起来<derived N>就表示这是一个临时表，N就是执行计划的id，表示结果来自这个查询</p></li><li><p>如果显示为尖括号括起来的&lt;union n,m&gt;也表示一个临时表，表示来自union查询id为n、m的结果集</p></li></ul><h2 id="partitions-列"><a href="#partitions-列" class="headerlink" title="partitions 列"></a>partitions 列</h2><p>分区信息</p><h2 id="type-列-（重要）"><a href="#type-列-（重要）" class="headerlink" title="type 列 （重要）"></a>type 列 （重要）</h2><p>依次从好到差：</p><pre><code>system、const、eq_ref、ref、full_text、ref_or_null、unique_subquery、index_subquery、range、index_merge、index、all</code></pre><p>除了 All 以外，其它的类型都可以用到索引，除了index_merge可以使用多个索引之外，其它的类型最多只能使用到一个索引。</p><ul><li><p>system：表中只有一行数据或者是空表</p></li><li><p>const：使用唯一索引或者主键，返回记录一定是一条的等值where条件时，通常type是const。</p></li><li><p>eq_ref:连接字段为主键或者唯一索引，此类型通常出现于多表的join查询，表示对于前表的每一个结果，都对应后表的唯一一条结果。并且查询的比较是=操作，查询效率比较高。</p></li><li><p>ref:</p><ol><li>非主键或者唯一键的等值查询</li><li>join连接字段是非主键或者唯一键</li><li>最左前缀索引匹配</li></ol></li><li><p>fulltext:全文检索索引。</p></li><li><p>ref_or_null:和ref类似，增加了null值判断</p></li><li><p>unique_subquery、 index_subquery:都是子查询，前者返回唯一值，后者返回可能有重复。</p></li><li><p>range (重要):索引范围扫描，常用于 &gt;&lt;,is null,between,in,like等</p></li><li><p>index_merge(索引合并):表示查询使用了两个或者以上的索引数量，常见于and或者or查询匹配上了多个不同索引的字段</p></li><li><p>index(辅助索引):减少回表次数,因为要查询的索引都在一颗索引树上</p></li><li><p>all: 全表扫描</p></li></ul><h2 id="possible-keys-列"><a href="#possible-keys-列" class="headerlink" title="possible_keys 列"></a>possible_keys 列</h2><p>此次查询中，可能选用的索引</p><h2 id="key列"><a href="#key列" class="headerlink" title="key列"></a>key列</h2><p>查询实际使用的索引，select_type为index_merge时，key列可能有多个索引，其它时候这里只会有一个</p><h2 id="key-len-列"><a href="#key-len-列" class="headerlink" title="key_len 列"></a>key_len 列</h2><ul><li>用于处理查询的索引长度，如果是单列索引，那么整个索引长度都会计算进去，如果是多列索引，那么查询不一定能使用到所有的列，具体使用了多少个列的索引，这里就会计算进去，没有使用到的索引，这里不会计算进去。</li><li>留意一下这个长度，计算一下就知道这个索引使用了多少列</li><li>另外，key_len 只计算 where 条件使用到索引长度，而排序和分组就算用到了索引也不会计算key_len</li></ul><h2 id="ref"><a href="#ref" class="headerlink" title="ref"></a>ref</h2><ul><li>如果是使用的常数等值查询，这里会显示const</li><li>如果是连接查询，被驱动表的执行计划这里会显示驱动表的关联字段</li><li>如果是条件使用了表达式或者函数，或者条件列发生了内部隐式转换，这里可能会显示func</li></ul><h2 id="rows"><a href="#rows" class="headerlink" title="rows"></a>rows</h2><p>执行计划估算的扫描行数，不是精确值（innodb不是精确值，myisam是精确值，主要是因为innodb使用了mvcc）</p><h2 id="extra"><a href="#extra" class="headerlink" title="extra"></a>extra</h2><p>这个列包含很多不适合在其它列显示的重要信息，有很多种，常用的有：</p><ul><li><p>using temporary</p></li><li><p>表示使用了临时表存储中间结果   </p></li><li><p>MySQL在对 order by和group by 时使用临时表</p></li><li><p>临时表可以是内存临时表和磁盘临时表，执行计划中看不出来，需要查看status变量：used_tmp_table、used_tmp_disk_table才可以看出来</p></li><li><p>no table used</p></li><li><p>不带from字句的查询或者from dual查询（explain select 1;）</p></li></ul><p>使用 not in() 形式的子查询查询或者not exists运算符的连接查询，这种叫做反链接</p><pre><code>即：一般连接先查询内表再查询外表，反链接就是先查询外表再查询内表</code></pre><ul><li><p>using filesort</p></li><li><p>排序时无法使用到所以就会出现这个，常见于order by和group by</p></li><li><p>说明MySQL会使用一个外部的索引进行排序，而不是按照索引顺序进行读取</p></li><li><p>MySQL中无法利用索引完成的排序就叫“文件排序”</p></li><li><p>using index 查询时候不需要回表</p><ul><li>表示相应的select查询中使用到了覆盖索引(Covering index)，避免访问表的数据行</li><li>如果同时出现了using where，说明索引被用来执行查询键值如果没有using where，表示读取数据而不是执行查找操作</li></ul></li><li><p>using where</p><ul><li>表示存储引擎返回的记录并不都是符合条件的，需要在server层进行筛选过滤，性能很低</li></ul></li><li><p>using index condition</p><ul><li>索引下推，不需要再在server层进行过滤,5.6.x开始支持</li></ul></li><li><p>first match</p><ul><li>5.6.x 开始出现的优化子查询的新特性之一，常见于where字句含有in()类型的子查询，如果内表数据量过大，可能出现</li></ul></li><li><p>loosescan</p><ul><li>5.6.x 开始出现的优化子查询的新特性之一，常见于where字句含有in()类型的子查询，如果内表返回有重复值，可能出现</li></ul></li></ul><h2 id="filtered-列"><a href="#filtered-列" class="headerlink" title="filtered 列"></a>filtered 列</h2><p>5.7之后的版本默认就有这个字段，不需要使用explain extended了。这个字段表示存储引擎返回的数据在server层过滤后，剩下多少满足查询的记录数量的比例，注意是百分比，不是具体记录数。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;MySQL 提供了一个EXPALIN 命令，可以用于对SQL语句的执行计划进行分析，并详细的输出分析结果，供开发人员进行针对性的优化。&lt;/p&gt;
&lt;p&gt;我们想要查询一条sql有没有用上索引，有没有全表查询，这些都可以通过explain这个命令来查看。&lt;/p&gt;
&lt;p&gt;通过exp</summary>
      
    
    
    
    <category term="MySQL" scheme="http://example.com/categories/MySQL/"/>
    
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
    <category term="Explain" scheme="http://example.com/tags/Explain/"/>
    
  </entry>
  
  <entry>
    <title>MySQL日志和索引相关问题</title>
    <link href="http://example.com/2022/04/10/MySQL%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"/>
    <id>http://example.com/2022/04/10/MySQL%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/</id>
    <published>2022-04-10T06:50:00.000Z</published>
    <updated>2022-04-10T07:15:47.220Z</updated>
    
    <content type="html"><![CDATA[<h3 id="MySQL怎么知道binlog是完整的"><a href="#MySQL怎么知道binlog是完整的" class="headerlink" title="MySQL怎么知道binlog是完整的?"></a>MySQL怎么知道binlog是完整的?</h3><p>一个事务的binlog是有完整格式的：</p><ul><li>statement格式的binlog，最后会有COMMIT； </li><li>row格式的binlog，最后会有一个XID event。<br>另外，在MySQL 5.6.2版本以后，还引入了binlog-checksum参数，用来验证binlog内容的正确性。对于binlog日志由于磁盘原因，可能会在日志中间出错的情况，MySQL可以通过校验checksum的结果来发现。所以，MySQL还是有办法验证事务binlog的完整性的。</li></ul><h3 id="redo-log-和binlog是怎么关联起来的"><a href="#redo-log-和binlog是怎么关联起来的" class="headerlink" title="redo log 和binlog是怎么关联起来的?"></a>redo log 和binlog是怎么关联起来的?</h3><p>它们有一个共同的数据字段，叫XID。崩溃恢复的时候，会按顺序扫描redo log：</p><ul><li>如果碰到既有prepare、又有commit的redo log，就直接提交； </li><li>如果碰到只有parepare、而没有commit的redo log，就拿着XID去binlog找对应的事务。</li></ul><h3 id="为什么需要两阶段提交？先写redo-log，崩溃恢复时，必须两个日志都完整才可以，不是一样的吗？"><a href="#为什么需要两阶段提交？先写redo-log，崩溃恢复时，必须两个日志都完整才可以，不是一样的吗？" class="headerlink" title="为什么需要两阶段提交？先写redo log，崩溃恢复时，必须两个日志都完整才可以，不是一样的吗？"></a>为什么需要两阶段提交？先写redo log，崩溃恢复时，必须两个日志都完整才可以，不是一样的吗？</h3><p>回答：其实，两阶段提交是经典的分布式系统问题，并不是MySQL独有的。 如果必须要举一个场景，来说明这么做的必要性的话，那就是事务的持久性问题。 对于InnoDB引擎来说，如果redo log提交完成了，事务就不能回滚（如果这还允许回滚，就可能覆盖掉别的事务的更新）。而如果redo log直接提交，然后binlog写入的时候失败，InnoDB又回滚不了，数据和binlog日志又不一致了。 两阶段提交就是为了给所有人一个机会，当每个人都说“我ok”的时候，再一起提交。</p><h3 id="不引入两个日志，也就没有两阶段提交。只需要用redolog支持崩溃恢复和归档不是也可以吗？"><a href="#不引入两个日志，也就没有两阶段提交。只需要用redolog支持崩溃恢复和归档不是也可以吗？" class="headerlink" title="不引入两个日志，也就没有两阶段提交。只需要用redolog支持崩溃恢复和归档不是也可以吗？"></a>不引入两个日志，也就没有两阶段提交。只需要用redolog支持崩溃恢复和归档不是也可以吗？</h3><p>回答：</p><p>如果只从崩溃恢复的角度来讲是可以的。你可以把binlog关掉，这样就没有两阶段提交了，但系统依然是crash-safe的。 </p><p>但是，如果你了解一下业界各个公司的使用场景的话，就会发现在正式的生产库上，binlog都是开着的。因为binlog有着redo log无法替代的功能。 </p><p>一个是归档。redo log是循环写，写到末尾是要回到开头继续写的。这样历史日志没法保留，redo log也就起不到归档的作用。 </p><p>一个就是MySQL系统依赖于binlog。binlog作为MySQL一开始就有的功能，被用在了很多地方。 </p><p>其中，MySQL系统高可用的基础，就是binlog复制。 还有很多公司有异构系统（比如一些数据分析系统），这些系统就靠消费MySQL的binlog来更新 自己的数据。关掉binlog的话，这些下游系统就没法输入了。 总之，由于现在包括MySQL高可用在内的很多系统机制都依赖于binlog，所以“鸠占鹊巢”redo log还做不到。你看，发展生态是多么重要。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;MySQL怎么知道binlog是完整的&quot;&gt;&lt;a href=&quot;#MySQL怎么知道binlog是完整的&quot; class=&quot;headerlink&quot; title=&quot;MySQL怎么知道binlog是完整的?&quot;&gt;&lt;/a&gt;MySQL怎么知道binlog是完整的?&lt;/h3&gt;&lt;p&gt;</summary>
      
    
    
    
    <category term="MySQL" scheme="http://example.com/categories/MySQL/"/>
    
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>自增id用完了怎么办？</title>
    <link href="http://example.com/2022/04/10/%E8%87%AA%E5%A2%9Eid%E7%94%A8%E5%AE%8C%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9F/"/>
    <id>http://example.com/2022/04/10/%E8%87%AA%E5%A2%9Eid%E7%94%A8%E5%AE%8C%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9F/</id>
    <published>2022-04-10T05:40:00.000Z</published>
    <updated>2022-04-10T06:29:48.479Z</updated>
    
    <content type="html"><![CDATA[<p>MySQL里有很多自增的id，每个自增id都是定义了初始值，然后不停地往上加步长。虽然自然数 是没有上限的，但是在计算机里，只要定义了表示这个数的字节长度，那它就有上限。比如，无 符号整型(unsigned int)是4个字节，上限就是2 -1。既然自增id有上限，就有可能被用完。但是，自增id用完了会怎么样呢？</p><h3 id="表定义自增id"><a href="#表定义自增id" class="headerlink" title="表定义自增id"></a>表定义自增id</h3><p>表定义的自增值达到上限后的逻辑是：再申请下一个id时，得到的值保持不变。即当自增id用完，在插入新数据会报错（主键冲突）</p><p>2^32 -1（4294967295）不是一个特别大的数，对于一个频繁插入删除数据的表来说，是可能会被 用完的。因此在建表的时候你需要考察你的表是否有可能达到这个上限，如果有可能，就应该创 建成8个字节的bigint unsigned。</p><h3 id="InnoDB系统自自增row-id"><a href="#InnoDB系统自自增row-id" class="headerlink" title="InnoDB系统自自增row__id"></a>InnoDB系统自自增row__id</h3><p>如果你创建的InnoDB表没有指定主键，那么InnoDB会给你创建一个不可见的，长度为6个字节 的row_id。InnoDB维护了一个全局的dict_sys.row_id值，所有无主键的InnoDB表，每插入一行 数据，都将当前的dict_sys.row_id值作为要插入数据的row_id，然后把dict_sys.row_id的值加1。</p><p>实际上，在代码实现时row_id是一个长度为8字节的无符号长整型(bigint unsigned)。但 是，InnoDB在设计时，给row_id留的只是6个字节的长度，这样写到数据表中时只放了最后6个 字节，所以row_id能写到数据表中的值，就有两个特征：</p><ol><li> row_id写入表中的值范围，是从0到2^48 -1；</li><li>当dict_sys.row_id=2 时，如果再有插入数据的行为要来申请row_id，拿到以后再取最后6个 字节的话就是0。</li></ol><p>也就是说，写入表的row_id是从0开始到2^48 -1。达到上限后，下一个值就是0，然后继续循环。 当然，2^48 -1这个值本身已经很大了，但是如果一个MySQL实例跑得足够久的话，还是可能达到这个上限的。在InnoDB逻辑里，申请到row_id=N后，就将这行数据写入表中；如果表中已经存在row_id=N的行，新写入的行就会覆盖原有的行。</p><pre><code>从这个角度看，我们还是应该在InnoDB表中主动创建自增主键。因为，表自增id到达上限后， 再插入数据时报主键冲突错误，是更能被接受的。 毕竟覆盖数据，就意味着数据丢失，影响的是数据可靠性；报主键冲突，是插入失败，影响的是 可用性。而一般情况下，可靠性优先于可用性。</code></pre><h3 id="Xid"><a href="#Xid" class="headerlink" title="Xid"></a>Xid</h3><p>MySQL内部维护了一个全局变量global_query_id，每次执行语句的时候将它赋值给Query_id， 然后给这个变量加1。如果当前语句是这个事务执行的第一条语句，那么MySQL还会同时把 Query_id赋值给这个事务的Xid。</p><p>而global_query_id是一个纯内存变量，重启之后就清零了。所以你就知道了，在同一个数据库实 例中，不同事务的Xid也是有可能相同的。</p><p>但是MySQL重启之后会重新生成新的binlog文件，这就保证了，同一个binlog文件里，Xid一定是 惟一的。</p><p>虽然MySQL重启不会导致同一个binlog里面出现两个相同的Xid，但是如果global_query_id达到 上限后，就会继续从0开始计数。从理论上讲，还是就会出现同一个binlog里面出现相同Xid的场景。</p><p>因为global_query_id定义的长度是8个字节，这个自增值的上限是2^64 -1。要出现这种情况，必须是下面这样的过程：</p><ol><li>执行一个事务，假设Xid是A；</li><li>接下来执行2 次查询语句，让global_query_id回到A； </li><li>再启动一个事务，这个事务的Xid也是A。<br>不过，2 这个值太大了，大到你可以认为这个可能性只会存在于理论上。</li></ol><h3 id="Innodb-trx-id"><a href="#Innodb-trx-id" class="headerlink" title="Innodb trx__id"></a>Innodb trx__id</h3><p>Xid和InnoDB的trx_id是两个容易混淆的概念。</p><p>Xid是由server层维护的。InnoDB内部使用Xid，就是为了能够在InnoDB事务和server之间做关 联。但是，InnoDB自己的trx_id，是另外维护的。</p><p>InnoDB内部维护了一个max_trx_id全局变量，每次需要申请一个新的trx_id时，就获得 max_trx_id的当前值，然后并将max_trx_id加1。</p><p>InnoDB数据可见性的核心思想是：每一行数据都记录了更新它的trx_id，当一个事务读到一行数据的时候，判断这个数据是否可见的方法，就是通过事务的一致性视图与这行数据的trx_id做对 比。</p><p>对于正在执行的事务，你可以从information_schema.innodb_trx表中看到事务的trx_id。</p><p>看下面这个例子：</p><p> <img src="/images/pasted-191.png" alt="upload successful"><br> session B里，我从innodb_trx表里查出的这两个字段，第二个字段trx_mysql_thread_id就是线程 id。显示线程id，是为了说明这两次查询看到的事务对应的线程id都是5，也就是session A所在的线程。</p><p> 可以看到，T2时刻显示的trx_id是一个很大的数；T4时刻显示的trx_id是1289，看上去是一个比 较正常的数字。这是什么原因呢？</p><p> 实际上，在T1时刻，session A还没有涉及到更新，是一个只读事务。而对于只读事务，InnoDB 并不会分配trx_id。也就是说：</p><ol><li>在T1时刻，trx_id的值其实就是0。而这个很大的数，只是显示用的。一会儿我会再和你说说这个数据的生成逻辑。 </li><li>直到session A 在T3时刻执行insert语句的时候，InnoDB才真正分配了trx_id。所以，T4时刻，session B查到的这个trx_id的值就是1289。</li></ol><p>需要注意的是，除了显而易见的修改类语句外，如果在select 语句后面加上for update，这个事 务也不是只读事务。</p><pre><code>另外注意：1. update 和 delete语句除了事务本身，还涉及到标记删除旧数据，也就是要把数据放到purge 队列里等待后续物理删除，这个操作也会把max_trx_id+1， 因此在一个事务中至少加2； 2. InnoDB的后台操作，比如表的索引信息统计这类操作，也是会启动内部事务的，因此你可能看到，trx_id值并不是按照加1递增的。</code></pre><p>那么，T2时刻查到的这个很大的数字是怎么来的呢？</p><p>其实，这个数字是每次查询的时候由系统临时计算出来的。它的算法是：把当前事务的trx变量的 指针地址转成整数，再加上2 。使用这个算法，就可以保证以下两点：</p><ol><li>因为同一个只读事务在执行期间，它的指针地址是不会变的，所以不论是在 innodb_trx还是 在innodb_locks表里，同一个只读事务查出来的trx_id就会是一样的。</li><li>如果有并行的多个只读事务，每个事务的trx变量的指针地址肯定不同。这样，不同的并发只读事务，查出来的trx_id就是不同的。<br>那么，为什么还要再加上2^48呢？<br>在显示值里面加上2 ，目的是要保证只读事务显示的trx_id值比较大，正常情况下就会区别于读 写事务的id。但是，trx_id跟row_id的逻辑类似，定义长度也是8个字节。因此，在理论上还是可 能出现一个读写事务与一个只读事务显示的trx_id相同的情况。不过这个概率很低，并且也没有 什么实质危害，可以不管它。</li></ol><p>另一个问题是，只读事务不分配trx__id，有什么好处呢？</p><ul><li>一个好处是，这样做可以减小事务视图里面活跃事务数组的大小。因为当前正在运行的只读事务，是不影响数据的可见性判断的。所以，在创建事务的一致性视图时，InnoDB就只需要拷贝读写事务的trx_id。 </li><li>另一个好处是，可以减少trx_id的申请次数。在InnoDB里，即使你只是执行一个普通的select语句，在执行过程中，也是要对应一个只读事务的。所以只读事务优化后，普通的查询语句 不需要申请trx_id，就大大减少了并发事务申请trx_id的锁冲突。</li></ul><p>由于只读事务不分配trx_id，一个自然而然的结果就是trx_id的增加速度变慢了。</p><p>但是，max_trx_id会持久化存储，重启也不会重置为0，那么从理论上讲，只要一个MySQL服务 跑得足够久，就可能出现max_trx_id达到2^48-1的上限，然后从0开始的情况。</p><p>当达到这个状态后，MySQL就会持续出现一个脏读的bug，我们来复现一下这个bug。</p><p>首先我们需要把当前的max_trx_id先修改成248-1。注意：这个case里使用的是可重复读隔离级 别。具体的操作流程如下：</p><p> <img src="/images/pasted-192.png" alt="upload successful"></p><p> 由于我们已经把系统的max_trx_id设置成了2^48-1，所以在session A启动的事务TA的低水位就是2^48-1</p><p> 在T2时刻，session B执行第一条update语句的事务id就是2 -1，而第二条update语句的事务id 就是0了，这条update语句执行后生成的数据版本上的trx_id就是0。</p><p> 在T3时刻，session A执行select语句的时候，判断可见性发现，c=3这个数据版本的trx_id，小于 事务TA的低水位，因此认为这个数据可见。</p><p> 但，这个是脏读。</p><p> 由于低水位值会持续增加，而事务id从0开始计数，就导致了系统在这个时刻之后，所有的查询 都会出现脏读的。</p><p>并且，MySQL重启时max_trx_id也不会清0，也就是说重启MySQL，这个bug仍然存在。 那么，这个bug也是只存在于理论上吗？</p><p>假设一个MySQL实例的TPS是每秒50万，持续这个压力的话，在17.8年后，就会出现这个情 况。如果TPS更高，这个年限自然也就更短了。但是，从MySQL的真正开始流行到现在，恐怕 都还没有实例跑到过这个上限。不过，这个bug是只要MySQL实例服务时间够长，就会必然出现的。</p><h3 id="thread-id"><a href="#thread-id" class="headerlink" title="thread_id"></a>thread_id</h3><p>接下来，我们再看看线程id（thread_id）。其实，线程id才是MySQL中最常见的一种自增id。平 时我们在查各种现场的时候，showprocesslist里面的第一列，就是thread_id。</p><p>thread_id的逻辑很好理解：系统保存了一个全局变量thread_id_counter，每新建一个连接，就 将thread_id_counter赋值给这个新连接的线程变量。</p><p>thread_id_counter定义的大小是4个字节，因此达到2 -1后，它就会重置为0，然后继续增加。 但是，你不会在showprocesslist里看到两个相同的thread_id。</p><p>这，是因为MySQL设计了一个唯一数组的逻辑，给新线程分配thread_id的时候，逻辑代码是这样的：</p><p> <img src="/images/pasted-193.png" alt="upload successful"></p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>MySQL不同的自增id达到上限以后的行为。数据库系统作为一个可能需要7*24小时全年无休的服务，考虑这些边界是非常有必要的。</p><p>每种自增id有各自的应用场景，在达到上限后的表现也不同：</p><ol><li>表的自增id达到上限后，再申请时它的值就不会改变，进而导致继续插入数据时报主键冲突 的错误。 </li><li>row_id达到上限后，则会归0再重新递增，如果出现相同的row_id，后写的数据会覆盖之前 的数据。 </li><li>Xid只需要不在同一个binlog文件中出现重复值即可。虽然理论上会出现重复值，但是概率极 小，可以忽略不计。 </li><li>InnoDB的max_trx_id 递增值每次MySQL重启都会被保存起来，所以我们文章中提到的脏读 的例子就是一个必现的bug，好在留给我们的时间还很充裕。 </li><li>thread_id是我们使用中最常见的，而且也是处理得最好的一个自增id逻辑了。</li></ol><p>不同的自增id有不同的上限值，上限值的大小取决于声明的类型长度。</p><pre><code>注：学习自MYSQL45讲</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;MySQL里有很多自增的id，每个自增id都是定义了初始值，然后不停地往上加步长。虽然自然数 是没有上限的，但是在计算机里，只要定义了表示这个数的字节长度，那它就有上限。比如，无 符号整型(unsigned int)是4个字节，上限就是2 -1。既然自增id有上限，就有可能</summary>
      
    
    
    
    <category term="MySQL" scheme="http://example.com/categories/MySQL/"/>
    
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
    <category term="自增id" scheme="http://example.com/tags/%E8%87%AA%E5%A2%9Eid/"/>
    
  </entry>
  
  <entry>
    <title>关于Java SE 8 的流库</title>
    <link href="http://example.com/2022/04/09/%E5%85%B3%E4%BA%8EJava-SE-8-%E7%9A%84%E6%B5%81%E5%BA%93/"/>
    <id>http://example.com/2022/04/09/%E5%85%B3%E4%BA%8EJava-SE-8-%E7%9A%84%E6%B5%81%E5%BA%93/</id>
    <published>2022-04-09T06:58:00.000Z</published>
    <updated>2022-04-09T08:39:23.181Z</updated>
    
    <content type="html"><![CDATA[<p>流提供了一种让我们可以在比集合更高的概念级别上去指定计算的数据视图。通过使用流，我们可以说明想要完成什么任务，而不是说明如何去实现它。我们将操作的调度留给具体实现去解决。</p><h3 id="1-1-从迭代到流的操作"><a href="#1-1-从迭代到流的操作" class="headerlink" title="1.1 从迭代到流的操作"></a>1.1 从迭代到流的操作</h3><p>在处理集合时，我们通常会迭代遍历它的元素，并在每个元素上执行某项操作。</p><p> <img src="/images/pasted-178.png" alt="upload successful"></p><p> 而在使用流时，相同的操作是这样的</p><p> <img src="/images/pasted-179.png" alt="upload successful"></p><p>流的版本比循环版本更易于阅读。将stream修改为parallelstream就可以让流库以并行方式来执行过滤和计数。</p><p> <img src="/images/pasted-180.png" alt="upload successful"><br> 流遵循了“做什么,而非怎么做”的原则。在上述例子中，我们描述了需要做什么：获取单词长度，对其计数。我们没有指定该操作应该以什么顺序或者在哪个线程中执行。</p><p> 流表面上看起来和集合类似，但实际上期存在较大差异：</p><ol><li>流并不存储其元素。这些元素可能存储在底层的集合中，或者是按需生成的。</li><li>流的操作不会修改器数据源。</li><li>流的操作是尽可能的惰性执行。即直至需要结果时，操作才会执行。</li></ol><p> 还是以上述例子：stream会产生一个用于words的stream，filter会返回另一个流，其中只包含长度大于12的单词，count方法会将这个流简化为一个结果。即：</p><ol><li>创建一个流</li><li>指定将初始化流转化为其他流的中间操作，可能包含多个步骤</li><li>应用终止，产生结果</li></ol><p> <img src="/images/pasted-181.png" alt="upload successful"></p><h3 id="1-2-流的创建"><a href="#1-2-流的创建" class="headerlink" title="1.2 流的创建"></a>1.2 流的创建</h3><p> 可以用collection接口的stream方法将任何集合转换为一个流。如果你有一个数组，可以使用静态方法Stream.of方法进行流化。除此之外，也可以使用Array.stream(array,from,to)从数组from到to元素创建一个流。<br> 也可以使用stream.empty创建一个不包含任何元素的流。</p><p> stream接口有两个用于创建无限流的静态方法。</p><ol><li>generate方法会接收一个不包含任何引元的函数。无论何时，只需要一个流类型的值，该函数会被调用产生一个这样的值。我们可以像下面这样获得一个常量值的流：</li></ol><p> <img src="/images/pasted-182.png" alt="upload successful"></p><ol start="2"><li>iterate方法会接收一个“种子”值，以及一个函数，并且反复的将函数应用到之前的结果上，例如：</li></ol><p> <img src="/images/pasted-183.png" alt="upload successful"></p><p> java API中有大量方法可以产生流。</p><p> <img src="/images/pasted-184.png" alt="upload successful"></p><h3 id="1-3filter、map和flatMap方法"><a href="#1-3filter、map和flatMap方法" class="headerlink" title="1.3filter、map和flatMap方法"></a>1.3filter、map和flatMap方法</h3><p> 流的转换会产生一个新的流，它的元素派生自另一个流的元素。</p><p> filter的引元是Predicate<T>，即从T到boolean的函数。</p><p>  通常我们想要按照某种方法来转换流中的值，此时，可以使用map方法并传递执行该转换的函数。例如，我们可以用下面的方式，将单词转化为小写：</p><p> <img src="/images/pasted-185.png" alt="upload successful"><br> 这里，我们使用的是带有方法引用的map，但是，通常我们可以使用lambda表达式来代替：</p><p> <img src="/images/pasted-186.png" alt="upload successful"></p><p>在使用map时，会有一个函数应用到每个元素上，并且其结果是包含了应用函数后所产生的所有结果的流。</p><p> <img src="/images/pasted-187.png" alt="upload successful"></p><h3 id="1-4-抽取子流和连接流"><a href="#1-4-抽取子流和连接流" class="headerlink" title="1.4 抽取子流和连接流"></a>1.4 抽取子流和连接流</h3><p> stream.limit（n）会返回一个新的流，它在n个元素之后结束（如果原来的流更短，那么就会在流结束时结束）。这个方法对于剪裁无限流的尺寸会显得特别有用。</p><p><img src="/images/pasted-189.png" alt="upload successful"><br> 会产生包含100个随机数的流。</p><p> 调用strea.skip(n)则相反，会对其前n个元素。</p><h3 id="1-5-其他的流转换"><a href="#1-5-其他的流转换" class="headerlink" title="1.5 其他的流转换"></a>1.5 其他的流转换</h3><p> distinct方法会返回一个流，他的元素时从原有流产生的，即剔除掉重复元素的流。</p><p> 对于流的排序，有多种sorted方法的变体可用。其中一种用于操作comparable元素的流，而另一种则支持comparator。</p><p> 与所有流转换一样，sorted方法会产生一个新的流（按照规则已经排序）。</p><p> 最后，peek方法会产生另一个流，在每次获取元素时，都会调用一个函数，对于调式很有用。</p><h3 id="1-6-简单约简"><a href="#1-6-简单约简" class="headerlink" title="1.6 简单约简"></a>1.6 简单约简</h3><p>简单约简是一种终结操作，他们会将流约简为可以在程序中使用的非流值。</p><p>count就是其中一种（返回流中的元素）</p><p>其他的简单约简还有max和min之类。他们会返回最大值和最小值。而这里返回的值是一个类型Optional<T>的值，它要么在其中包装了答案，要么表示没有任何值（流碰巧为空）。所有Optional的引入是为了避免空指针异常这类问题的出现。</p><p>  如果只想知道是否匹配，那么可以使用angMatch、allMatch、nonMatch方法。</p><h3 id="1-7-收集结果"><a href="#1-7-收集结果" class="headerlink" title="1.7 收集结果"></a>1.7 收集结果</h3><p>当流处理完后，可以使用iterator方法查看结果，也可以使用forEach（在并行流中，调用forEach会以任意顺序遍历，可以使用forEachOrdered方法，当然该方法会丧失并行处理的优势）</p><p>如果想将结果收集到数据结构中，可以使用toArray函数。</p><p>如果是收集到另一个目标中，可以使用collect方法。</p><h3 id="1-8-收集到映射表中"><a href="#1-8-收集到映射表中" class="headerlink" title="1.8 收集到映射表中"></a>1.8 收集到映射表中</h3><p>Collectors.toMap方法有两个函数引元，他们用来产生映射表的键和值</p><p>如果有多个元素具体相同的键，则会存在冲突，收集器将会抛出一个Illeagel-StateException对象。可以通过第三个函数引元来覆盖这种行为。</p><p>未完，待续…</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;流提供了一种让我们可以在比集合更高的概念级别上去指定计算的数据视图。通过使用流，我们可以说明想要完成什么任务，而不是说明如何去实现它。我们将操作的调度留给具体实现去解决。&lt;/p&gt;
&lt;h3 id=&quot;1-1-从迭代到流的操作&quot;&gt;&lt;a href=&quot;#1-1-从迭代到流的操作&quot; c</summary>
      
    
    
    
    <category term="Java" scheme="http://example.com/categories/Java/"/>
    
    
    <category term="Java" scheme="http://example.com/tags/Java/"/>
    
    <category term="流" scheme="http://example.com/tags/%E6%B5%81/"/>
    
  </entry>
  
  <entry>
    <title>关于Java泛型</title>
    <link href="http://example.com/2022/04/09/%E5%85%B3%E4%BA%8EJava%E6%B3%9B%E5%9E%8B/"/>
    <id>http://example.com/2022/04/09/%E5%85%B3%E4%BA%8EJava%E6%B3%9B%E5%9E%8B/</id>
    <published>2022-04-09T06:32:00.000Z</published>
    <updated>2022-04-09T06:50:29.403Z</updated>
    
    <content type="html"><![CDATA[<h3 id="泛型的理解："><a href="#泛型的理解：" class="headerlink" title="泛型的理解："></a>泛型的理解：</h3><ol><li><p>泛型又称参数化类型，于jdk5.0提出的提特性，解决数据类型的安全性问题</p></li><li><p>在类声明或实例化时只要指定好需要的具体的类型即可</p></li><li><p>java的泛型可以保证如果程序在编译时没有发出警告，运行时就不会产生ClassCastException异常。同时，代码更加简介、健壮</p></li><li><p>泛型的作用是在类声明时通过一个标识表示类中某个属性类型，或者是某个方法返回值的类型，或者是参数类型</p></li></ol><h3 id="泛型的好处："><a href="#泛型的好处：" class="headerlink" title="泛型的好处："></a>泛型的好处：</h3><ol><li><p>编译时，检查添加元素的类型，提高了安全性</p></li><li><p>减少了类型转换的次数，提高了效率</p></li><li><p>不再提示编译警告</p></li></ol><h3 id="自定义泛型类："><a href="#自定义泛型类：" class="headerlink" title="自定义泛型类："></a>自定义泛型类：</h3><pre><code>public class Solution&lt;K,V&gt; &#123;    K k;    V v;    public K getK()&#123;        return null;    &#125;&#125;</code></pre><p>注意：</p><ol><li><p>普通成员可以使用泛型（属性、方法）</p></li><li><p>使用泛型的数组不能初始化</p></li><li><p>静态方法中不能使用类的泛型</p></li><li><p>泛型类的类型是在创建对象时确定的</p></li><li><p>如果在创建对象时没有指定类型，默认为object</p></li></ol><h3 id="自定义泛型接口"><a href="#自定义泛型接口" class="headerlink" title="自定义泛型接口"></a>自定义泛型接口</h3><pre><code>interface B&lt;T&gt;&#123;    T getInstance();&#125;</code></pre><p>注意：</p><ol><li><p>接口中，静态成员不能使用泛型</p></li><li><p>泛型接口的类型在继承接口或者实现接口时确定</p></li><li><p>没有指定类型，默认为Object</p></li></ol><h3 id="泛型的继承和通配符"><a href="#泛型的继承和通配符" class="headerlink" title="泛型的继承和通配符"></a>泛型的继承和通配符</h3><ol><li><p>泛型不具备继承性</p></li><li><?> :支持任意泛型类型</li><li><? extends A>: 支持A类以及A的子类，规定了泛型的上限</li><li><? super A>:支持A类以及A的父类，不限于直接父类，规避了泛型的下限</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;泛型的理解：&quot;&gt;&lt;a href=&quot;#泛型的理解：&quot; class=&quot;headerlink&quot; title=&quot;泛型的理解：&quot;&gt;&lt;/a&gt;泛型的理解：&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;泛型又称参数化类型，于jdk5.0提出的提特性，解决数据类型的安全性问题&lt;/p&gt;
&lt;/li</summary>
      
    
    
    
    <category term="Java" scheme="http://example.com/categories/Java/"/>
    
    
    <category term="Java" scheme="http://example.com/tags/Java/"/>
    
    <category term="泛型" scheme="http://example.com/tags/%E6%B3%9B%E5%9E%8B/"/>
    
  </entry>
  
</feed>
