<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-04-10T08:44:36.967Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MySQL EXPLAIN 详解</title>
    <link href="http://example.com/2022/04/10/MySQL-EXPLAIN-%E8%AF%A6%E8%A7%A3/"/>
    <id>http://example.com/2022/04/10/MySQL-EXPLAIN-%E8%AF%A6%E8%A7%A3/</id>
    <published>2022-04-10T08:23:00.000Z</published>
    <updated>2022-04-10T08:44:36.967Z</updated>
    
    <content type="html"><![CDATA[<p>MySQL 提供了一个EXPALIN 命令，可以用于对SQL语句的执行计划进行分析，并详细的输出分析结果，供开发人员进行针对性的优化。</p><p>我们想要查询一条sql有没有用上索引，有没有全表查询，这些都可以通过explain这个命令来查看。</p><p>通过explain命令，我们可以深入了解到MySQL的基于开销的优化器，还可以获得很多被优化器考虑到的访问策略的细节以及运行sql语句时哪种策略预计会被优化器采用。</p><p>xplain的使用十分简单，通过在查询语句前面加一个explain关键字即可。</p><h2 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h2><p>explain 命令一共返回12列信息，分别是：</p><pre><code>id、select_type、table、partitions、type、possible_keys、key、key_len、ref、rows、filtered、Extra</code></pre><h3 id="id-列"><a href="#id-列" class="headerlink" title="id 列"></a>id 列</h3><ul><li>每个select语句都会自动分配的一个唯一标识符</li><li>表示查询中，操作表的顺序，有三种情况<pre><code>id相同，执行顺序从上到下id不同，如果是子查询，id号会自增，id越大，优先级越高id相同的不相同的同时存在</code></pre></li><li>id列为null表示为结果集，不需要使用这个语句来查询</li></ul><h3 id="select-type-列（很重要）"><a href="#select-type-列（很重要）" class="headerlink" title="select_type 列（很重要）"></a>select_type 列（很重要）</h3><p>查询类型，主要用于区别 普通查询、联合查询（union、union all）、子查询等复杂查询。</p><ul><li><p>simple：表示不需要union操作或者不包含子查询的简单select查询。有连接查询时，外层的查询为simple，且只有一个。</p></li><li><p>primary：一个需要使用union的操作或者含有子查询的select，位于最外层的单位查询的select_type即为primary。且只有一个</p></li><li><p>subquery:除了from子句中包含的子查询外，其它地方出现的子查询都可能时subquery</p></li><li><p>dependent subquery:子查询的结果受到外层的影响</p></li><li><p>union、union result:union 连接的多表查询，第一个查询primary，后面的是union, 结果集是 union result</p></li><li><p>dependent union:和union一样，出现在union或者union all中，但是这个查询要受到外部查询的影响</p></li><li><p>derived:在from子句后面的子查询，也叫派生表,注意，在MySql5.6 对于此查询没有优化，所以查询类型是derived.在mysql 5.7 使用了 Merge Derived table 优化，查询类型变为SIMPLE。通过控制参数: optimizer_switch=’derived=on|off’ 决定开始还是优化。默认开启。</p></li></ul><h2 id="table列"><a href="#table列" class="headerlink" title="table列"></a>table列</h2><ul><li><p>显示的查询表名，如果查询使用了别名，那么这里显示的就是别名</p></li><li><p>如果不涉及对数据表的操作，那么这里就是null</p></li><li><p>如果显示为尖括号括起来<derived N>就表示这是一个临时表，N就是执行计划的id，表示结果来自这个查询</p></li><li><p>如果显示为尖括号括起来的&lt;union n,m&gt;也表示一个临时表，表示来自union查询id为n、m的结果集</p></li></ul><h2 id="partitions-列"><a href="#partitions-列" class="headerlink" title="partitions 列"></a>partitions 列</h2><p>分区信息</p><h2 id="type-列-（重要）"><a href="#type-列-（重要）" class="headerlink" title="type 列 （重要）"></a>type 列 （重要）</h2><p>依次从好到差：</p><pre><code>system、const、eq_ref、ref、full_text、ref_or_null、unique_subquery、index_subquery、range、index_merge、index、all</code></pre><p>除了 All 以外，其它的类型都可以用到索引，除了index_merge可以使用多个索引之外，其它的类型最多只能使用到一个索引。</p><ul><li><p>system：表中只有一行数据或者是空表</p></li><li><p>const：使用唯一索引或者主键，返回记录一定是一条的等值where条件时，通常type是const。</p></li><li><p>eq_ref:连接字段为主键或者唯一索引，此类型通常出现于多表的join查询，表示对于前表的每一个结果，都对应后表的唯一一条结果。并且查询的比较是=操作，查询效率比较高。</p></li><li><p>ref:</p><ol><li>非主键或者唯一键的等值查询</li><li>join连接字段是非主键或者唯一键</li><li>最左前缀索引匹配</li></ol></li><li><p>fulltext:全文检索索引。</p></li><li><p>ref_or_null:和ref类似，增加了null值判断</p></li><li><p>unique_subquery、 index_subquery:都是子查询，前者返回唯一值，后者返回可能有重复。</p></li><li><p>range (重要):索引范围扫描，常用于 &gt;&lt;,is null,between,in,like等</p></li><li><p>index_merge(索引合并):表示查询使用了两个或者以上的索引数量，常见于and或者or查询匹配上了多个不同索引的字段</p></li><li><p>index(辅助索引):减少回表次数,因为要查询的索引都在一颗索引树上</p></li><li><p>all: 全表扫描</p></li></ul><h2 id="possible-keys-列"><a href="#possible-keys-列" class="headerlink" title="possible_keys 列"></a>possible_keys 列</h2><p>此次查询中，可能选用的索引</p><h2 id="key列"><a href="#key列" class="headerlink" title="key列"></a>key列</h2><p>查询实际使用的索引，select_type为index_merge时，key列可能有多个索引，其它时候这里只会有一个</p><h2 id="key-len-列"><a href="#key-len-列" class="headerlink" title="key_len 列"></a>key_len 列</h2><ul><li>用于处理查询的索引长度，如果是单列索引，那么整个索引长度都会计算进去，如果是多列索引，那么查询不一定能使用到所有的列，具体使用了多少个列的索引，这里就会计算进去，没有使用到的索引，这里不会计算进去。</li><li>留意一下这个长度，计算一下就知道这个索引使用了多少列</li><li>另外，key_len 只计算 where 条件使用到索引长度，而排序和分组就算用到了索引也不会计算key_len</li></ul><h2 id="ref"><a href="#ref" class="headerlink" title="ref"></a>ref</h2><ul><li>如果是使用的常数等值查询，这里会显示const</li><li>如果是连接查询，被驱动表的执行计划这里会显示驱动表的关联字段</li><li>如果是条件使用了表达式或者函数，或者条件列发生了内部隐式转换，这里可能会显示func</li></ul><h2 id="rows"><a href="#rows" class="headerlink" title="rows"></a>rows</h2><p>执行计划估算的扫描行数，不是精确值（innodb不是精确值，myisam是精确值，主要是因为innodb使用了mvcc）</p><h2 id="extra"><a href="#extra" class="headerlink" title="extra"></a>extra</h2><p>这个列包含很多不适合在其它列显示的重要信息，有很多种，常用的有：</p><ul><li><p>using temporary</p></li><li><p>表示使用了临时表存储中间结果   </p></li><li><p>MySQL在对 order by和group by 时使用临时表</p></li><li><p>临时表可以是内存临时表和磁盘临时表，执行计划中看不出来，需要查看status变量：used_tmp_table、used_tmp_disk_table才可以看出来</p></li><li><p>no table used</p></li><li><p>不带from字句的查询或者from dual查询（explain select 1;）</p></li></ul><p>使用 not in() 形式的子查询查询或者not exists运算符的连接查询，这种叫做反链接</p><pre><code>即：一般连接先查询内表再查询外表，反链接就是先查询外表再查询内表</code></pre><ul><li><p>using filesort</p></li><li><p>排序时无法使用到所以就会出现这个，常见于order by和group by</p></li><li><p>说明MySQL会使用一个外部的索引进行排序，而不是按照索引顺序进行读取</p></li><li><p>MySQL中无法利用索引完成的排序就叫“文件排序”</p></li><li><p>using index 查询时候不需要回表</p><ul><li>表示相应的select查询中使用到了覆盖索引(Covering index)，避免访问表的数据行</li><li>如果同时出现了using where，说明索引被用来执行查询键值如果没有using where，表示读取数据而不是执行查找操作</li></ul></li><li><p>using where</p><ul><li>表示存储引擎返回的记录并不都是符合条件的，需要在server层进行筛选过滤，性能很低</li></ul></li><li><p>using index condition</p><ul><li>索引下推，不需要再在server层进行过滤,5.6.x开始支持</li></ul></li><li><p>first match</p><ul><li>5.6.x 开始出现的优化子查询的新特性之一，常见于where字句含有in()类型的子查询，如果内表数据量过大，可能出现</li></ul></li><li><p>loosescan</p><ul><li>5.6.x 开始出现的优化子查询的新特性之一，常见于where字句含有in()类型的子查询，如果内表返回有重复值，可能出现</li></ul></li></ul><h2 id="filtered-列"><a href="#filtered-列" class="headerlink" title="filtered 列"></a>filtered 列</h2><p>5.7之后的版本默认就有这个字段，不需要使用explain extended了。这个字段表示存储引擎返回的数据在server层过滤后，剩下多少满足查询的记录数量的比例，注意是百分比，不是具体记录数。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;MySQL 提供了一个EXPALIN 命令，可以用于对SQL语句的执行计划进行分析，并详细的输出分析结果，供开发人员进行针对性的优化。&lt;/p&gt;
&lt;p&gt;我们想要查询一条sql有没有用上索引，有没有全表查询，这些都可以通过explain这个命令来查看。&lt;/p&gt;
&lt;p&gt;通过exp</summary>
      
    
    
    
    <category term="MySQL" scheme="http://example.com/categories/MySQL/"/>
    
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
    <category term="Explain" scheme="http://example.com/tags/Explain/"/>
    
  </entry>
  
  <entry>
    <title>MySQL日志和索引相关问题</title>
    <link href="http://example.com/2022/04/10/MySQL%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"/>
    <id>http://example.com/2022/04/10/MySQL%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/</id>
    <published>2022-04-10T06:50:00.000Z</published>
    <updated>2022-04-10T07:15:47.220Z</updated>
    
    <content type="html"><![CDATA[<h3 id="MySQL怎么知道binlog是完整的"><a href="#MySQL怎么知道binlog是完整的" class="headerlink" title="MySQL怎么知道binlog是完整的?"></a>MySQL怎么知道binlog是完整的?</h3><p>一个事务的binlog是有完整格式的：</p><ul><li>statement格式的binlog，最后会有COMMIT； </li><li>row格式的binlog，最后会有一个XID event。<br>另外，在MySQL 5.6.2版本以后，还引入了binlog-checksum参数，用来验证binlog内容的正确性。对于binlog日志由于磁盘原因，可能会在日志中间出错的情况，MySQL可以通过校验checksum的结果来发现。所以，MySQL还是有办法验证事务binlog的完整性的。</li></ul><h3 id="redo-log-和binlog是怎么关联起来的"><a href="#redo-log-和binlog是怎么关联起来的" class="headerlink" title="redo log 和binlog是怎么关联起来的?"></a>redo log 和binlog是怎么关联起来的?</h3><p>它们有一个共同的数据字段，叫XID。崩溃恢复的时候，会按顺序扫描redo log：</p><ul><li>如果碰到既有prepare、又有commit的redo log，就直接提交； </li><li>如果碰到只有parepare、而没有commit的redo log，就拿着XID去binlog找对应的事务。</li></ul><h3 id="为什么需要两阶段提交？先写redo-log，崩溃恢复时，必须两个日志都完整才可以，不是一样的吗？"><a href="#为什么需要两阶段提交？先写redo-log，崩溃恢复时，必须两个日志都完整才可以，不是一样的吗？" class="headerlink" title="为什么需要两阶段提交？先写redo log，崩溃恢复时，必须两个日志都完整才可以，不是一样的吗？"></a>为什么需要两阶段提交？先写redo log，崩溃恢复时，必须两个日志都完整才可以，不是一样的吗？</h3><p>回答：其实，两阶段提交是经典的分布式系统问题，并不是MySQL独有的。 如果必须要举一个场景，来说明这么做的必要性的话，那就是事务的持久性问题。 对于InnoDB引擎来说，如果redo log提交完成了，事务就不能回滚（如果这还允许回滚，就可能覆盖掉别的事务的更新）。而如果redo log直接提交，然后binlog写入的时候失败，InnoDB又回滚不了，数据和binlog日志又不一致了。 两阶段提交就是为了给所有人一个机会，当每个人都说“我ok”的时候，再一起提交。</p><h3 id="不引入两个日志，也就没有两阶段提交。只需要用redolog支持崩溃恢复和归档不是也可以吗？"><a href="#不引入两个日志，也就没有两阶段提交。只需要用redolog支持崩溃恢复和归档不是也可以吗？" class="headerlink" title="不引入两个日志，也就没有两阶段提交。只需要用redolog支持崩溃恢复和归档不是也可以吗？"></a>不引入两个日志，也就没有两阶段提交。只需要用redolog支持崩溃恢复和归档不是也可以吗？</h3><p>回答：</p><p>如果只从崩溃恢复的角度来讲是可以的。你可以把binlog关掉，这样就没有两阶段提交了，但系统依然是crash-safe的。 </p><p>但是，如果你了解一下业界各个公司的使用场景的话，就会发现在正式的生产库上，binlog都是开着的。因为binlog有着redo log无法替代的功能。 </p><p>一个是归档。redo log是循环写，写到末尾是要回到开头继续写的。这样历史日志没法保留，redo log也就起不到归档的作用。 </p><p>一个就是MySQL系统依赖于binlog。binlog作为MySQL一开始就有的功能，被用在了很多地方。 </p><p>其中，MySQL系统高可用的基础，就是binlog复制。 还有很多公司有异构系统（比如一些数据分析系统），这些系统就靠消费MySQL的binlog来更新 自己的数据。关掉binlog的话，这些下游系统就没法输入了。 总之，由于现在包括MySQL高可用在内的很多系统机制都依赖于binlog，所以“鸠占鹊巢”redo log还做不到。你看，发展生态是多么重要。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;MySQL怎么知道binlog是完整的&quot;&gt;&lt;a href=&quot;#MySQL怎么知道binlog是完整的&quot; class=&quot;headerlink&quot; title=&quot;MySQL怎么知道binlog是完整的?&quot;&gt;&lt;/a&gt;MySQL怎么知道binlog是完整的?&lt;/h3&gt;&lt;p&gt;</summary>
      
    
    
    
    <category term="MySQL" scheme="http://example.com/categories/MySQL/"/>
    
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>自增id用完了怎么办？</title>
    <link href="http://example.com/2022/04/10/%E8%87%AA%E5%A2%9Eid%E7%94%A8%E5%AE%8C%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9F/"/>
    <id>http://example.com/2022/04/10/%E8%87%AA%E5%A2%9Eid%E7%94%A8%E5%AE%8C%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9F/</id>
    <published>2022-04-10T05:40:00.000Z</published>
    <updated>2022-04-10T06:29:48.479Z</updated>
    
    <content type="html"><![CDATA[<p>MySQL里有很多自增的id，每个自增id都是定义了初始值，然后不停地往上加步长。虽然自然数 是没有上限的，但是在计算机里，只要定义了表示这个数的字节长度，那它就有上限。比如，无 符号整型(unsigned int)是4个字节，上限就是2 -1。既然自增id有上限，就有可能被用完。但是，自增id用完了会怎么样呢？</p><h3 id="表定义自增id"><a href="#表定义自增id" class="headerlink" title="表定义自增id"></a>表定义自增id</h3><p>表定义的自增值达到上限后的逻辑是：再申请下一个id时，得到的值保持不变。即当自增id用完，在插入新数据会报错（主键冲突）</p><p>2^32 -1（4294967295）不是一个特别大的数，对于一个频繁插入删除数据的表来说，是可能会被 用完的。因此在建表的时候你需要考察你的表是否有可能达到这个上限，如果有可能，就应该创 建成8个字节的bigint unsigned。</p><h3 id="InnoDB系统自自增row-id"><a href="#InnoDB系统自自增row-id" class="headerlink" title="InnoDB系统自自增row__id"></a>InnoDB系统自自增row__id</h3><p>如果你创建的InnoDB表没有指定主键，那么InnoDB会给你创建一个不可见的，长度为6个字节 的row_id。InnoDB维护了一个全局的dict_sys.row_id值，所有无主键的InnoDB表，每插入一行 数据，都将当前的dict_sys.row_id值作为要插入数据的row_id，然后把dict_sys.row_id的值加1。</p><p>实际上，在代码实现时row_id是一个长度为8字节的无符号长整型(bigint unsigned)。但 是，InnoDB在设计时，给row_id留的只是6个字节的长度，这样写到数据表中时只放了最后6个 字节，所以row_id能写到数据表中的值，就有两个特征：</p><ol><li> row_id写入表中的值范围，是从0到2^48 -1；</li><li>当dict_sys.row_id=2 时，如果再有插入数据的行为要来申请row_id，拿到以后再取最后6个 字节的话就是0。</li></ol><p>也就是说，写入表的row_id是从0开始到2^48 -1。达到上限后，下一个值就是0，然后继续循环。 当然，2^48 -1这个值本身已经很大了，但是如果一个MySQL实例跑得足够久的话，还是可能达到这个上限的。在InnoDB逻辑里，申请到row_id=N后，就将这行数据写入表中；如果表中已经存在row_id=N的行，新写入的行就会覆盖原有的行。</p><pre><code>从这个角度看，我们还是应该在InnoDB表中主动创建自增主键。因为，表自增id到达上限后， 再插入数据时报主键冲突错误，是更能被接受的。 毕竟覆盖数据，就意味着数据丢失，影响的是数据可靠性；报主键冲突，是插入失败，影响的是 可用性。而一般情况下，可靠性优先于可用性。</code></pre><h3 id="Xid"><a href="#Xid" class="headerlink" title="Xid"></a>Xid</h3><p>MySQL内部维护了一个全局变量global_query_id，每次执行语句的时候将它赋值给Query_id， 然后给这个变量加1。如果当前语句是这个事务执行的第一条语句，那么MySQL还会同时把 Query_id赋值给这个事务的Xid。</p><p>而global_query_id是一个纯内存变量，重启之后就清零了。所以你就知道了，在同一个数据库实 例中，不同事务的Xid也是有可能相同的。</p><p>但是MySQL重启之后会重新生成新的binlog文件，这就保证了，同一个binlog文件里，Xid一定是 惟一的。</p><p>虽然MySQL重启不会导致同一个binlog里面出现两个相同的Xid，但是如果global_query_id达到 上限后，就会继续从0开始计数。从理论上讲，还是就会出现同一个binlog里面出现相同Xid的场景。</p><p>因为global_query_id定义的长度是8个字节，这个自增值的上限是2^64 -1。要出现这种情况，必须是下面这样的过程：</p><ol><li>执行一个事务，假设Xid是A；</li><li>接下来执行2 次查询语句，让global_query_id回到A； </li><li>再启动一个事务，这个事务的Xid也是A。<br>不过，2 这个值太大了，大到你可以认为这个可能性只会存在于理论上。</li></ol><h3 id="Innodb-trx-id"><a href="#Innodb-trx-id" class="headerlink" title="Innodb trx__id"></a>Innodb trx__id</h3><p>Xid和InnoDB的trx_id是两个容易混淆的概念。</p><p>Xid是由server层维护的。InnoDB内部使用Xid，就是为了能够在InnoDB事务和server之间做关 联。但是，InnoDB自己的trx_id，是另外维护的。</p><p>InnoDB内部维护了一个max_trx_id全局变量，每次需要申请一个新的trx_id时，就获得 max_trx_id的当前值，然后并将max_trx_id加1。</p><p>InnoDB数据可见性的核心思想是：每一行数据都记录了更新它的trx_id，当一个事务读到一行数据的时候，判断这个数据是否可见的方法，就是通过事务的一致性视图与这行数据的trx_id做对 比。</p><p>对于正在执行的事务，你可以从information_schema.innodb_trx表中看到事务的trx_id。</p><p>看下面这个例子：</p><p> <img src="/images/pasted-191.png" alt="upload successful"><br> session B里，我从innodb_trx表里查出的这两个字段，第二个字段trx_mysql_thread_id就是线程 id。显示线程id，是为了说明这两次查询看到的事务对应的线程id都是5，也就是session A所在的线程。</p><p> 可以看到，T2时刻显示的trx_id是一个很大的数；T4时刻显示的trx_id是1289，看上去是一个比 较正常的数字。这是什么原因呢？</p><p> 实际上，在T1时刻，session A还没有涉及到更新，是一个只读事务。而对于只读事务，InnoDB 并不会分配trx_id。也就是说：</p><ol><li>在T1时刻，trx_id的值其实就是0。而这个很大的数，只是显示用的。一会儿我会再和你说说这个数据的生成逻辑。 </li><li>直到session A 在T3时刻执行insert语句的时候，InnoDB才真正分配了trx_id。所以，T4时刻，session B查到的这个trx_id的值就是1289。</li></ol><p>需要注意的是，除了显而易见的修改类语句外，如果在select 语句后面加上for update，这个事 务也不是只读事务。</p><pre><code>另外注意：1. update 和 delete语句除了事务本身，还涉及到标记删除旧数据，也就是要把数据放到purge 队列里等待后续物理删除，这个操作也会把max_trx_id+1， 因此在一个事务中至少加2； 2. InnoDB的后台操作，比如表的索引信息统计这类操作，也是会启动内部事务的，因此你可能看到，trx_id值并不是按照加1递增的。</code></pre><p>那么，T2时刻查到的这个很大的数字是怎么来的呢？</p><p>其实，这个数字是每次查询的时候由系统临时计算出来的。它的算法是：把当前事务的trx变量的 指针地址转成整数，再加上2 。使用这个算法，就可以保证以下两点：</p><ol><li>因为同一个只读事务在执行期间，它的指针地址是不会变的，所以不论是在 innodb_trx还是 在innodb_locks表里，同一个只读事务查出来的trx_id就会是一样的。</li><li>如果有并行的多个只读事务，每个事务的trx变量的指针地址肯定不同。这样，不同的并发只读事务，查出来的trx_id就是不同的。<br>那么，为什么还要再加上2^48呢？<br>在显示值里面加上2 ，目的是要保证只读事务显示的trx_id值比较大，正常情况下就会区别于读 写事务的id。但是，trx_id跟row_id的逻辑类似，定义长度也是8个字节。因此，在理论上还是可 能出现一个读写事务与一个只读事务显示的trx_id相同的情况。不过这个概率很低，并且也没有 什么实质危害，可以不管它。</li></ol><p>另一个问题是，只读事务不分配trx__id，有什么好处呢？</p><ul><li>一个好处是，这样做可以减小事务视图里面活跃事务数组的大小。因为当前正在运行的只读事务，是不影响数据的可见性判断的。所以，在创建事务的一致性视图时，InnoDB就只需要拷贝读写事务的trx_id。 </li><li>另一个好处是，可以减少trx_id的申请次数。在InnoDB里，即使你只是执行一个普通的select语句，在执行过程中，也是要对应一个只读事务的。所以只读事务优化后，普通的查询语句 不需要申请trx_id，就大大减少了并发事务申请trx_id的锁冲突。</li></ul><p>由于只读事务不分配trx_id，一个自然而然的结果就是trx_id的增加速度变慢了。</p><p>但是，max_trx_id会持久化存储，重启也不会重置为0，那么从理论上讲，只要一个MySQL服务 跑得足够久，就可能出现max_trx_id达到2^48-1的上限，然后从0开始的情况。</p><p>当达到这个状态后，MySQL就会持续出现一个脏读的bug，我们来复现一下这个bug。</p><p>首先我们需要把当前的max_trx_id先修改成248-1。注意：这个case里使用的是可重复读隔离级 别。具体的操作流程如下：</p><p> <img src="/images/pasted-192.png" alt="upload successful"></p><p> 由于我们已经把系统的max_trx_id设置成了2^48-1，所以在session A启动的事务TA的低水位就是2^48-1</p><p> 在T2时刻，session B执行第一条update语句的事务id就是2 -1，而第二条update语句的事务id 就是0了，这条update语句执行后生成的数据版本上的trx_id就是0。</p><p> 在T3时刻，session A执行select语句的时候，判断可见性发现，c=3这个数据版本的trx_id，小于 事务TA的低水位，因此认为这个数据可见。</p><p> 但，这个是脏读。</p><p> 由于低水位值会持续增加，而事务id从0开始计数，就导致了系统在这个时刻之后，所有的查询 都会出现脏读的。</p><p>并且，MySQL重启时max_trx_id也不会清0，也就是说重启MySQL，这个bug仍然存在。 那么，这个bug也是只存在于理论上吗？</p><p>假设一个MySQL实例的TPS是每秒50万，持续这个压力的话，在17.8年后，就会出现这个情 况。如果TPS更高，这个年限自然也就更短了。但是，从MySQL的真正开始流行到现在，恐怕 都还没有实例跑到过这个上限。不过，这个bug是只要MySQL实例服务时间够长，就会必然出现的。</p><h3 id="thread-id"><a href="#thread-id" class="headerlink" title="thread_id"></a>thread_id</h3><p>接下来，我们再看看线程id（thread_id）。其实，线程id才是MySQL中最常见的一种自增id。平 时我们在查各种现场的时候，showprocesslist里面的第一列，就是thread_id。</p><p>thread_id的逻辑很好理解：系统保存了一个全局变量thread_id_counter，每新建一个连接，就 将thread_id_counter赋值给这个新连接的线程变量。</p><p>thread_id_counter定义的大小是4个字节，因此达到2 -1后，它就会重置为0，然后继续增加。 但是，你不会在showprocesslist里看到两个相同的thread_id。</p><p>这，是因为MySQL设计了一个唯一数组的逻辑，给新线程分配thread_id的时候，逻辑代码是这样的：</p><p> <img src="/images/pasted-193.png" alt="upload successful"></p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>MySQL不同的自增id达到上限以后的行为。数据库系统作为一个可能需要7*24小时全年无休的服务，考虑这些边界是非常有必要的。</p><p>每种自增id有各自的应用场景，在达到上限后的表现也不同：</p><ol><li>表的自增id达到上限后，再申请时它的值就不会改变，进而导致继续插入数据时报主键冲突 的错误。 </li><li>row_id达到上限后，则会归0再重新递增，如果出现相同的row_id，后写的数据会覆盖之前 的数据。 </li><li>Xid只需要不在同一个binlog文件中出现重复值即可。虽然理论上会出现重复值，但是概率极 小，可以忽略不计。 </li><li>InnoDB的max_trx_id 递增值每次MySQL重启都会被保存起来，所以我们文章中提到的脏读 的例子就是一个必现的bug，好在留给我们的时间还很充裕。 </li><li>thread_id是我们使用中最常见的，而且也是处理得最好的一个自增id逻辑了。</li></ol><p>不同的自增id有不同的上限值，上限值的大小取决于声明的类型长度。</p><pre><code>注：学习自MYSQL45讲</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;MySQL里有很多自增的id，每个自增id都是定义了初始值，然后不停地往上加步长。虽然自然数 是没有上限的，但是在计算机里，只要定义了表示这个数的字节长度，那它就有上限。比如，无 符号整型(unsigned int)是4个字节，上限就是2 -1。既然自增id有上限，就有可能</summary>
      
    
    
    
    <category term="MySQL" scheme="http://example.com/categories/MySQL/"/>
    
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
    <category term="自增id" scheme="http://example.com/tags/%E8%87%AA%E5%A2%9Eid/"/>
    
  </entry>
  
  <entry>
    <title>关于Java SE 8 的流库</title>
    <link href="http://example.com/2022/04/09/%E5%85%B3%E4%BA%8EJava-SE-8-%E7%9A%84%E6%B5%81%E5%BA%93/"/>
    <id>http://example.com/2022/04/09/%E5%85%B3%E4%BA%8EJava-SE-8-%E7%9A%84%E6%B5%81%E5%BA%93/</id>
    <published>2022-04-09T06:58:00.000Z</published>
    <updated>2022-04-09T08:39:23.181Z</updated>
    
    <content type="html"><![CDATA[<p>流提供了一种让我们可以在比集合更高的概念级别上去指定计算的数据视图。通过使用流，我们可以说明想要完成什么任务，而不是说明如何去实现它。我们将操作的调度留给具体实现去解决。</p><h3 id="1-1-从迭代到流的操作"><a href="#1-1-从迭代到流的操作" class="headerlink" title="1.1 从迭代到流的操作"></a>1.1 从迭代到流的操作</h3><p>在处理集合时，我们通常会迭代遍历它的元素，并在每个元素上执行某项操作。</p><p> <img src="/images/pasted-178.png" alt="upload successful"></p><p> 而在使用流时，相同的操作是这样的</p><p> <img src="/images/pasted-179.png" alt="upload successful"></p><p>流的版本比循环版本更易于阅读。将stream修改为parallelstream就可以让流库以并行方式来执行过滤和计数。</p><p> <img src="/images/pasted-180.png" alt="upload successful"><br> 流遵循了“做什么,而非怎么做”的原则。在上述例子中，我们描述了需要做什么：获取单词长度，对其计数。我们没有指定该操作应该以什么顺序或者在哪个线程中执行。</p><p> 流表面上看起来和集合类似，但实际上期存在较大差异：</p><ol><li>流并不存储其元素。这些元素可能存储在底层的集合中，或者是按需生成的。</li><li>流的操作不会修改器数据源。</li><li>流的操作是尽可能的惰性执行。即直至需要结果时，操作才会执行。</li></ol><p> 还是以上述例子：stream会产生一个用于words的stream，filter会返回另一个流，其中只包含长度大于12的单词，count方法会将这个流简化为一个结果。即：</p><ol><li>创建一个流</li><li>指定将初始化流转化为其他流的中间操作，可能包含多个步骤</li><li>应用终止，产生结果</li></ol><p> <img src="/images/pasted-181.png" alt="upload successful"></p><h3 id="1-2-流的创建"><a href="#1-2-流的创建" class="headerlink" title="1.2 流的创建"></a>1.2 流的创建</h3><p> 可以用collection接口的stream方法将任何集合转换为一个流。如果你有一个数组，可以使用静态方法Stream.of方法进行流化。除此之外，也可以使用Array.stream(array,from,to)从数组from到to元素创建一个流。<br> 也可以使用stream.empty创建一个不包含任何元素的流。</p><p> stream接口有两个用于创建无限流的静态方法。</p><ol><li>generate方法会接收一个不包含任何引元的函数。无论何时，只需要一个流类型的值，该函数会被调用产生一个这样的值。我们可以像下面这样获得一个常量值的流：</li></ol><p> <img src="/images/pasted-182.png" alt="upload successful"></p><ol start="2"><li>iterate方法会接收一个“种子”值，以及一个函数，并且反复的将函数应用到之前的结果上，例如：</li></ol><p> <img src="/images/pasted-183.png" alt="upload successful"></p><p> java API中有大量方法可以产生流。</p><p> <img src="/images/pasted-184.png" alt="upload successful"></p><h3 id="1-3filter、map和flatMap方法"><a href="#1-3filter、map和flatMap方法" class="headerlink" title="1.3filter、map和flatMap方法"></a>1.3filter、map和flatMap方法</h3><p> 流的转换会产生一个新的流，它的元素派生自另一个流的元素。</p><p> filter的引元是Predicate<T>，即从T到boolean的函数。</p><p>  通常我们想要按照某种方法来转换流中的值，此时，可以使用map方法并传递执行该转换的函数。例如，我们可以用下面的方式，将单词转化为小写：</p><p> <img src="/images/pasted-185.png" alt="upload successful"><br> 这里，我们使用的是带有方法引用的map，但是，通常我们可以使用lambda表达式来代替：</p><p> <img src="/images/pasted-186.png" alt="upload successful"></p><p>在使用map时，会有一个函数应用到每个元素上，并且其结果是包含了应用函数后所产生的所有结果的流。</p><p> <img src="/images/pasted-187.png" alt="upload successful"></p><h3 id="1-4-抽取子流和连接流"><a href="#1-4-抽取子流和连接流" class="headerlink" title="1.4 抽取子流和连接流"></a>1.4 抽取子流和连接流</h3><p> stream.limit（n）会返回一个新的流，它在n个元素之后结束（如果原来的流更短，那么就会在流结束时结束）。这个方法对于剪裁无限流的尺寸会显得特别有用。</p><p><img src="/images/pasted-189.png" alt="upload successful"><br> 会产生包含100个随机数的流。</p><p> 调用strea.skip(n)则相反，会对其前n个元素。</p><h3 id="1-5-其他的流转换"><a href="#1-5-其他的流转换" class="headerlink" title="1.5 其他的流转换"></a>1.5 其他的流转换</h3><p> distinct方法会返回一个流，他的元素时从原有流产生的，即剔除掉重复元素的流。</p><p> 对于流的排序，有多种sorted方法的变体可用。其中一种用于操作comparable元素的流，而另一种则支持comparator。</p><p> 与所有流转换一样，sorted方法会产生一个新的流（按照规则已经排序）。</p><p> 最后，peek方法会产生另一个流，在每次获取元素时，都会调用一个函数，对于调式很有用。</p><h3 id="1-6-简单约简"><a href="#1-6-简单约简" class="headerlink" title="1.6 简单约简"></a>1.6 简单约简</h3><p>简单约简是一种终结操作，他们会将流约简为可以在程序中使用的非流值。</p><p>count就是其中一种（返回流中的元素）</p><p>其他的简单约简还有max和min之类。他们会返回最大值和最小值。而这里返回的值是一个类型Optional<T>的值，它要么在其中包装了答案，要么表示没有任何值（流碰巧为空）。所有Optional的引入是为了避免空指针异常这类问题的出现。</p><p>  如果只想知道是否匹配，那么可以使用angMatch、allMatch、nonMatch方法。</p><h3 id="1-7-收集结果"><a href="#1-7-收集结果" class="headerlink" title="1.7 收集结果"></a>1.7 收集结果</h3><p>当流处理完后，可以使用iterator方法查看结果，也可以使用forEach（在并行流中，调用forEach会以任意顺序遍历，可以使用forEachOrdered方法，当然该方法会丧失并行处理的优势）</p><p>如果想将结果收集到数据结构中，可以使用toArray函数。</p><p>如果是收集到另一个目标中，可以使用collect方法。</p><h3 id="1-8-收集到映射表中"><a href="#1-8-收集到映射表中" class="headerlink" title="1.8 收集到映射表中"></a>1.8 收集到映射表中</h3><p>Collectors.toMap方法有两个函数引元，他们用来产生映射表的键和值</p><p>如果有多个元素具体相同的键，则会存在冲突，收集器将会抛出一个Illeagel-StateException对象。可以通过第三个函数引元来覆盖这种行为。</p><p>未完，待续…</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;流提供了一种让我们可以在比集合更高的概念级别上去指定计算的数据视图。通过使用流，我们可以说明想要完成什么任务，而不是说明如何去实现它。我们将操作的调度留给具体实现去解决。&lt;/p&gt;
&lt;h3 id=&quot;1-1-从迭代到流的操作&quot;&gt;&lt;a href=&quot;#1-1-从迭代到流的操作&quot; c</summary>
      
    
    
    
    <category term="Java" scheme="http://example.com/categories/Java/"/>
    
    
    <category term="Java" scheme="http://example.com/tags/Java/"/>
    
    <category term="流" scheme="http://example.com/tags/%E6%B5%81/"/>
    
  </entry>
  
  <entry>
    <title>关于Java泛型</title>
    <link href="http://example.com/2022/04/09/%E5%85%B3%E4%BA%8EJava%E6%B3%9B%E5%9E%8B/"/>
    <id>http://example.com/2022/04/09/%E5%85%B3%E4%BA%8EJava%E6%B3%9B%E5%9E%8B/</id>
    <published>2022-04-09T06:32:00.000Z</published>
    <updated>2022-04-09T06:50:29.403Z</updated>
    
    <content type="html"><![CDATA[<h3 id="泛型的理解："><a href="#泛型的理解：" class="headerlink" title="泛型的理解："></a>泛型的理解：</h3><ol><li><p>泛型又称参数化类型，于jdk5.0提出的提特性，解决数据类型的安全性问题</p></li><li><p>在类声明或实例化时只要指定好需要的具体的类型即可</p></li><li><p>java的泛型可以保证如果程序在编译时没有发出警告，运行时就不会产生ClassCastException异常。同时，代码更加简介、健壮</p></li><li><p>泛型的作用是在类声明时通过一个标识表示类中某个属性类型，或者是某个方法返回值的类型，或者是参数类型</p></li></ol><h3 id="泛型的好处："><a href="#泛型的好处：" class="headerlink" title="泛型的好处："></a>泛型的好处：</h3><ol><li><p>编译时，检查添加元素的类型，提高了安全性</p></li><li><p>减少了类型转换的次数，提高了效率</p></li><li><p>不再提示编译警告</p></li></ol><h3 id="自定义泛型类："><a href="#自定义泛型类：" class="headerlink" title="自定义泛型类："></a>自定义泛型类：</h3><pre><code>public class Solution&lt;K,V&gt; &#123;    K k;    V v;    public K getK()&#123;        return null;    &#125;&#125;</code></pre><p>注意：</p><ol><li><p>普通成员可以使用泛型（属性、方法）</p></li><li><p>使用泛型的数组不能初始化</p></li><li><p>静态方法中不能使用类的泛型</p></li><li><p>泛型类的类型是在创建对象时确定的</p></li><li><p>如果在创建对象时没有指定类型，默认为object</p></li></ol><h3 id="自定义泛型接口"><a href="#自定义泛型接口" class="headerlink" title="自定义泛型接口"></a>自定义泛型接口</h3><pre><code>interface B&lt;T&gt;&#123;    T getInstance();&#125;</code></pre><p>注意：</p><ol><li><p>接口中，静态成员不能使用泛型</p></li><li><p>泛型接口的类型在继承接口或者实现接口时确定</p></li><li><p>没有指定类型，默认为Object</p></li></ol><h3 id="泛型的继承和通配符"><a href="#泛型的继承和通配符" class="headerlink" title="泛型的继承和通配符"></a>泛型的继承和通配符</h3><ol><li><p>泛型不具备继承性</p></li><li><?> :支持任意泛型类型</li><li><? extends A>: 支持A类以及A的子类，规定了泛型的上限</li><li><? super A>:支持A类以及A的父类，不限于直接父类，规避了泛型的下限</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;泛型的理解：&quot;&gt;&lt;a href=&quot;#泛型的理解：&quot; class=&quot;headerlink&quot; title=&quot;泛型的理解：&quot;&gt;&lt;/a&gt;泛型的理解：&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;泛型又称参数化类型，于jdk5.0提出的提特性，解决数据类型的安全性问题&lt;/p&gt;
&lt;/li</summary>
      
    
    
    
    <category term="Java" scheme="http://example.com/categories/Java/"/>
    
    
    <category term="Java" scheme="http://example.com/tags/Java/"/>
    
    <category term="泛型" scheme="http://example.com/tags/%E6%B3%9B%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>关于next-key lock的加锁规则</title>
    <link href="http://example.com/2022/04/05/%E5%85%B3%E4%BA%8Enext-key-lock%E7%9A%84%E5%8A%A0%E9%94%81%E8%A7%84%E5%88%99/"/>
    <id>http://example.com/2022/04/05/%E5%85%B3%E4%BA%8Enext-key-lock%E7%9A%84%E5%8A%A0%E9%94%81%E8%A7%84%E5%88%99/</id>
    <published>2022-04-05T08:29:00.000Z</published>
    <updated>2022-04-05T10:24:43.455Z</updated>
    
    <content type="html"><![CDATA[<h1 id="next-key-lock的加锁规则"><a href="#next-key-lock的加锁规则" class="headerlink" title="next-key lock的加锁规则"></a>next-key lock的加锁规则</h1><p>总结的加锁规则里面，包含了两个 “ “ 原则 ” ” 、两个 “ “ 优化 ” ” 和一个 “bug” 。 </p><ol><li>原则 1 ：加锁的基本单位是 next-key lock 。 next-key lock 是前开后闭区间。</li><li>原则 2 ：查找过程中访问到的对象才会加锁。任何辅助索引上的锁，或者非索引列上的锁，最终都要回溯到主键上，在主键上也要加一把锁。</li><li>优化 1 ：索引上的等值查询，给唯一索引加锁的时候， next-key lock 退化为行锁。也就是说如果InnoDB扫描的是一个主键、或是一个唯一索引的话，那InnoDB只会采用行锁方式来加锁</li><li>优化 2 ：索引上（不一定是唯一索引）的等值查询，向右遍历时且最后一个值不满足等值条件的时候， next-keylock 退化为间隙锁。</li><li>一个 bug ：唯一索引上的范围查询会访问到不满足条件的第一个值为止。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;next-key-lock的加锁规则&quot;&gt;&lt;a href=&quot;#next-key-lock的加锁规则&quot; class=&quot;headerlink&quot; title=&quot;next-key lock的加锁规则&quot;&gt;&lt;/a&gt;next-key lock的加锁规则&lt;/h1&gt;&lt;p&gt;总结的加锁规</summary>
      
    
    
    
    <category term="MySQL" scheme="http://example.com/categories/MySQL/"/>
    
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
    <category term="next-key lock" scheme="http://example.com/tags/next-key-lock/"/>
    
  </entry>
  
  <entry>
    <title>读已提交下，为什么建议binlog使用row格式</title>
    <link href="http://example.com/2022/04/05/%E8%AF%BB%E5%B7%B2%E6%8F%90%E4%BA%A4%E4%B8%8B%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BB%BA%E8%AE%AEbinlog%E4%BD%BF%E7%94%A8row%E6%A0%BC%E5%BC%8F/"/>
    <id>http://example.com/2022/04/05/%E8%AF%BB%E5%B7%B2%E6%8F%90%E4%BA%A4%E4%B8%8B%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BB%BA%E8%AE%AEbinlog%E4%BD%BF%E7%94%A8row%E6%A0%BC%E5%BC%8F/</id>
    <published>2022-04-05T05:16:00.000Z</published>
    <updated>2022-04-05T08:29:04.394Z</updated>
    
    <content type="html"><![CDATA[<p>next-key lock 实际上是由间隙锁加行锁实现的。而间隙锁是在可重复读隔离级别下才会生效的。因此在读已提交这个隔离级别下，为了解决数据和日志的不一致问题，需要将binlog的格式改为row，而不是statement。（statement记录的是逻辑上的SQL，而row记录的是对应行的变化情况）</p><p>解释：</p><p>CREATE TABLE <code>t</code> (<br>  <code>id</code> int(11) NOT NULL,<br>  <code>c</code> int(11) DEFAULT NULL,<br>  <code>d</code> int(11) DEFAULT NULL,<br>  PRIMARY KEY (<code>id</code>),<br>  KEY <code>c</code> (<code>c</code>)<br>) ENGINE=InnoDB;</p><p>insert into t values(0,0,0),(5,5,5),<br>(10,10,10),(15,15,15),(20,20,20),(25,25,25);</p><p> <img src="/images/pasted-177.png" alt="upload successful"><br>建表执行过程如上所述，首先针对于T3和T5时session A查询会出现幻读问题（语义上，我们是要把d=5的所有数据锁住，而后续SessionB和C都能够操作d=5的数据），其次会出现数据和日志不一致的问题（binlog写入是根据commit提交决定的，因此SessionB会先写入，随后Session C，最后才是Session A，这样导致的结果就是，我们数据库的数据是：</p><ol><li>经过 T1 时刻，id=5 这一行变成 (5,5,100)，当然这个结果最终是在 T6 时刻正式提交的 ;</li><li>经过 T2 时刻，id=0 这一行变成 (0,5,5);</li><li>经过 T4 时刻，表里面多了一行 (1,5,5);</li><li>其他行跟这个执行序列无关，保持不变。</li></ol><p>而binlog日志里面记录的数据却是这样的：<br>update t set d=5 where id=0; /<em>(0,0,5)</em>/</p><p>update t set c=5 where id=0; /<em>(0,5,5)</em>/</p><p>insert into t values(1,1,5); /<em>(1,1,5)</em>/</p><p>update t set c=5 where id=1; /<em>(1,5,5)</em>/</p><p>update t set d=100 where d=5;/* 所有 d=5 的行，d 改成 100*/</p><p>这个语句序列，不论是拿到备库去执行，还是以后用 binlog 来克隆一个库，这三行的结果，都变成了 (0,5,100)、(1,5,100) 和 (5,5,100)。<br>也就是说，id=0 和 id=1 这两行，发生了数据不一致。<br>）</p><p>因此在读已提交下，使用Statement格式的binlog日志是不可取的。而如果你改用row格式，那么对于上面这种情况，记录的是对应行的修改，而不是逻辑上的SQL语句，能够避免这种问题。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;next-key lock 实际上是由间隙锁加行锁实现的。而间隙锁是在可重复读隔离级别下才会生效的。因此在读已提交这个隔离级别下，为了解决数据和日志的不一致问题，需要将binlog的格式改为row，而不是statement。（statement记录的是逻辑上的SQL，而ro</summary>
      
    
    
    
    <category term="MySQL" scheme="http://example.com/categories/MySQL/"/>
    
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
    <category term="读已提交" scheme="http://example.com/tags/%E8%AF%BB%E5%B7%B2%E6%8F%90%E4%BA%A4/"/>
    
    <category term="binlog" scheme="http://example.com/tags/binlog/"/>
    
  </entry>
  
  <entry>
    <title>MySQL中的自增长锁需要注意的点</title>
    <link href="http://example.com/2022/04/05/MySQL%E4%B8%AD%E7%9A%84%E8%87%AA%E5%A2%9E%E9%95%BF%E9%94%81%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E7%82%B9/"/>
    <id>http://example.com/2022/04/05/MySQL%E4%B8%AD%E7%9A%84%E8%87%AA%E5%A2%9E%E9%95%BF%E9%94%81%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E7%82%B9/</id>
    <published>2022-04-05T02:13:00.000Z</published>
    <updated>2022-04-05T02:24:26.726Z</updated>
    
    <content type="html"><![CDATA[<p>在表设计的时候，int类型自增长的主键一直是我们的最爱，但是如果你不了解自增长主键，那么在某些情况下可能会给你带来一些意想不到的错误。</p><p>在innoDB存储引擎中，对每个含有自增长值的表都有一个自增长计数器，当对该表插入记录时，这个计数器会+n，插入方式会根据这个计数器的值确定自增id。而这种插入，为了避免在多线程下的冲突问题，采用了表锁来处理。同时为了提高插入性能，该锁并不是在一个事务提交之后才释放，而是完成对自增长值插入SQL语句后立即释放。尽管如此，这种方式的插入性能还是很差。事务必须等前一个插入完成，其次对于大批量的数据插入影响性能。</p><p>因此从MySQL5.1.22开始，innoDB提供了一种轻量级互斥遍历的自增长实现机制，通过innoDB_autoinc_lock_mode来控制自增长的模式。如下图所示</p><p> <img src="/images/pasted-175.png" alt="upload successful"></p><p> <img src="/images/pasted-176.png" alt="upload successful"></p><p> 此外，还需要特别注意的是innoDB存储引擎的自增长和MyISAM不同，MyISAM是表级锁，自增长无需考虑并发插入。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在表设计的时候，int类型自增长的主键一直是我们的最爱，但是如果你不了解自增长主键，那么在某些情况下可能会给你带来一些意想不到的错误。&lt;/p&gt;
&lt;p&gt;在innoDB存储引擎中，对每个含有自增长值的表都有一个自增长计数器，当对该表插入记录时，这个计数器会+n，插入方式会根据这</summary>
      
    
    
    
    <category term="MySQL" scheme="http://example.com/categories/MySQL/"/>
    
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
    <category term="自增主键" scheme="http://example.com/tags/%E8%87%AA%E5%A2%9E%E4%B8%BB%E9%94%AE/"/>
    
    <category term="锁" scheme="http://example.com/tags/%E9%94%81/"/>
    
  </entry>
  
  <entry>
    <title>Redis的通信协议RESP</title>
    <link href="http://example.com/2022/04/03/Redis%E7%9A%84%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AERESP/"/>
    <id>http://example.com/2022/04/03/Redis%E7%9A%84%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AERESP/</id>
    <published>2022-04-03T14:00:00.000Z</published>
    <updated>2022-04-03T14:09:04.344Z</updated>
    
    <content type="html"><![CDATA[<p>我们知道，Redis 客户端与服务端是通过命令的方式来完成交互过程的，主要分为两个部分：网络模型和序列化协议。前者讨论的是数据交互的组织方式，后者讨论的是数据如何序列化。</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Redis 的通信协议是 Redis Serialization Protocol，翻译为 Redis 序列化协议，简称 RESP。它具有如下特征：</p><ul><li>在 TCP 层</li><li>是二进制安全的</li><li>基于请求 - 响应模式</li><li>简单、易懂（人都可以看懂）</li><li>RESP 所描述的是 Redis 客户端 - 服务端的交互方式。</li></ul><h2 id="RESP描述"><a href="#RESP描述" class="headerlink" title="RESP描述"></a>RESP描述</h2><p>Redis 协议将传输的结构数据分为 5 种类型，单元结束时统一加上回车换行符号 \r\n。</p><ul><li>单行字符串，第一个字节为 +</li><li>错误消息，第一个字节为 -</li><li>整型数字，第一个字节为 :，后跟整数的字符串</li><li>多行字符串，第一个字节为 $，后跟字符串的长度</li><li>数组，第一个字节为 *，后跟跟着数组的长度</li></ul><h2 id="请求命令"><a href="#请求命令" class="headerlink" title="请求命令"></a>请求命令</h2><p>Redis 对每一条请求命令都做了统一规范，格式如下：</p><pre><code>        *&lt;number of arguments&gt; CR LF        $&lt;number of bytes of argument 1&gt; CR LF        &lt;argument data&gt; CR LF        ...        $&lt;number of bytes of argument N&gt; CR LF        &lt;argument data&gt; CR LF</code></pre><p>翻译如下：</p><ul><li><p>number of arguments ： 参数的数量</p></li><li><p>CR LF：\r\n</p></li><li><p>number of bytes of argument 1：参数 1 的字节数</p></li><li><p>number of bytes of argument N：参数 N 的字节数<br>以命令 set userName chenssy 为例，如下：</p><pre><code>  *3  $3  SET  $8  userName  $7  chenssy</code></pre><p>解释：</p><pre><code>  *3 数组，表明有三个参数 SET、userName、chenssy  $3 多行字符串，第一个参数 SET ，有 3 个字符  $8 多行字符串，第二个参数 userName，有 8 个字符  $7 多行字符串，第三个参数 chenssy，有 7 个字符</code></pre><p>上面只是格式化显示的结果，真正传输的结果如下：</p><pre><code>  *3\r\n$3\r\nSET\r\n$8\r\nuserName\r\n$7\r\nchenssy\r\n</code></pre><h2 id="回复命令"><a href="#回复命令" class="headerlink" title="回复命令"></a>回复命令</h2><p>Redis 服务端响应要支持多种数据格式，所以回复命令一般都会显得复杂些，但是无论如何它都逃脱不了上面 5 中类型及其组合。</p></li></ul><p>从上面我们可以看出 RESP 协议是非常简单直观的一种协议，我们肉眼都可以看懂，而且数据结构类型也只有少少的 5 中，所以实现起来就变得很简单了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我们知道，Redis 客户端与服务端是通过命令的方式来完成交互过程的，主要分为两个部分：网络模型和序列化协议。前者讨论的是数据交互的组织方式，后者讨论的是数据如何序列化。&lt;/p&gt;
&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; ti</summary>
      
    
    
    
    <category term="Redis" scheme="http://example.com/categories/Redis/"/>
    
    
    <category term="RESP" scheme="http://example.com/tags/RESP/"/>
    
    <category term="通信协议" scheme="http://example.com/tags/%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/"/>
    
    <category term="Redis" scheme="http://example.com/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Java中的Queue那些事</title>
    <link href="http://example.com/2022/04/03/Java%E4%B8%AD%E7%9A%84Queue%E9%82%A3%E4%BA%9B%E4%BA%8B/"/>
    <id>http://example.com/2022/04/03/Java%E4%B8%AD%E7%9A%84Queue%E9%82%A3%E4%BA%9B%E4%BA%8B/</id>
    <published>2022-04-03T08:37:00.000Z</published>
    <updated>2022-04-03T11:42:52.074Z</updated>
    
    <content type="html"><![CDATA[<h2 id="PriorityQueue"><a href="#PriorityQueue" class="headerlink" title="PriorityQueue"></a>PriorityQueue</h2><p>优先级队列，是0个或多个元素的集合，集合中的每个元素都有一个权重值，每次出队都弹出优先级最大或最小的元素。</p><h3 id="主要属性"><a href="#主要属性" class="headerlink" title="主要属性"></a>主要属性</h3><p>（1）默认容量是11；</p><p>（2）queue，元素存储在数组中，堆一般使用数组来存储；</p><p>（3）comparator，比较器，在优先级队列中，也有两种方式比较元素，一种是元素的自然顺序，一种是通过比较器来比较；</p><p>（4）modCount，修改次数，有这个属性表示PriorityQueue也是fast-fail的；</p><h3 id="入队"><a href="#入队" class="headerlink" title="入队"></a>入队</h3><p>（1）入队不允许null元素；</p><p>（2）如果数组不够用了，先扩容；</p><p>（3）如果还没有元素，就插入下标0的位置；</p><p>（4）如果有元素了，就插入到最后一个元素往后的一个位置（实际并没有插入哈）；</p><p>（5）自下而上堆化，一直往上跟父节点比较；</p><p>（6）如果比父节点小，就与父节点交换位置，直到出现比父节点大为止；</p><p>（7）由此可见，PriorityQueue是一个小顶堆。</p><h3 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h3><p>（1）当数组比较小（小于64）的时候每次扩容容量翻倍；</p><p>（2）当数组比较大的时候每次扩容只增加一半的容量；</p><h3 id="出队"><a href="#出队" class="headerlink" title="出队"></a>出队</h3><p>（1）将队列首元素弹出；</p><p>（2）将队列末元素移到队列首；</p><p>（3）自上而下堆化，一直往下与最小的子节点比较；</p><p>（4）如果比最小的子节点大，就交换位置，再继续与最小的子节点比较；</p><p>（5）如果比最小的子节点小，就不用交换位置了，堆化结束；</p><p>（6）这就是堆中的删除堆顶元素；</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>（1）PriorityQueue是一个小顶堆；</p><p>（2）PriorityQueue是非线程安全的；</p><p>（3）PriorityQueue不是有序的，只有堆顶存储着最小的元素；</p><p>（4）入队就是堆的插入元素的实现；</p><p>（5）出队就是堆的删除元素的实现；</p><h2 id="ArrayBlockingQueue"><a href="#ArrayBlockingQueue" class="headerlink" title="ArrayBlockingQueue"></a>ArrayBlockingQueue</h2><p>ArrayBlockingQueue是java并发包下一个以数组实现的阻塞队列，它是线程安全的</p><h3 id="主要属性-1"><a href="#主要属性-1" class="headerlink" title="主要属性"></a>主要属性</h3><p>（1）利用数组存储元素；</p><p>（2）通过放指针和取指针来标记下一次操作的位置；</p><p>（3）利用重入锁来保证并发安全；</p><h3 id="构造方法"><a href="#构造方法" class="headerlink" title="构造方法"></a>构造方法</h3><p>（1）ArrayBlockingQueue初始化时必须传入容量，也就是数组的大小；</p><p>（2）可以通过构造方法控制重入锁的类型是公平锁还是非公平锁；</p><h3 id="入队-1"><a href="#入队-1" class="headerlink" title="入队"></a>入队</h3><p>（1）add(e)时如果队列满了则抛出异常；</p><p>（2）offer(e)时如果队列满了则返回false；</p><p>（3）put(e)时如果队列满了则使用notFull等待；</p><p>（4）offer(e, timeout, unit)时如果队列满了则等待一段时间后如果队列依然满就返回false；</p><p>（5）利用放指针循环使用数组来存储元素；</p><h3 id="出队-1"><a href="#出队-1" class="headerlink" title="出队"></a>出队</h3><p>（1）remove()时如果队列为空则抛出异常；</p><p>（2）poll()时如果队列为空则返回null；</p><p>（3）take()时如果队列为空则阻塞等待在条件notEmpty上；</p><p>（4）poll(timeout, unit)时如果队列为空则阻塞等待一段时间后如果还为空就返回null；</p><p>（5）利用取指针循环从数组中取元素；</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>（1）ArrayBlockingQueue不需要扩容，因为是初始化时指定容量，并循环利用数组；</p><p>（2）ArrayBlockingQueue利用takeIndex和putIndex循环利用数组；</p><p>（3）入队和出队各定义了四组方法为满足不同的用途；</p><p>（4）利用重入锁和两个条件保证并发安全；</p><h3 id="ArrayBlockingQueue有哪些缺点呢？"><a href="#ArrayBlockingQueue有哪些缺点呢？" class="headerlink" title="ArrayBlockingQueue有哪些缺点呢？"></a>ArrayBlockingQueue有哪些缺点呢？</h3><p>a）队列长度固定且必须在初始化时指定，所以使用之前一定要慎重考虑好容量；</p><p>b）如果消费速度跟不上入队速度，则会导致提供者线程一直阻塞，且越阻塞越多，非常危险；</p><p>c）只使用了一个锁来控制入队出队，效率较低，可以借助分段的思想把入队出队分裂成两个锁。</p><h2 id="LinkedBlockingQueue"><a href="#LinkedBlockingQueue" class="headerlink" title="LinkedBlockingQueue"></a>LinkedBlockingQueue</h2><p>LinkedBlockingQueue是java并发包下一个以单链表实现的阻塞队列，它是线程安全的</p><h3 id="主要属性-2"><a href="#主要属性-2" class="headerlink" title="主要属性"></a>主要属性</h3><p>（1）capacity，有容量，可以理解为LinkedBlockingQueue是有界队列</p><p>（2）head, last，链表头、链表尾指针</p><p>（3）takeLock，notEmpty，take锁及其对应的条件</p><p>（4）putLock, notFull，put锁及其对应的条件</p><p>（5）入队、出队使用两个不同的锁控制，锁分离，提高效率</p><h3 id="入队-2"><a href="#入队-2" class="headerlink" title="入队"></a>入队</h3><p>（1）使用putLock加锁；</p><p>（2）如果队列满了就阻塞在notFull条件上；</p><p>（3）否则就入队；</p><p>（4）如果入队后元素数量小于容量，唤醒其它阻塞在notFull条件上的线程；</p><p>（5）释放锁；</p><p>（6）如果放元素之前队列长度为0，就唤醒notEmpty条件；</p><h3 id="出队-2"><a href="#出队-2" class="headerlink" title="出队"></a>出队</h3><p>（1）使用takeLock加锁；</p><p>（2）如果队列空了就阻塞在notEmpty条件上；</p><p>（3）否则就出队；</p><p>（4）如果出队前元素数量大于1，唤醒其它阻塞在notEmpty条件上的线程；</p><p>（5）释放锁；</p><p>（6）如果取元素之前队列长度等于容量，就唤醒notFull条件；</p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>（1）LinkedBlockingQueue采用单链表的形式实现；</p><p>（2）LinkedBlockingQueue采用两把锁的锁分离技术实现入队出队互不阻塞；</p><p>（3）LinkedBlockingQueue是有界队列，不传入容量时默认为最大int值；</p><h3 id="LinkedBlockingQueue与ArrayBlockingQueue对比？"><a href="#LinkedBlockingQueue与ArrayBlockingQueue对比？" class="headerlink" title="LinkedBlockingQueue与ArrayBlockingQueue对比？"></a>LinkedBlockingQueue与ArrayBlockingQueue对比？</h3><p>a）后者入队出队采用一把锁，导致入队出队相互阻塞，效率低下；</p><p>b）前才入队出队采用两把锁，入队出队互不干扰，效率较高；</p><p>c）二者都是有界队列，如果长度相等且出队速度跟不上入队速度，都会导致大量线程阻塞；</p><p>d）前者如果初始化不传入初始容量，则使用最大int值，如果出队速度跟不上入队速度，会导致队列特别长，占用大量内存；</p><h2 id="SynchronousQueue"><a href="#SynchronousQueue" class="headerlink" title="SynchronousQueue"></a>SynchronousQueue</h2><p>SynchronousQueue是java并发包下无缓冲阻塞队列，它用来在两个线程之间移交元素</p><h3 id="主要属性-3"><a href="#主要属性-3" class="headerlink" title="主要属性"></a>主要属性</h3><p>（1）这个阻塞队列里面是会自旋的；</p><p>（2）它使用了一个叫做transferer的东西来交换元素；</p><h3 id="主要内部类"><a href="#主要内部类" class="headerlink" title="主要内部类"></a>主要内部类</h3><p>（1）定义了一个抽象类Transferer，里面定义了一个传输元素的方法；</p><p>（2）有两种传输元素的方法，一种是栈，一种是队列；</p><p>（3）栈的特点是后进先出，队列的特点是先进行出；</p><p>（4）栈只需要保存一个头节点就可以了，因为存取元素都是操作头节点；</p><p>（5）队列需要保存一个头节点一个尾节点，因为存元素操作尾节点，取元素操作头节点；</p><p>（6）每个节点中保存着存储的元素、等待着的线程，以及下一个节点；</p><h3 id="构造方法-1"><a href="#构造方法-1" class="headerlink" title="构造方法"></a>构造方法</h3><p>（1）默认使用非公平模式，也就是栈结构；</p><p>（2）公平模式使用队列，非公平模式使用栈；</p><h3 id="入队-3"><a href="#入队-3" class="headerlink" title="入队"></a>入队</h3><p>调用transferer的transfer()方法，传入元素e，说明是生产者</p><h3 id="出队-3"><a href="#出队-3" class="headerlink" title="出队"></a>出队</h3><p>调用transferer的transfer()方法，传入null，说明是消费者。</p><h3 id="transferer"><a href="#transferer" class="headerlink" title="transferer"></a>transferer</h3><p>（1）如果栈中没有元素，或者栈顶元素跟将要入栈的元素模式一样，就入栈；</p><p>（2）入栈后自旋等待一会看有没有其它线程匹配到它，自旋完了还没匹配到元素就阻塞等待；</p><p>（3）阻塞等待被唤醒了说明其它线程匹配到了当前的元素，就返回匹配到的元素；</p><p>（4）如果两者模式不一样，且头节点没有在匹配中，就拿当前节点跟它匹配，匹配成功了就返回匹配到的元素；</p><p>（5）如果两者模式不一样，且头节点正在匹配中，当前线程就协助去匹配，匹配完成了再让当前节点重新入栈重新匹配；</p><h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><p>1）SynchronousQueue是java里的无缓冲队列，用于在两个线程之间直接移交元素；</p><p>（2）SynchronousQueue有两种实现方式，一种是公平（队列）方式，一种是非公平（栈）方式；</p><p>（3）栈方式中的节点有三种模式：生产者、消费者、正在匹配中；</p><p>（4）栈方式的大致思路是如果栈顶元素跟自己一样的模式就入栈并等待被匹配，否则就匹配，匹配到了就返回；</p><h3 id="SynchronousQueue真的是无缓冲的队列吗？"><a href="#SynchronousQueue真的是无缓冲的队列吗？" class="headerlink" title="SynchronousQueue真的是无缓冲的队列吗？"></a>SynchronousQueue真的是无缓冲的队列吗？</h3><p>通过源码分析，我们可以发现其实SynchronousQueue内部或者使用栈或者使用队列来存储包含线程和元素值的节点，如果同一个模式的节点过多的话，它们都会存储进来，且都会阻塞着，所以，严格上来说，SynchronousQueue并不能算是一个无缓冲队列。</p><h3 id="SynchronousQueue有什么缺点呢？"><a href="#SynchronousQueue有什么缺点呢？" class="headerlink" title="SynchronousQueue有什么缺点呢？"></a>SynchronousQueue有什么缺点呢？</h3><p>试想一下，如果有多个生产者，但只有一个消费者，如果消费者处理不过来，是不是生产者都会阻塞起来？反之亦然。</p><p>这是一件很危险的事，所以，SynchronousQueue一般用于生产、消费的速度大致相当的情况，这样才不会导致系统中过多的线程处于阻塞状态。</p><h2 id="PriorityBlockingQueue"><a href="#PriorityBlockingQueue" class="headerlink" title="PriorityBlockingQueue"></a>PriorityBlockingQueue</h2><p>PriorityBlockingQueue是java并发包下的优先级阻塞队列，它是线程安全的</p><h3 id="主要属性-4"><a href="#主要属性-4" class="headerlink" title="主要属性"></a>主要属性</h3><p>（1）依然是使用一个数组来使用元素；</p><p>（2）使用一个锁加一个notEmpty条件来保证并发安全；</p><p>（3）使用一个变量的CAS操作来控制扩容；</p><h3 id="入队-4"><a href="#入队-4" class="headerlink" title="入队"></a>入队</h3><p>入队的整个操作跟PriorityQueue几乎一致：</p><p>（1）加锁；</p><p>（2）判断是否需要扩容；</p><p>（3）添加元素并做自下而上的堆化；</p><p>（4）元素个数加1并唤醒notEmpty条件，唤醒取元素的线程；</p><p>（5）解锁；</p><h3 id="扩容-1"><a href="#扩容-1" class="headerlink" title="扩容"></a>扩容</h3><p>（1）解锁，解除offer()方法中加的锁；</p><p>（2）使用allocationSpinLock变量的CAS操作来控制扩容的过程；</p><p>（3）旧容量小于64则翻倍，旧容量大于64则增加一半；</p><p>（4）创建新数组；</p><p>（5）修改allocationSpinLock为0，相当于解锁；</p><p>（6）其它线程在扩容的过程中要让出CPU；</p><p>（7）再次加锁；</p><p>（8）新数组创建成功，把旧数组元素拷贝过来，并返回到offer()方法中继续添加元素操作；</p><h3 id="出队-4"><a href="#出队-4" class="headerlink" title="出队"></a>出队</h3><p>（1）加锁；</p><p>（2）判断是否出队成功，未成功就阻塞在notEmpty条件上；</p><p>（3）出队时弹出堆顶元素，并把堆尾元素拿到堆顶；</p><p>（4）再做自上而下的堆化；</p><p>（5）解锁；</p><h3 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h3><p>（1）PriorityBlockingQueue整个入队出队的过程与PriorityQueue基本是保持一致的；</p><p>（2）PriorityBlockingQueue使用一个锁+一个notEmpty条件控制并发安全；</p><p>（3）PriorityBlockingQueue扩容时使用一个单独变量的CAS操作来控制只有一个线程进行扩容；</p><p>（4）入队使用自下而上的堆化；</p><p>（5）出队使用自上而下的堆化；</p><ul><li><p>为什么PriorityBlockingQueue不需要notFull条件？</p></li><li><p>因为PriorityBlockingQueue在入队的时候如果没有空间了是会自动扩容的，也就不存在队列满了的状态，也就是不需要等待通知队列不满了可以放元素了，所以也就不需要notFull条件了。</p></li></ul><h2 id="LinkedTransferQueue"><a href="#LinkedTransferQueue" class="headerlink" title="LinkedTransferQueue"></a>LinkedTransferQueue</h2><p>LinkedTransferQueue是LinkedBlockingQueue、SynchronousQueue（公平模式）、ConcurrentLinkedQueue三者的集合体，它综合了这三者的方法，并且提供了更加高效的实现方式。</p><h3 id="继承体系"><a href="#继承体系" class="headerlink" title="继承体系"></a>继承体系</h3><p>LinkedTransferQueue实现了TransferQueue接口，而TransferQueue接口是继承自BlockingQueue的，所以LinkedTransferQueue也是一个阻塞队列。</p><h3 id="存储结构"><a href="#存储结构" class="headerlink" title="存储结构"></a>存储结构</h3><p>LinkedTransferQueue使用了一个叫做dual data structure的数据结构，或者叫做dual queue，译为双重数据结构或者双重队列。</p><p>双重队列是什么意思呢？</p><p>放取元素使用同一个队列，队列中的节点具有两种模式，一种是数据节点，一种是非数据节点。</p><p>放元素时先跟队列头节点对比，如果头节点是非数据节点，就让他们匹配，如果头节点是数据节点，就生成一个数据节点放在队列尾端（入队）。</p><p>取元素时也是先跟队列头节点对比，如果头节点是数据节点，就让他们匹配，如果头节点是非数据节点，就生成一个非数据节点放在队列尾端（入队）。<br>不管是放元素还是取元素，都先跟头节点对比，如果二者模式不一样就匹配它们，如果二者模式一样，就入队。</p><p>典型的单链表结构，内部除了存储元素的值和下一个节点的指针外，还包含了是否为数据节点和持有元素的线程。是无界的一个阻塞队列。</p><h3 id="总结-4"><a href="#总结-4" class="headerlink" title="总结"></a>总结</h3><p>（1）LinkedTransferQueue可以看作LinkedBlockingQueue、SynchronousQueue（公平模式）、ConcurrentLinkedQueue三者的集合体；</p><p>（2）LinkedTransferQueue的实现方式是使用一种叫做双重队列的数据结构；</p><p>（3）不管是取元素还是放元素都会入队；</p><p>（4）先尝试跟头节点比较，如果二者模式不一样，就匹配它们，组成CP，然后返回对方的值；</p><p>（5）如果二者模式一样，就入队，并自旋或阻塞等待被唤醒；</p><p>（6）至于是否入队及阻塞有四种模式，NOW、ASYNC、SYNC、TIMED；</p><p>（7）LinkedTransferQueue全程都没有使用synchronized、重入锁等比较重的锁，基本是通过 自旋+CAS 实现；</p><p>（8）对于入队之后，先自旋一定次数后再调用LockSupport.park()或LockSupport.parkNanos阻塞；</p><ul><li>LinkedTransferQueue与SynchronousQueue（公平模式）有什么异同呢？</li></ul><p>（1）在java8中两者的实现方式基本一致，都是使用的双重队列；</p><p>（2）前者完全实现了后者，但比后者更灵活；</p><p>（3）后者不管放元素还是取元素，如果没有可匹配的元素，所在的线程都会阻塞；</p><p>（4）前者可以自己控制放元素是否需要阻塞线程，比如使用四个添加元素的方法就不会阻塞线程，只入队元素，使用transfer()会阻塞线程；</p><p>（5）取元素两者基本一样，都会阻塞等待有新的元素进入被匹配到；</p><h2 id="ConcurrentLinkedQueue"><a href="#ConcurrentLinkedQueue" class="headerlink" title="ConcurrentLinkedQueue"></a>ConcurrentLinkedQueue</h2><p>ConcurrentLinkedQueue只实现了Queue接口，并没有实现BlockingQueue接口，所以它不是阻塞队列，也不能用于线程池中，但是它是线程安全的，可用于多线程环境中。</p><h3 id="主要属性-5"><a href="#主要属性-5" class="headerlink" title="主要属性"></a>主要属性</h3><p>就这两个主要属性，一个头节点，一个尾节点。这是一个无界的单链表实现的队列。</p><h3 id="入队-5"><a href="#入队-5" class="headerlink" title="入队"></a>入队</h3><p>入队整个流程还是比较清晰的，这里有个前提是出队时会把出队的那个节点的next设置为节点本身。</p><p>（1）定位到链表尾部，尝试把新节点到后面；</p><p>（2）如果尾部变化了，则重新获取尾部，再重试；</p><h3 id="出队-5"><a href="#出队-5" class="headerlink" title="出队"></a>出队</h3><p>（1）定位到头节点，尝试更新其值为null；</p><p>（2）如果成功了，就成功出队；</p><p>（3）如果失败或者头节点变化了，就重新寻找头节点，并重试；</p><p>（4）整个出队过程没有一点阻塞相关的代码，所以出队的时候不会阻塞线程，没找到元素就返回null；</p><h3 id="总结-5"><a href="#总结-5" class="headerlink" title="总结"></a>总结</h3><p>（1）ConcurrentLinkedQueue不是阻塞队列；</p><p>（2）ConcurrentLinkedQueue不能用在线程池中；</p><p>（3）ConcurrentLinkedQueue使用（CAS+自旋）更新头尾节点控制出队入队操作；</p><ul><li>ConcurrentLinkedQueue与LinkedBlockingQueue对比？</li></ul><p>（1）两者都是线程安全的队列；</p><p>（2）两者都可以实现取元素时队列为空直接返回null，后者的poll()方法可以实现此功能；</p><p>（3）前者全程无锁，后者全部都是使用重入锁控制的；</p><p>（4）前者效率较高，后者效率较低；</p><p>（5）前者无法实现如果队列为空等待元素到来的操作；</p><p>（6）前者是非阻塞队列，后者是阻塞队列；</p><p>（7）前者无法用在线程池中，后者可以；</p><h2 id="DelayQueue"><a href="#DelayQueue" class="headerlink" title="DelayQueue"></a>DelayQueue</h2><p>DelayQueue是java并发包下的延时阻塞队列，常用于实现定时任务。</p><p>从继承体系可以看到，DelayQueue实现了BlockingQueue，所以它是一个阻塞队列。</p><p>另外，DelayQueue还组合了一个叫做Delayed的接口，DelayQueue中存储的所有元素必须实现Delayed接口。</p><p>Delayed是一个继承自Comparable的接口，并且定义了一个getDelay()方法，用于表示还有多少时间到期，到期了应返回小于等于0的数值。</p><h3 id="主要属性-6"><a href="#主要属性-6" class="headerlink" title="主要属性"></a>主要属性</h3><p>从属性我们可以知道，延时队列主要使用优先级队列来实现，并辅以重入锁和条件来控制并发安全。</p><p>因为优先级队列是无界的，所以这里只需要一个条件就可以了。</p><h3 id="入队-6"><a href="#入队-6" class="headerlink" title="入队"></a>入队</h3><p>（1）加锁；</p><p>（2）添加元素到优先级队列中；</p><p>（3）如果添加的元素是堆顶元素，就把leader置为空，并唤醒等待在条件available上的线程；</p><p>（4）解锁；</p><h3 id="出队-6"><a href="#出队-6" class="headerlink" title="出队"></a>出队</h3><p>（1）加锁；</p><p>（2）检查第一个元素，如果为空或者还没到期，就返回null；</p><p>（3）如果第一个元素到期了就调用poll()弹出第一个元素；</p><p>（4）解锁。</p><h3 id="总结-6"><a href="#总结-6" class="headerlink" title="总结"></a>总结</h3><p>（1）DelayQueue是阻塞队列；</p><p>（2）DelayQueue内部存储结构使用优先级队列；</p><p>（3）DelayQueue使用重入锁和条件来控制并发安全；</p><p>（4）DelayQueue常用于定时任务；</p><ul><li>java中的线程池实现定时任务是直接用的DelayQueue吗？</li></ul><p>当然不是，ScheduledThreadPoolExecutor中使用的是它自己定义的内部类DelayedWorkQueue，其实里面的实现逻辑基本都是一样的，只不过DelayedWorkQueue里面没有使用现在的PriorityQueue，而是使用数组又实现了一遍优先级队列，本质上没有什么区别。</p><h2 id="ArrayDeque"><a href="#ArrayDeque" class="headerlink" title="ArrayDeque"></a>ArrayDeque</h2><p>双端队列是一种特殊的队列，它的两端都可以进出元素，故而得名双端队列。</p><p>ArrayDeque是一种以数组方式实现的双端队列，它是非线程安全的。</p><p>通过继承体系可以看，ArrayDeque实现了Deque接口，Deque接口继承自Queue接口，它是对Queue的一种增强。</p><p>从属性我们可以看到，ArrayDeque使用数组存储元素，并使用头尾指针标识队列的头和尾，其最小容量是8。</p><p>通过构造方法，我们知道默认初始容量是16，最小容量是8。</p><h2 id="入队-7"><a href="#入队-7" class="headerlink" title="入队"></a>入队</h2><p>（1）入队有两种方式，从队列头或者从队列尾；</p><p>（2）如果容量不够了，直接扩大为两倍；</p><p>（3）通过取模的方式让头尾指针在数组范围内循环；</p><p>（4）x &amp; (len - 1) = x % len，使用&amp;的方式更快；</p><h3 id="出队-7"><a href="#出队-7" class="headerlink" title="出队"></a>出队</h3><p>（1）出队有两种方式，从队列头或者从队列尾；</p><p>（2）通过取模的方式让头尾指针在数组范围内循环；</p><p>（3）出队之后没有缩容哈哈^^</p><h3 id="总结-7"><a href="#总结-7" class="headerlink" title="总结"></a>总结</h3><p>（1）ArrayDeque是采用数组方式实现的双端队列；</p><p>（2）ArrayDeque的出队入队是通过头尾指针循环利用数组实现的；</p><p>（3）ArrayDeque容量不足时是会扩容的，每次扩容容量增加一倍；</p><p>（4）ArrayDeque可以直接作为栈使用；</p><h2 id="LinkedList"><a href="#LinkedList" class="headerlink" title="LinkedList"></a>LinkedList</h2><p>（1）LinkedList是一个以双链表实现的List；</p><p>（2）LinkedList还是一个双端队列，具有队列、双端队列、栈的特性；</p><p>（3）LinkedList在队列首尾添加、删除元素非常高效，时间复杂度为O(1)；</p><p>（4）LinkedList在中间添加、删除元素比较低效，时间复杂度为O(n)；</p><p>（5）LinkedList不支持随机访问，所以访问非队列首尾的元素比较低效；</p><p>（6）LinkedList在功能上等于ArrayList + ArrayDeque；</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;PriorityQueue&quot;&gt;&lt;a href=&quot;#PriorityQueue&quot; class=&quot;headerlink&quot; title=&quot;PriorityQueue&quot;&gt;&lt;/a&gt;PriorityQueue&lt;/h2&gt;&lt;p&gt;优先级队列，是0个或多个元素的集合，集合中的每个元素</summary>
      
    
    
    
    <category term="Java" scheme="http://example.com/categories/Java/"/>
    
    <category term="集合" scheme="http://example.com/categories/Java/%E9%9B%86%E5%90%88/"/>
    
    
    <category term="集合" scheme="http://example.com/tags/%E9%9B%86%E5%90%88/"/>
    
    <category term="Java" scheme="http://example.com/tags/Java/"/>
    
    <category term="Queue" scheme="http://example.com/tags/Queue/"/>
    
  </entry>
  
  <entry>
    <title>Java中的Set那些事</title>
    <link href="http://example.com/2022/04/03/Java%E4%B8%AD%E7%9A%84Set%E9%82%A3%E4%BA%9B%E4%BA%8B/"/>
    <id>http://example.com/2022/04/03/Java%E4%B8%AD%E7%9A%84Set%E9%82%A3%E4%BA%9B%E4%BA%8B/</id>
    <published>2022-04-03T07:31:00.000Z</published>
    <updated>2022-04-03T08:00:23.733Z</updated>
    
    <content type="html"><![CDATA[<h2 id="HashSet"><a href="#HashSet" class="headerlink" title="HashSet"></a>HashSet</h2><p>（1）HashSet内部使用HashMap的key存储元素，以此来保证元素不重复；</p><p>（2）HashSet是无序的，因为HashMap的key是无序的；</p><p>（3）HashSet中允许有一个null元素，因为HashMap允许key为null；</p><p>（4）HashSet是非线程安全的；</p><p>（5）HashSet是没有get()方法的；</p><pre><code> public class HashSet&lt;E&gt;        extends AbstractSet&lt;E&gt;        implements Set&lt;E&gt;, Cloneable, java.io.Serializable    &#123;        static final long serialVersionUID = -5024744406713321676L;            // 内部元素存储在HashMap中        private transient HashMap&lt;E,Object&gt; map;            // 虚拟元素，用来存到map元素的value中的，没有实际意义        private static final Object PRESENT = new Object();            // 空构造方法        public HashSet() &#123;            map = new HashMap&lt;&gt;();        &#125;            // 把另一个集合的元素全都添加到当前Set中        // 注意，这里初始化map的时候是计算了它的初始容量的        public HashSet(Collection&lt;? extends E&gt; c) &#123;            map = new HashMap&lt;&gt;(Math.max((int) (c.size()/.75f) + 1, 16));            addAll(c);        &#125;            // 指定初始容量和装载因子        public HashSet(int initialCapacity, float loadFactor) &#123;            map = new HashMap&lt;&gt;(initialCapacity, loadFactor);        &#125;            // 只指定初始容量        public HashSet(int initialCapacity) &#123;            map = new HashMap&lt;&gt;(initialCapacity);        &#125;            // LinkedHashSet专用的方法        // dummy是没有实际意义的, 只是为了跟上上面那个操持方法签名不同而已        HashSet(int initialCapacity, float loadFactor, boolean dummy) &#123;            map = new LinkedHashMap&lt;&gt;(initialCapacity, loadFactor);        &#125;            // 迭代器        public Iterator&lt;E&gt; iterator() &#123;            return map.keySet().iterator();        &#125;            // 元素个数        public int size() &#123;            return map.size();        &#125;            // 检查是否为空        public boolean isEmpty() &#123;            return map.isEmpty();        &#125;            // 检查是否包含某个元素        public boolean contains(Object o) &#123;            return map.containsKey(o);        &#125;            // 添加元素        public boolean add(E e) &#123;            return map.put(e, PRESENT)==null;        &#125;            // 删除元素        public boolean remove(Object o) &#123;            return map.remove(o)==PRESENT;        &#125;            // 清空所有元素        public void clear() &#123;            map.clear();        &#125;            // 克隆方法        @SuppressWarnings(&quot;unchecked&quot;)        public Object clone() &#123;            try &#123;                HashSet&lt;E&gt; newSet = (HashSet&lt;E&gt;) super.clone();                newSet.map = (HashMap&lt;E, Object&gt;) map.clone();                return newSet;            &#125; catch (CloneNotSupportedException e) &#123;                throw new InternalError(e);            &#125;        &#125;            // 序列化写出方法        private void writeObject(java.io.ObjectOutputStream s)            throws java.io.IOException &#123;            // 写出非static非transient属性            s.defaultWriteObject();                // 写出map的容量和装载因子            s.writeInt(map.capacity());            s.writeFloat(map.loadFactor());                // 写出元素个数            s.writeInt(map.size());                // 遍历写出所有元素            for (E e : map.keySet())                s.writeObject(e);        &#125;            // 序列化读入方法        private void readObject(java.io.ObjectInputStream s)            throws java.io.IOException, ClassNotFoundException &#123;            // 读入非static非transient属性            s.defaultReadObject();                // 读入容量, 并检查不能小于0            int capacity = s.readInt();            if (capacity &lt; 0) &#123;                throw new InvalidObjectException(&quot;Illegal capacity: &quot; +                                                 capacity);            &#125;                // 读入装载因子, 并检查不能小于等于0或者是NaN(Not a Number)            // java.lang.Float.NaN = 0.0f / 0.0f;            float loadFactor = s.readFloat();            if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) &#123;                throw new InvalidObjectException(&quot;Illegal load factor: &quot; +                                                 loadFactor);            &#125;                // 读入元素个数并检查不能小于0            int size = s.readInt();            if (size &lt; 0) &#123;                throw new InvalidObjectException(&quot;Illegal size: &quot; +                                                 size);            &#125;            // 根据元素个数重新设置容量            // 这是为了保证map有足够的容量容纳所有元素, 防止无意义的扩容            capacity = (int) Math.min(size * Math.min(1 / loadFactor, 4.0f),                    HashMap.MAXIMUM_CAPACITY);                // 再次检查某些东西, 不重要的代码忽视掉            SharedSecrets.getJavaOISAccess()                         .checkArray(s, Map.Entry[].class, HashMap.tableSizeFor(capacity));                // 创建map, 检查是不是LinkedHashSet类型            map = (((HashSet&lt;?&gt;)this) instanceof LinkedHashSet ?                   new LinkedHashMap&lt;E,Object&gt;(capacity, loadFactor) :                   new HashMap&lt;E,Object&gt;(capacity, loadFactor));                // 读入所有元素, 并放入map中            for (int i=0; i&lt;size; i++) &#123;                @SuppressWarnings(&quot;unchecked&quot;)                    E e = (E) s.readObject();                map.put(e, PRESENT);            &#125;        &#125;            // 可分割的迭代器, 主要用于多线程并行迭代处理时使用        public Spliterator&lt;E&gt; spliterator() &#123;            return new HashMap.KeySpliterator&lt;E,Object&gt;(map, 0, -1, 0, 0);        &#125;    &#125;</code></pre><h2 id="LinkedHashSet"><a href="#LinkedHashSet" class="headerlink" title="LinkedHashSet"></a>LinkedHashSet</h2><p>（1）LinkedHashSet的底层使用LinkedHashMap存储元素。</p><p>（2）LinkedHashSet是有序的，它是按照插入的顺序排序的。</p><ul><li>注意：LinkedHashSet是不支持按访问顺序对元素排序的，只能按插入顺序排序。<pre><code>  package java.util;    // LinkedHashSet继承自HashSet  public class LinkedHashSet&lt;E&gt;      extends HashSet&lt;E&gt;      implements Set&lt;E&gt;, Cloneable, java.io.Serializable &#123;        private static final long serialVersionUID = -2851667679971038690L;        // 传入容量和装载因子      public LinkedHashSet(int initialCapacity, float loadFactor) &#123;          super(initialCapacity, loadFactor, true);      &#125;        // 只传入容量, 装载因子默认为0.75      public LinkedHashSet(int initialCapacity) &#123;          super(initialCapacity, .75f, true);      &#125;        // 使用默认容量16, 默认装载因子0.75      public LinkedHashSet() &#123;          super(16, .75f, true);      &#125;        // 将集合c中的所有元素添加到LinkedHashSet中      // 好奇怪, 这里计算容量的方式又变了      // HashSet中使用的是Math.max((int) (c.size()/.75f) + 1, 16)      // 这一点有点不得其解, 是作者偷懒？      public LinkedHashSet(Collection&lt;? extends E&gt; c) &#123;          super(Math.max(2*c.size(), 11), .75f, true);          addAll(c);      &#125;        // 可分割的迭代器, 主要用于多线程并行迭代处理时使用      @Override      public Spliterator&lt;E&gt; spliterator() &#123;          return Spliterators.spliterator(this, Spliterator.DISTINCT | Spliterator.ORDERED);      &#125;  &#125;</code></pre></li></ul><h2 id="TreeSet"><a href="#TreeSet" class="headerlink" title="TreeSet"></a>TreeSet</h2><p>（1）TreeSet底层使用NavigableMap存储元素；</p><p>（2）TreeSet是有序的；</p><p>（3）TreeSet是非线程安全的；</p><p>（4）TreeSet实现了NavigableSet接口，而NavigableSet继承自SortedSet接口；</p><p>（5）TreeSet实现了SortedSet接口；</p><pre><code>package java.util;        // TreeSet实现了NavigableSet接口，所以它是有序的    public class TreeSet&lt;E&gt; extends AbstractSet&lt;E&gt;        implements NavigableSet&lt;E&gt;, Cloneable, java.io.Serializable    &#123;        // 元素存储在NavigableMap中        // 注意它不一定就是TreeMap        private transient NavigableMap&lt;E,Object&gt; m;            // 虚拟元素, 用来作为value存储在map中        private static final Object PRESENT = new Object();            // 直接使用传进来的NavigableMap存储元素        // 这里不是深拷贝,如果外面的map有增删元素也会反映到这里        // 而且, 这个方法不是public的, 说明只能给同包使用        TreeSet(NavigableMap&lt;E,Object&gt; m) &#123;            this.m = m;        &#125;            // 使用TreeMap初始化        public TreeSet() &#123;            this(new TreeMap&lt;E,Object&gt;());        &#125;            // 使用带comparator的TreeMap初始化        public TreeSet(Comparator&lt;? super E&gt; comparator) &#123;            this(new TreeMap&lt;&gt;(comparator));        &#125;            // 将集合c中的所有元素添加的TreeSet中        public TreeSet(Collection&lt;? extends E&gt; c) &#123;            this();            addAll(c);        &#125;            // 将SortedSet中的所有元素添加到TreeSet中        public TreeSet(SortedSet&lt;E&gt; s) &#123;            this(s.comparator());            addAll(s);        &#125;            // 迭代器        public Iterator&lt;E&gt; iterator() &#123;            return m.navigableKeySet().iterator();        &#125;            // 逆序迭代器        public Iterator&lt;E&gt; descendingIterator() &#123;            return m.descendingKeySet().iterator();        &#125;            // 以逆序返回一个新的TreeSet        public NavigableSet&lt;E&gt; descendingSet() &#123;            return new TreeSet&lt;&gt;(m.descendingMap());        &#125;            // 元素个数        public int size() &#123;            return m.size();        &#125;            // 判断是否为空        public boolean isEmpty() &#123;            return m.isEmpty();        &#125;            // 判断是否包含某元素        public boolean contains(Object o) &#123;            return m.containsKey(o);        &#125;            // 添加元素, 调用map的put()方法, value为PRESENT        public boolean add(E e) &#123;            return m.put(e, PRESENT)==null;        &#125;            // 删除元素        public boolean remove(Object o) &#123;            return m.remove(o)==PRESENT;        &#125;            // 清空所有元素        public void clear() &#123;            m.clear();        &#125;            // 添加集合c中的所有元素        public  boolean addAll(Collection&lt;? extends E&gt; c) &#123;            // 满足一定条件时直接调用TreeMap的addAllForTreeSet()方法添加元素            if (m.size()==0 &amp;&amp; c.size() &gt; 0 &amp;&amp;                c instanceof SortedSet &amp;&amp;                m instanceof TreeMap) &#123;                SortedSet&lt;? extends E&gt; set = (SortedSet&lt;? extends E&gt;) c;                TreeMap&lt;E,Object&gt; map = (TreeMap&lt;E, Object&gt;) m;                Comparator&lt;?&gt; cc = set.comparator();                Comparator&lt;? super E&gt; mc = map.comparator();                if (cc==mc || (cc != null &amp;&amp; cc.equals(mc))) &#123;                    map.addAllForTreeSet(set, PRESENT);                    return true;                &#125;            &#125;            // 不满足上述条件, 调用父类的addAll()通过遍历的方式一个一个地添加元素            return super.addAll(c);        &#125;            // 子set（NavigableSet中的方法）        public NavigableSet&lt;E&gt; subSet(E fromElement, boolean fromInclusive,                                      E toElement,   boolean toInclusive) &#123;            return new TreeSet&lt;&gt;(m.subMap(fromElement, fromInclusive,                                           toElement,   toInclusive));        &#125;            // 头set（NavigableSet中的方法）        public NavigableSet&lt;E&gt; headSet(E toElement, boolean inclusive) &#123;            return new TreeSet&lt;&gt;(m.headMap(toElement, inclusive));        &#125;            // 尾set（NavigableSet中的方法）        public NavigableSet&lt;E&gt; tailSet(E fromElement, boolean inclusive) &#123;            return new TreeSet&lt;&gt;(m.tailMap(fromElement, inclusive));        &#125;            // 子set（SortedSet接口中的方法）        public SortedSet&lt;E&gt; subSet(E fromElement, E toElement) &#123;            return subSet(fromElement, true, toElement, false);        &#125;            // 头set（SortedSet接口中的方法）        public SortedSet&lt;E&gt; headSet(E toElement) &#123;            return headSet(toElement, false);        &#125;            // 尾set（SortedSet接口中的方法）        public SortedSet&lt;E&gt; tailSet(E fromElement) &#123;            return tailSet(fromElement, true);        &#125;            // 比较器        public Comparator&lt;? super E&gt; comparator() &#123;            return m.comparator();        &#125;            // 返回最小的元素        public E first() &#123;            return m.firstKey();        &#125;            // 返回最大的元素        public E last() &#123;            return m.lastKey();        &#125;            // 返回小于e的最大的元素        public E lower(E e) &#123;            return m.lowerKey(e);        &#125;            // 返回小于等于e的最大的元素        public E floor(E e) &#123;            return m.floorKey(e);        &#125;            // 返回大于等于e的最小的元素        public E ceiling(E e) &#123;            return m.ceilingKey(e);        &#125;            // 返回大于e的最小的元素        public E higher(E e) &#123;            return m.higherKey(e);        &#125;            // 弹出最小的元素        public E pollFirst() &#123;            Map.Entry&lt;E,?&gt; e = m.pollFirstEntry();            return (e == null) ? null : e.getKey();        &#125;            public E pollLast() &#123;            Map.Entry&lt;E,?&gt; e = m.pollLastEntry();            return (e == null) ? null : e.getKey();        &#125;            // 克隆方法        @SuppressWarnings(&quot;unchecked&quot;)        public Object clone() &#123;            TreeSet&lt;E&gt; clone;            try &#123;                clone = (TreeSet&lt;E&gt;) super.clone();            &#125; catch (CloneNotSupportedException e) &#123;                throw new InternalError(e);            &#125;                clone.m = new TreeMap&lt;&gt;(m);            return clone;        &#125;            // 序列化写出方法        private void writeObject(java.io.ObjectOutputStream s)            throws java.io.IOException &#123;            // Write out any hidden stuff            s.defaultWriteObject();                // Write out Comparator            s.writeObject(m.comparator());                // Write out size            s.writeInt(m.size());                // Write out all elements in the proper order.            for (E e : m.keySet())                s.writeObject(e);        &#125;            // 序列化写入方法        private void readObject(java.io.ObjectInputStream s)            throws java.io.IOException, ClassNotFoundException &#123;            // Read in any hidden stuff            s.defaultReadObject();                // Read in Comparator            @SuppressWarnings(&quot;unchecked&quot;)                Comparator&lt;? super E&gt; c = (Comparator&lt;? super E&gt;) s.readObject();                // Create backing TreeMap            TreeMap&lt;E,Object&gt; tm = new TreeMap&lt;&gt;(c);            m = tm;                // Read in size            int size = s.readInt();                tm.readTreeSet(size, s, PRESENT);        &#125;            // 可分割的迭代器        public Spliterator&lt;E&gt; spliterator() &#123;            return TreeMap.keySpliteratorFor(m);        &#125;            // 序列化id        private static final long serialVersionUID = -2479143000061671589L;    &#125;    </code></pre><p>（1）我们知道TreeSet和LinkedHashSet都是有序的，那它们有何不同？</p><ul><li>LinkedHashSet并没有实现SortedSet接口，它的有序性主要依赖于LinkedHashMap的有序性，所以它的有序性是指按照插入顺序保证的有序性；而TreeSet实现了SortedSet接口，它的有序性主要依赖于NavigableMap的有序性，而NavigableMap又继承自SortedMap，这个接口的有序性是指按照key的自然排序保证的有序性，而key的自然排序又有两种实现方式，一种是key实现Comparable接口，一种是构造方法传入Comparator比较器。</li></ul><p>（2）TreeSet里面真的是使用TreeMap来存储元素的吗？</p><ul><li>我们知道TreeSet里面实际上是使用的NavigableMap来存储元素，虽然大部分时候这个map确实是TreeMap，但不是所有时候都是TreeMap。所以，TreeSet的底层不完全是使用TreeMap来实现的，更准确地说，应该是NavigableMap。</li></ul><h2 id="CopyOnWriteArraySet"><a href="#CopyOnWriteArraySet" class="headerlink" title="CopyOnWriteArraySet"></a>CopyOnWriteArraySet</h2><p>（1）CopyOnWriteArraySet是用Map实现的吗？</p><ul><li>CopyOnWriteArraySet底层是使用CopyOnWriteArrayList存储元素的，所以它并不是使用Map来存储元素的。</li></ul><p>（2）CopyOnWriteArraySet是有序的吗？</p><ul><li>是有序的</li></ul><p>（3）CopyOnWriteArraySet是并发安全的吗？</p><ul><li>因为底层是使用了CopyOnWriteArrayList，因此CopyOnWriteArraySet是并发安全的，而且是读写分离的。</li></ul><p>（4）CopyOnWriteArraySet以何种方式保证元素不重复？</p><ul><li>CopyOnWriteArrayList底层其实是一个数组，它是允许元素重复的。而CopyOnWriteArraySet通过调用其addIfAbsent来保证元素的不重复</li></ul><pre><code>    public class CopyOnWriteArraySet&lt;E&gt; extends AbstractSet&lt;E&gt;            implements java.io.Serializable &#123;        private static final long serialVersionUID = 5457747651344034263L;            // 内部使用CopyOnWriteArrayList存储元素        private final CopyOnWriteArrayList&lt;E&gt; al;            // 构造方法        public CopyOnWriteArraySet() &#123;            al = new CopyOnWriteArrayList&lt;E&gt;();        &#125;            // 将集合c中的元素初始化到CopyOnWriteArraySet中        public CopyOnWriteArraySet(Collection&lt;? extends E&gt; c) &#123;            if (c.getClass() == CopyOnWriteArraySet.class) &#123;                // 如果c是CopyOnWriteArraySet类型，说明没有重复元素，                // 直接调用CopyOnWriteArrayList的构造方法初始化                @SuppressWarnings(&quot;unchecked&quot;) CopyOnWriteArraySet&lt;E&gt; cc =                    (CopyOnWriteArraySet&lt;E&gt;)c;                al = new CopyOnWriteArrayList&lt;E&gt;(cc.al);            &#125;            else &#123;                // 如果c不是CopyOnWriteArraySet类型，说明有重复元素                // 调用CopyOnWriteArrayList的addAllAbsent()方法初始化                // 它会把重复元素排除掉                al = new CopyOnWriteArrayList&lt;E&gt;();                al.addAllAbsent(c);            &#125;        &#125;            // 获取元素个数        public int size() &#123;            return al.size();        &#125;            // 检查集合是否为空        public boolean isEmpty() &#123;            return al.isEmpty();        &#125;            // 检查是否包含某个元素        public boolean contains(Object o) &#123;            return al.contains(o);        &#125;            // 集合转数组        public Object[] toArray() &#123;            return al.toArray();        &#125;            // 集合转数组，这里是可能有bug的，详情见ArrayList中分析        public &lt;T&gt; T[] toArray(T[] a) &#123;            return al.toArray(a);        &#125;            // 清空所有元素        public void clear() &#123;            al.clear();        &#125;            // 删除元素        public boolean remove(Object o) &#123;            return al.remove(o);        &#125;            // 添加元素        // 这里是调用CopyOnWriteArrayList的addIfAbsent()方法        // 它会检测元素不存在的时候才添加        // 还记得这个方法吗？当时有分析过的，建议把CopyOnWriteArrayList拿出来再看看        public boolean add(E e) &#123;            return al.addIfAbsent(e);        &#125;            // 是否包含c中的所有元素        public boolean containsAll(Collection&lt;?&gt; c) &#123;            return al.containsAll(c);        &#125;            // 并集        public boolean addAll(Collection&lt;? extends E&gt; c) &#123;            return al.addAllAbsent(c) &gt; 0;        &#125;            // 单方向差集        public boolean removeAll(Collection&lt;?&gt; c) &#123;            return al.removeAll(c);        &#125;            // 交集        public boolean retainAll(Collection&lt;?&gt; c) &#123;            return al.retainAll(c);        &#125;            // 迭代器        public Iterator&lt;E&gt; iterator() &#123;            return al.iterator();        &#125;            // equals()方法        public boolean equals(Object o) &#123;            // 如果两者是同一个对象，返回true            if (o == this)                return true;            // 如果o不是Set对象，返回false            if (!(o instanceof Set))                return false;            Set&lt;?&gt; set = (Set&lt;?&gt;)(o);            Iterator&lt;?&gt; it = set.iterator();                // 集合元素数组的快照            Object[] elements = al.getArray();            int len = elements.length;                // 我觉得这里的设计不太好            // 首先，Set中的元素本来就是不重复的，所以不需要再用个matched[]数组记录有没有出现过            // 其次，两个集合的元素个数如果不相等，那肯定不相等了，这个是不是应该作为第一要素先检查            boolean[] matched = new boolean[len];            int k = 0;            // 从o这个集合开始遍历            outer: while (it.hasNext()) &#123;                // 如果k&gt;len了，说明o中元素多了                if (++k &gt; len)                    return false;                // 取值                Object x = it.next();                // 遍历检查是否在当前集合中                for (int i = 0; i &lt; len; ++i) &#123;                    if (!matched[i] &amp;&amp; eq(x, elements[i])) &#123;                        matched[i] = true;                        continue outer;                    &#125;                &#125;                // 如果不在当前集合中，返回false                return false;            &#125;            return k == len;        &#125;            // 移除满足过滤条件的元素        public boolean removeIf(Predicate&lt;? super E&gt; filter) &#123;            return al.removeIf(filter);        &#125;            // 遍历元素        public void forEach(Consumer&lt;? super E&gt; action) &#123;            al.forEach(action);        &#125;            // 分割的迭代器        public Spliterator&lt;E&gt; spliterator() &#123;            return Spliterators.spliterator                (al.getArray(), Spliterator.IMMUTABLE | Spliterator.DISTINCT);        &#125;            // 比较两个元素是否相等        private static boolean eq(Object o1, Object o2) &#123;            return (o1 == null) ? o2 == null : o1.equals(o2);        &#125;    &#125;    </code></pre><h2 id="ConcurrentSkipListSet"><a href="#ConcurrentSkipListSet" class="headerlink" title="ConcurrentSkipListSet"></a>ConcurrentSkipListSet</h2><p>（1）ConcurrentSkipListSet的底层是ConcurrentSkipListMap吗？</p><ul><li>ConcurrentSkipListSet底层是通过ConcurrentNavigableMap来实现的，</li></ul><p>（2）ConcurrentSkipListSet是线程安全的吗？</p><ul><li>它是一个有序的线程安全的集合。</li></ul><p>（3）ConcurrentSkipListSet是有序的吗？</p><ul><li>有序的</li></ul><p>（4）ConcurrentSkipListSet和之前讲的Set有何不同？</p><ul><li>ConcurrentSkipListSet基本上都是使用ConcurrentSkipListMap实现的，虽然取子set部分是使用ConcurrentSkipListMap中的内部类，但是这些内部类其实也是和ConcurrentSkipListMap相关的，它们返回ConcurrentSkipListMap的一部分数据。</li></ul><pre><code>   // 实现了NavigableSet接口，并没有所谓的ConcurrentNavigableSet接口    public class ConcurrentSkipListSet&lt;E&gt;        extends AbstractSet&lt;E&gt;        implements NavigableSet&lt;E&gt;, Cloneable, java.io.Serializable &#123;            private static final long serialVersionUID = -2479143111061671589L;            // 存储使用的map        private final ConcurrentNavigableMap&lt;E,Object&gt; m;            // 初始化        public ConcurrentSkipListSet() &#123;            m = new ConcurrentSkipListMap&lt;E,Object&gt;();        &#125;            // 传入比较器        public ConcurrentSkipListSet(Comparator&lt;? super E&gt; comparator) &#123;            m = new ConcurrentSkipListMap&lt;E,Object&gt;(comparator);        &#125;            // 使用ConcurrentSkipListMap初始化map        // 并将集合c中所有元素放入到map中        public ConcurrentSkipListSet(Collection&lt;? extends E&gt; c) &#123;            m = new ConcurrentSkipListMap&lt;E,Object&gt;();            addAll(c);        &#125;            // 使用ConcurrentSkipListMap初始化map        // 并将有序Set中所有元素放入到map中        public ConcurrentSkipListSet(SortedSet&lt;E&gt; s) &#123;            m = new ConcurrentSkipListMap&lt;E,Object&gt;(s.comparator());            addAll(s);        &#125;            // ConcurrentSkipListSet类内部返回子set时使用的        ConcurrentSkipListSet(ConcurrentNavigableMap&lt;E,Object&gt; m) &#123;            this.m = m;        &#125;            // 克隆方法        public ConcurrentSkipListSet&lt;E&gt; clone() &#123;            try &#123;                @SuppressWarnings(&quot;unchecked&quot;)                ConcurrentSkipListSet&lt;E&gt; clone =                    (ConcurrentSkipListSet&lt;E&gt;) super.clone();                clone.setMap(new ConcurrentSkipListMap&lt;E,Object&gt;(m));                return clone;            &#125; catch (CloneNotSupportedException e) &#123;                throw new InternalError();            &#125;        &#125;            /* ---------------- Set operations -------------- */        // 返回元素个数        public int size() &#123;            return m.size();        &#125;            // 检查是否为空        public boolean isEmpty() &#123;            return m.isEmpty();        &#125;            // 检查是否包含某个元素        public boolean contains(Object o) &#123;            return m.containsKey(o);        &#125;            // 添加一个元素        // 调用map的putIfAbsent()方法        public boolean add(E e) &#123;            return m.putIfAbsent(e, Boolean.TRUE) == null;        &#125;            // 移除一个元素        public boolean remove(Object o) &#123;            return m.remove(o, Boolean.TRUE);        &#125;            // 清空所有元素        public void clear() &#123;            m.clear();        &#125;            // 迭代器        public Iterator&lt;E&gt; iterator() &#123;            return m.navigableKeySet().iterator();        &#125;            // 降序迭代器        public Iterator&lt;E&gt; descendingIterator() &#123;            return m.descendingKeySet().iterator();        &#125;                /* ---------------- AbstractSet Overrides -------------- */        // 比较相等方法        public boolean equals(Object o) &#123;            // Override AbstractSet version to avoid calling size()            if (o == this)                return true;            if (!(o instanceof Set))                return false;            Collection&lt;?&gt; c = (Collection&lt;?&gt;) o;            try &#123;                // 这里是通过两次两层for循环来比较                // 这里是有很大优化空间的，参考上篇文章CopyOnWriteArraySet中的彩蛋                return containsAll(c) &amp;&amp; c.containsAll(this);            &#125; catch (ClassCastException unused) &#123;                return false;            &#125; catch (NullPointerException unused) &#123;                return false;            &#125;        &#125;            // 移除集合c中所有元素        public boolean removeAll(Collection&lt;?&gt; c) &#123;            // Override AbstractSet version to avoid unnecessary call to size()            boolean modified = false;            for (Object e : c)                if (remove(e))                    modified = true;            return modified;        &#125;            /* ---------------- Relational operations -------------- */            // 小于e的最大元素        public E lower(E e) &#123;            return m.lowerKey(e);        &#125;            // 小于等于e的最大元素        public E floor(E e) &#123;            return m.floorKey(e);        &#125;            // 大于等于e的最小元素        public E ceiling(E e) &#123;            return m.ceilingKey(e);        &#125;            // 大于e的最小元素        public E higher(E e) &#123;            return m.higherKey(e);        &#125;            // 弹出最小的元素        public E pollFirst() &#123;            Map.Entry&lt;E,Object&gt; e = m.pollFirstEntry();            return (e == null) ? null : e.getKey();        &#125;            // 弹出最大的元素        public E pollLast() &#123;            Map.Entry&lt;E,Object&gt; e = m.pollLastEntry();            return (e == null) ? null : e.getKey();        &#125;                /* ---------------- SortedSet operations -------------- */            // 取比较器        public Comparator&lt;? super E&gt; comparator() &#123;            return m.comparator();        &#125;            // 最小的元素        public E first() &#123;            return m.firstKey();        &#125;            // 最大的元素        public E last() &#123;            return m.lastKey();        &#125;            // 取两个元素之间的子set        public NavigableSet&lt;E&gt; subSet(E fromElement,                                      boolean fromInclusive,                                      E toElement,                                      boolean toInclusive) &#123;            return new ConcurrentSkipListSet&lt;E&gt;                (m.subMap(fromElement, fromInclusive,                          toElement,   toInclusive));        &#125;            // 取头子set        public NavigableSet&lt;E&gt; headSet(E toElement, boolean inclusive) &#123;            return new ConcurrentSkipListSet&lt;E&gt;(m.headMap(toElement, inclusive));        &#125;            // 取尾子set        public NavigableSet&lt;E&gt; tailSet(E fromElement, boolean inclusive) &#123;            return new ConcurrentSkipListSet&lt;E&gt;(m.tailMap(fromElement, inclusive));        &#125;            // 取子set，包含from，不包含to        public NavigableSet&lt;E&gt; subSet(E fromElement, E toElement) &#123;            return subSet(fromElement, true, toElement, false);        &#125;            // 取头子set，不包含to        public NavigableSet&lt;E&gt; headSet(E toElement) &#123;            return headSet(toElement, false);        &#125;            // 取尾子set，包含from        public NavigableSet&lt;E&gt; tailSet(E fromElement) &#123;            return tailSet(fromElement, true);        &#125;            // 降序set        public NavigableSet&lt;E&gt; descendingSet() &#123;            return new ConcurrentSkipListSet&lt;E&gt;(m.descendingMap());        &#125;            // 可分割的迭代器        @SuppressWarnings(&quot;unchecked&quot;)        public Spliterator&lt;E&gt; spliterator() &#123;            if (m instanceof ConcurrentSkipListMap)                return ((ConcurrentSkipListMap&lt;E,?&gt;)m).keySpliterator();            else                return (Spliterator&lt;E&gt;)((ConcurrentSkipListMap.SubMap&lt;E,?&gt;)m).keyIterator();        &#125;            // 原子更新map，给clone方法使用        private void setMap(ConcurrentNavigableMap&lt;E,Object&gt; map) &#123;            UNSAFE.putObjectVolatile(this, mapOffset, map);        &#125;            // 原子操作相关内容        private static final sun.misc.Unsafe UNSAFE;        private static final long mapOffset;        static &#123;            try &#123;                UNSAFE = sun.misc.Unsafe.getUnsafe();                Class&lt;?&gt; k = ConcurrentSkipListSet.class;                mapOffset = UNSAFE.objectFieldOffset                    (k.getDeclaredField(&quot;m&quot;));            &#125; catch (Exception e) &#123;                throw new Error(e);            &#125;        &#125;    &#125;        </code></pre><h1 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h1><p> <img src="/images/pasted-174.png" alt="upload successful"></p><p>（1）除了HashSet其它Set都是有序的；</p><p>（2）实现了NavigableSet或者SortedSet接口的都是自然顺序的；</p><p>（3）使用并发安全的集合实现的Set也是并发安全的；</p><p>（4）TreeSet虽然不是全部都是使用的TreeMap实现的，但其实都是跟TreeMap相关的（TreeMap的子Map中组合了TreeMap）；</p><p>（5）ConcurrentSkipListSet虽然不是全部都是使用的ConcurrentSkipListMap实现的，但其实都是跟ConcurrentSkipListMap相关的（ConcurrentSkipListeMap的子Map中组合了ConcurrentSkipListMap）； </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;HashSet&quot;&gt;&lt;a href=&quot;#HashSet&quot; class=&quot;headerlink&quot; title=&quot;HashSet&quot;&gt;&lt;/a&gt;HashSet&lt;/h2&gt;&lt;p&gt;（1）HashSet内部使用HashMap的key存储元素，以此来保证元素不重复；&lt;/p&gt;
&lt;p&gt;（</summary>
      
    
    
    
    <category term="Java" scheme="http://example.com/categories/Java/"/>
    
    <category term="集合" scheme="http://example.com/categories/Java/%E9%9B%86%E5%90%88/"/>
    
    
    <category term="集合" scheme="http://example.com/tags/%E9%9B%86%E5%90%88/"/>
    
    <category term="Java" scheme="http://example.com/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>关于fail-safe</title>
    <link href="http://example.com/2022/04/01/%E5%85%B3%E4%BA%8Efail-safe/"/>
    <id>http://example.com/2022/04/01/%E5%85%B3%E4%BA%8Efail-safe/</id>
    <published>2022-04-01T14:00:00.000Z</published>
    <updated>2022-04-02T00:19:14.601Z</updated>
    
    <content type="html"><![CDATA[<p>Fail-Safe 迭代的出现，是为了解决fail-fast抛出异常处理不方便的情况。fail-safe是针对线程安全的集合类。</p><p>采⽤安全失败机制的集合容器，在遍历时不是直接在集合内容上访问的，⽽是先复制原有集合内容，在拷⻉的集合上进⾏遍历。所以，在遍历过程中对原集合所作的修改并不能被迭代器检测到，故不会抛ConcurrentModificationException 异常。</p><p>换句话说，并发容器的iterate方法返回的iterator对象，内部都是保存了该集合对象的一个快照副本，并且没有modCount等数值做检查。这也造成了并发容器的iterator读取的数据是某个时间点的快照版本。你可以并发读取，不会抛出异常，但是不保证你遍历读取的值和当前集合对象的状态是一致的！这就是安全失败的含义。</p><p>所以Fail-Safe 迭代的缺点是：首先是iterator不能保证返回集合更新后的数据，因为其工作在集合克隆上，而非集合本身。其次，创建集合拷贝需要相应的开销，包括时间和内存。</p><p>在java.util.concurrent 包中集合的迭代器，如 ConcurrentHashMap, CopyOnWriteArrayList等默认为都是Fail-Safe。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Fail-Safe 迭代的出现，是为了解决fail-fast抛出异常处理不方便的情况。fail-safe是针对线程安全的集合类。&lt;/p&gt;
&lt;p&gt;采⽤安全失败机制的集合容器，在遍历时不是直接在集合内容上访问的，⽽是先复制原有集合内容，在拷⻉的集合上进⾏遍历。所以，在遍历过程中</summary>
      
    
    
    
    <category term="集合" scheme="http://example.com/categories/%E9%9B%86%E5%90%88/"/>
    
    <category term="Java" scheme="http://example.com/categories/%E9%9B%86%E5%90%88/Java/"/>
    
    
    <category term="Java" scheme="http://example.com/tags/Java/"/>
    
    <category term="fail-safe" scheme="http://example.com/tags/fail-safe/"/>
    
  </entry>
  
  <entry>
    <title>Arrays.asList()避坑</title>
    <link href="http://example.com/2022/04/01/Arrays-asList-%E9%81%BF%E5%9D%91/"/>
    <id>http://example.com/2022/04/01/Arrays-asList-%E9%81%BF%E5%9D%91/</id>
    <published>2022-04-01T13:53:00.000Z</published>
    <updated>2022-04-01T13:59:09.226Z</updated>
    
    <content type="html"><![CDATA[<p>Arrays.asList() 我们可以使⽤它将⼀个数组转换为⼀个List集合。</p><p>jdk对这个方法的说明：返回由指定数组⽀持的固定⼤⼩的列表。此⽅法作为基于数组和基于集合的API之间的桥梁，与 Collection.toArray()结合使⽤。返回的List是可序列化并实现RandomAccess接⼝。</p><p>《阿⾥巴巴 Java 开发⼿册》对其的描述：Arrays.asList() 将数组转换为集合后,底层其实还是数组。强制使用add/remove/clear等方法会抛出异常。asList返回的对象是一个Arrays内部类，并没有实现集合的修改方法。Arrays.asList（）体现的是适配器模式，只是接口转换，后台的数据仍是数组。</p><p>传递的数组必须是对象数组，⽽不是基本类型。Arrays.asList() 是泛型⽅法，传⼊的对象必须是对象数组。</p><p> <img src="/images/pasted-173.png" alt="upload successful"></p><p>当传⼊⼀个原⽣数据类型数组时， Arrays.asList() 的真正得到的参数就不是数组中的元素，⽽是<br>数组对象本身！此时 List 的唯⼀元素就是这个数组，这也就解释了上⾯的代码。我们使用包装类可以解决该问题，但调用add/remove/clear等方法仍是会报错。</p><p>Arrays.asList() ⽅法返回的并不是 java.util.ArrayList ，⽽是 java.util.Arrays 的⼀个内部类,这个内部类并没有实现集合的修改⽅法或者说并没有重写这些⽅法。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Arrays.asList() 我们可以使⽤它将⼀个数组转换为⼀个List集合。&lt;/p&gt;
&lt;p&gt;jdk对这个方法的说明：返回由指定数组⽀持的固定⼤⼩的列表。此⽅法作为基于数组和基于集合的API之间的桥梁，与 Collection.toArray()结合使⽤。返回的List是</summary>
      
    
    
    
    <category term="Java" scheme="http://example.com/categories/Java/"/>
    
    <category term="集合" scheme="http://example.com/categories/Java/%E9%9B%86%E5%90%88/"/>
    
    
    <category term="集合" scheme="http://example.com/tags/%E9%9B%86%E5%90%88/"/>
    
    <category term="Java" scheme="http://example.com/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>内存模型之伪共享(False Sharing)</title>
    <link href="http://example.com/2022/03/23/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E4%B9%8B%E4%BC%AA%E5%85%B1%E4%BA%AB-False-Sharing/"/>
    <id>http://example.com/2022/03/23/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E4%B9%8B%E4%BC%AA%E5%85%B1%E4%BA%AB-False-Sharing/</id>
    <published>2022-03-23T13:09:00.000Z</published>
    <updated>2022-03-23T13:15:05.799Z</updated>
    
    <content type="html"><![CDATA[<p>在对称多处理器(SMP)系统中，每个处理器均有一个本地高速缓存。内存系统必须保证高速缓存的一致性。当不同处理器上的线程修改驻留在同一高速缓存行中的变量时就会发生假共享，结果导致高速缓存行无效，并强制执行更新，进而影响系统性能。</p><p> <img src="/images/pasted-172.png" alt="upload successful"><br>线程0和线程1会用到不同变量，它们在内存中彼此相邻，并驻留在同一高速缓存行。高速缓存行被加载到CPU0和CPU1的高速缓存中（灰色箭头）。<br>尽管这些线程修改的是不同变量（红色和蓝色箭头），高速缓存行仍会无效，并强制内存更新以维持高速缓存的一致性。</p><p>缓存系统中是以缓存行（cacheline）为单位存储的。缓存行是2的整数幂个连续字节，一般为32-256个字节。最常见的缓存行大小是64个字节。一个Java的long类型是8字节，因此在一个缓存行中可以存8个long类型的变量。所以，如果你访问一个long数组，当数组中的一个值被加载到缓存中，它会额外加载另外7个，这会带来一些优势。但是也有伪共享问题，比如两个线程，修改long数组的第一个与第七个，会频发发生缓存失效，影响性能。解决办法就是填充，在JDK8中提供了@sun.misc.Contended注解来避免伪共享，即通过padding填充，让数据占据不同的缓存行。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在对称多处理器(SMP)系统中，每个处理器均有一个本地高速缓存。内存系统必须保证高速缓存的一致性。当不同处理器上的线程修改驻留在同一高速缓存行中的变量时就会发生假共享，结果导致高速缓存行无效，并强制执行更新，进而影响系统性能。&lt;/p&gt;
&lt;p&gt; &lt;img src=&quot;/imag</summary>
      
    
    
    
    <category term="Java" scheme="http://example.com/categories/Java/"/>
    
    <category term="JMM" scheme="http://example.com/categories/Java/JMM/"/>
    
    
    <category term="Java" scheme="http://example.com/tags/Java/"/>
    
    <category term="JMM" scheme="http://example.com/tags/JMM/"/>
    
  </entry>
  
  <entry>
    <title>MySQL其他一下查询优化策略</title>
    <link href="http://example.com/2022/03/23/MySQL%E5%85%B6%E4%BB%96%E4%B8%80%E4%B8%8B%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5/"/>
    <id>http://example.com/2022/03/23/MySQL%E5%85%B6%E4%BB%96%E4%B8%80%E4%B8%8B%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5/</id>
    <published>2022-03-23T04:01:00.000Z</published>
    <updated>2022-03-23T04:09:30.098Z</updated>
    
    <content type="html"><![CDATA[<p>1、 in 和 exists的区别: 如果子查询得出的结果集记录较少，主查询中的表较大且又有索引时应该用in, 反之如果外层的主查询记录较少，子查询中的表大，又有索引时使用exists。其实我们区分in和exists主要是造成了驱动顺序的改变(这是性能变化的关键)，如果是exists，那么以外层表为驱动表，先被访问，如果是IN，那么先执行子查询，所以我们会以驱动表的快速返回为目标，那么就会考虑到索引及结果集的关系了 ，另外IN时不对NULL进行处理。</p><ul><li><p>in 是把外表和内表作hash 连接，而exists是对外表作loop循环，每次loop循环再对内表进行查询。一直以来认为exists比in效率高的说法是不准确的。</p></li><li><p>not in 和not exists：如果查询语句使用了not in 那么内外表都进行全表扫描，没有用到索引；而not extsts 的子查询依然能用到表上的索引。所以无论那个表大，用not exists都比not in要快。</p></li></ul><p>2、COUNT(*)与COUNT(具体字段)效率</p><ul><li>在表查询中，建议明确字段，不要使用 * 作为查询的字段列表，推荐使用SELECT &lt;字段列表&gt; 查询。原因：① MySQL 在解析的过程中，会通过 查询数据字典 将”*”按序转换成所有列名，这会大大的耗费资源和时间。② 无法使用 覆盖索引</li></ul><p>3、 LIMIT 1 对优化的影响</p><ul><li>针对的是会扫描全表的 SQL 语句，如果你可以确定结果集只有一条，那么加上 LIMIT 1 的时候，当找到一条结果的时候就不会继续扫描了，这样会加快查询速度。如果数据表已经对字段建立了唯一索引，那么可以通过索引进行查询，不会全表扫描的话，就不需要加上 LIMIT 1 了。</li></ul><p>4、多使用COMMIT</p><ul><li>只要有可能，在程序中尽量多使用 COMMIT，这样程序的性能得到提高，需求也会因为 COMMIT 所释放的资源而减少。COMMIT 所释放的资源：1、回滚段上用于恢复数据的信息2、被程序语句获得的锁 3、redo / undo log buffer 中的空间 4、管理上述 3 种资源中的内部花费</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;1、 in 和 exists的区别: 如果子查询得出的结果集记录较少，主查询中的表较大且又有索引时应该用in, 反之如果外层的主查询记录较少，子查询中的表大，又有索引时使用exists。其实我们区分in和exists主要是造成了驱动顺序的改变(这是性能变化的关键)，如果是e</summary>
      
    
    
    
    <category term="MySQL" scheme="http://example.com/categories/MySQL/"/>
    
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
    <category term="其它" scheme="http://example.com/tags/%E5%85%B6%E5%AE%83/"/>
    
    <category term="SQL优化" scheme="http://example.com/tags/SQL%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>change buffer的使用场景</title>
    <link href="http://example.com/2022/03/23/change-buffer%E7%9A%84%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/"/>
    <id>http://example.com/2022/03/23/change-buffer%E7%9A%84%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/</id>
    <published>2022-03-23T03:58:00.000Z</published>
    <updated>2022-03-23T03:59:55.566Z</updated>
    
    <content type="html"><![CDATA[<ol><li>普通索引和唯一索引应该怎么选择？其实，这两类索引在查询能力上是没差别的，主要考虑的是对 更新性能 的影响。所以，建议你 尽量选择普通索引 。 2. 在实际使用中会发现， 普通索引 和 change buffer 的配合使用，对于 数据量大 的表的更新优化还是很明显的。</li><li>如果所有的更新后面，都马上 伴随着对这个记录的查询 ，那么你应该 关闭change buffer 。而在其他情况下，change buffer都能提升更新性能。</li><li>由于唯一索引用不changebuffer的优化机制，因此如果 业务可以接受 ，从性能角度出发建议优先考虑非唯一索引。但是如果”业务可能无法确保”的情况下，怎么处理呢？</li></ol><p>首先， 业务正确性优先 。我们的前提是“业务代码已经保证不会写入重复数据”的情况下，讨论性能问题。如果业务不能保证，或者业务就是要求数据库来做约束，那么没得选，必须创建唯一索引。这种情况下，本节的意义在于，如果碰上了大量插入数据慢、内存命中率低的时候，给你多提供一个排查思路。</p><p>然后，在一些“ 归档库 ”的场景，你是可以考虑使用唯一索引的。比如，线上数据只需要保留半年，然后历史数据保存在归档库。这时候，归档数据已经是确保没有唯一键冲突了。要提高归档效率，可以考虑把表里面的唯一索引改成普通索引。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;ol&gt;
&lt;li&gt;普通索引和唯一索引应该怎么选择？其实，这两类索引在查询能力上是没差别的，主要考虑的是对 更新性能 的影响。所以，建议你 尽量选择普通索引 。 2. 在实际使用中会发现， 普通索引 和 change buffer 的配合使用，对于 数据量大 的表的更新优化还是很明</summary>
      
    
    
    
    <category term="MySQL" scheme="http://example.com/categories/MySQL/"/>
    
    
    <category term="change buffer" scheme="http://example.com/tags/change-buffer/"/>
    
  </entry>
  
  <entry>
    <title>索引下推</title>
    <link href="http://example.com/2022/03/23/%E7%B4%A2%E5%BC%95%E4%B8%8B%E6%8E%A8/"/>
    <id>http://example.com/2022/03/23/%E7%B4%A2%E5%BC%95%E4%B8%8B%E6%8E%A8/</id>
    <published>2022-03-23T03:50:00.000Z</published>
    <updated>2022-03-23T03:55:43.730Z</updated>
    
    <content type="html"><![CDATA[<p>Index Condition Pushdown(ICP)是MySQL 5.6中新特性，是一种在存储引擎层使用索引过滤数据的一种优化方式。ICP可以减少存储引擎访问基表的次数以及MySQL服务器访问存储引擎的次数。</p><p>在不使用ICP索引扫描的过程：</p><ul><li>storage层：只将满足index key条件的索引记录对应的整行记录取出，返回给server层 </li><li>server 层：对返回的数据，使用后面的where条件过滤，直至返回最后一行。</li></ul><p> <img src="/images/pasted-171.png" alt="upload successful"></p><p> 使用ICP扫描的过程：</p><ul><li>storage层：首先将index key条件满足的索引记录区间确定，然后在索引上使用index filter进行过滤。将满足的index filter条件的索引记录才去回表取出整行记录返回server层。不满足index filter条件的索引记录丢弃，不回表、也不会返回server层。</li><li>server 层：对返回的数据，使用table filter条件做最后的过滤。</li></ul><p>使用前后的成本差别：使用前，存储层多返回了需要被index filter过滤掉的整行记录使用ICP后，直接就去掉了不满足index filter条件的记录，省去了他们回表和传递到server层的成本。ICP的 加速效果 取决于在存储引擎内通过 ICP筛选 掉的数据的比例。</p><p>ICP的使用条件：</p><p>① 只能用于二级索引(secondary index) </p><p>②explain显示的执行计划中type值（join 类型）为 range 、 ref 、 eq_ref 或者 ref_or_null 。 </p><p>③ 并非全部where条件都可以用ICP筛选，如果where条件的字段不在索引列中，还是要读取整表的记录<br>到server端做where过滤。</p><p>④ ICP可以用于MyISAM和InnnoDB存储引擎</p><p>⑤ MySQL 5.6版本的不支持分区表的ICP功能，5.7版本的开始支持。</p><p>⑥ 当SQL使用覆盖索引时，不支持ICP优化方法。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Index Condition Pushdown(ICP)是MySQL 5.6中新特性，是一种在存储引擎层使用索引过滤数据的一种优化方式。ICP可以减少存储引擎访问基表的次数以及MySQL服务器访问存储引擎的次数。&lt;/p&gt;
&lt;p&gt;在不使用ICP索引扫描的过程：&lt;/p&gt;
&lt;u</summary>
      
    
    
    
    <category term="MySQL" scheme="http://example.com/categories/MySQL/"/>
    
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
    <category term="索引下推" scheme="http://example.com/tags/%E7%B4%A2%E5%BC%95%E4%B8%8B%E6%8E%A8/"/>
    
  </entry>
  
  <entry>
    <title>优化分页查询</title>
    <link href="http://example.com/2022/03/23/%E4%BC%98%E5%8C%96%E5%88%86%E9%A1%B5%E6%9F%A5%E8%AF%A2/"/>
    <id>http://example.com/2022/03/23/%E4%BC%98%E5%8C%96%E5%88%86%E9%A1%B5%E6%9F%A5%E8%AF%A2/</id>
    <published>2022-03-23T02:56:00.000Z</published>
    <updated>2022-03-23T02:58:13.556Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>在索引上完成排序分页操作，最后根据主键关联回原表查询所需要的其他列内容。</p></li><li><p>该方案适用于主键自增的表，可以把Limit 查询转换成某个位置的查询 。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;p&gt;在索引上完成排序分页操作，最后根据主键关联回原表查询所需要的其他列内容。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;该方案适用于主键自增的表，可以把Limit 查询转换成某个位置的查询 。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</summary>
      
    
    
    
    <category term="MySQL" scheme="http://example.com/categories/MySQL/"/>
    
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
    <category term="分页查询" scheme="http://example.com/tags/%E5%88%86%E9%A1%B5%E6%9F%A5%E8%AF%A2/"/>
    
  </entry>
  
  <entry>
    <title>GROUP BY优化</title>
    <link href="http://example.com/2022/03/23/GROUP-BY%E4%BC%98%E5%8C%96/"/>
    <id>http://example.com/2022/03/23/GROUP-BY%E4%BC%98%E5%8C%96/</id>
    <published>2022-03-23T02:55:00.000Z</published>
    <updated>2022-03-23T02:56:43.112Z</updated>
    
    <content type="html"><![CDATA[<ul><li>group by 使用索引的原则几乎跟order by一致 ，group by 即使没有过滤条件用到索引，也可以直接使用索引。</li><li>group by 先排序再分组，遵照索引建的最佳左前缀法则</li><li>当无法使用索引列，增大 max_length_for_sort_data 和 sort_buffer_size 参数的设置</li><li>where效率高于having，能写在where限定的条件就不要写在having中了</li><li>减少使用order by，和业务沟通能不排序就不排序，或将排序放到程序端去做。Order by、group by、distinct这些语句较为耗费CPU，数据库的CPU资源是极其宝贵的。</li><li>包含了order by、group by、distinct这些查询的语句，where条件过滤出来的结果集请保持在1000行以内，否则SQL会很慢。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;group by 使用索引的原则几乎跟order by一致 ，group by 即使没有过滤条件用到索引，也可以直接使用索引。&lt;/li&gt;
&lt;li&gt;group by 先排序再分组，遵照索引建的最佳左前缀法则&lt;/li&gt;
&lt;li&gt;当无法使用索引列，增大 max_len</summary>
      
    
    
    
    <category term="MySQL" scheme="http://example.com/categories/MySQL/"/>
    
    
    <category term="GROUP BY优化" scheme="http://example.com/tags/GROUP-BY%E4%BC%98%E5%8C%96/"/>
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>排序优化</title>
    <link href="http://example.com/2022/03/23/%E6%8E%92%E5%BA%8F%E4%BC%98%E5%8C%96/"/>
    <id>http://example.com/2022/03/23/%E6%8E%92%E5%BA%8F%E4%BC%98%E5%8C%96/</id>
    <published>2022-03-23T02:47:00.000Z</published>
    <updated>2022-03-23T02:54:53.268Z</updated>
    
    <content type="html"><![CDATA[<p>在 WHERE 条件字段上加索引，但是为什么在 ORDER BY 字段上还要加索引呢？</p><p>优化建议：</p><ol><li>SQL 中，可以在 WHERE 子句和 ORDER BY 子句中使用索引，目的是在 WHERE 子句中 避免全表扫 描 ，在 ORDER BY 子句 避免使用 FileSort 排序 。当然，某些情况下全表扫描，或者 FileSort 排序不一定比索引慢。但总的来说，我们还是要避免，以提高查询效率。</li><li>尽量使用 Index 完成 ORDER BY 排序。如果 WHERE 和 ORDER BY 后面是相同的列就使用单索引列；如果不同就使用联合索引。</li><li>无法使用 Index 时，需要对 FileSort 方式进行调优。</li></ol><p> <img src="/images/pasted-169.png" alt="upload successful"></p><p> <img src="/images/pasted-170.png" alt="upload successful"></p><ol><li>两个索引同时存在，mysql自动选择最优的方案。（对于这个例子，mysql选idx_age_stuno_name）。但是， 随着数据量的变化，选择的索引也会随之变化的 。 </li><li>当【范围条件】和【group by 或者 order by】的字段出现二选一时，优先观察条件字段的过滤数量，如果过滤的数据足够多，而需要排序的数据并不多时，优先把索引放在范围字段上。反之，亦然。</li></ol><p>filesort：双路排序和单路排序</p><p>双路排序 （慢）<br>MySQL 4.1之前是使用双路排序 ，字面意思就是两次扫描磁盘，最终得到数据， 读取行指针和<br>order by列 ，对他们进行排序，然后扫描已经排序好的列表，按照列表中的值重新从列表中读取<br>对应的数据输出<br>从磁盘取排序字段，在buffer进行排序，再从 磁盘取其他字段 。<br>取一批数据，要对磁盘进行两次扫描，众所周知，IO是很耗时的，所以在mysql4.1之后，出现了第二种<br>改进的算法，就是单路排序。</p><p>单路排序 （快）<br>从磁盘读取查询需要的 所有列 ，按照order by列在buffer对它们进行排序，然后扫描排序后的列表进行输<br>出， 它的效率更快一些，避免了第二次读取数据。并且把随机IO变成了顺序IO，但是它会使用更多的空<br>间， 因为它把每一行都保存在内存中了。</p><p>优化策略</p><ol><li>尝试提高 sort_buffer_size 2. 尝试提高 max_length_for_sort_data </li><li>Order by 时select * 是一个大忌。最好只Query需要的字段。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在 WHERE 条件字段上加索引，但是为什么在 ORDER BY 字段上还要加索引呢？&lt;/p&gt;
&lt;p&gt;优化建议：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;SQL 中，可以在 WHERE 子句和 ORDER BY 子句中使用索引，目的是在 WHERE 子句中 避免全表扫 描 ，在 ORDE</summary>
      
    
    
    
    <category term="MySQL" scheme="http://example.com/categories/MySQL/"/>
    
    
    <category term="MySQL" scheme="http://example.com/tags/MySQL/"/>
    
    <category term="排序优化" scheme="http://example.com/tags/%E6%8E%92%E5%BA%8F%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
</feed>
