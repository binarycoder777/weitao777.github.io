<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-03-12T11:44:57.876Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>I/O 多路复⽤：select/poll/epoll</title>
    <link href="http://example.com/2022/03/12/I-O-%E5%A4%9A%E8%B7%AF%E5%A4%8D%E2%BD%A4%EF%BC%9Aselect-poll-epoll/"/>
    <id>http://example.com/2022/03/12/I-O-%E5%A4%9A%E8%B7%AF%E5%A4%8D%E2%BD%A4%EF%BC%9Aselect-poll-epoll/</id>
    <published>2022-03-12T11:22:00.000Z</published>
    <updated>2022-03-12T11:44:57.876Z</updated>
    
    <content type="html"><![CDATA[<p>最基本的 Socket 模型：要想客户端和服务器能在⽹络中通信，那必须得使⽤ Socket 编程，它是进程间通信⾥⽐较特别的⽅式，特别之处在于它是可以跨主机间通信。创建 Socket 的时候，可以指定⽹络层使⽤的是 IPv4 还是 IPv6，传输层使⽤的是 TCP 还是 UDP。</p><p>服务端⾸先调⽤ socket() 函数，创建⽹络协议为 IPv4，以及传输协议为 TCP 的 Socket ，接着调⽤<br>bind() 函数，给这个 Socket 绑定⼀个 IP 地址和端⼝。</p><p>绑定端⼝的⽬的：当内核收到 TCP 报⽂，通过 TCP 头⾥⾯的端⼝号，来找到我们的应⽤程序，然后把数据传递给我们。</p><p>绑定 IP 地址的⽬的：⼀台机器是可以有多个⽹卡的，每个⽹卡都有对应的 IP 地址，当绑定⼀个⽹卡时，内核在收到该⽹卡上的包，才会发给我们；</p><p>绑定完 IP 地址和端⼝后，就可以调⽤ listen() 函数进⾏监听，此时对应 TCP 状态图中的 listen ，如果<br>我们要判定服务器中⼀个⽹络程序有没有启动，可以通过 netstat 命令查看对应的端⼝号是否有被监听。</p><p>服务端进⼊了监听状态后，通过调⽤ accept() 函数，来从内核获取客户端的连接，如果没有客户端连<br>接，则会阻塞等待客户端连接的到来。</p><p>客户端在创建好 Socket 后，调⽤ connect() 函数发起连接，该函数的参数要指明服务端的 IP 地址和端⼝号，然后万众期待的 TCP 三次握⼿就开始了。</p><p>在 TCP 连接的过程中，服务器的内核实际上为每个 Socket 维护了两个队列：</p><p>⼀个是还没完全建⽴连接的队列，称为 TCP 半连接队列，这个队列都是没有完成三次握⼿的连接，</p><p>此时服务端处于 syn_rcvd 的状态；<br>⼀个是⼀件建⽴连接的队列，称为 TCP 全连接队列，这个队列都是完成了三次握⼿的连接，此时服务端处于 established 状态；</p><p>当 TCP 全连接队列不为空后，服务端的 accept() 函数，就会从内核中的 TCP 全连接队列⾥拿出⼀个已<br>经完成连接的 Socket 返回应⽤程序，后续数据传输都⽤这个 Socket。</p><p>（注意，监听的 Socket 和真正⽤来传数据的 Socket 是两个：<br>⼀个叫作监听 Socket；<br>⼀个叫作已连接 Socket；）</p><p>连接建⽴后，客户端和服务端就开始相互传输数据了，双⽅都可以通过 read() 和 write() 函数来读写数<br>据。</p><p> <img src="/images/pasted-155.png" alt="upload successful"></p><p> 基于 Linux ⼀切皆⽂件的理念，在内核中 Socket 也是以「⽂件」的形式存在的，也是有对应的⽂件<br>描述符。</p><p>上面提到的TCP Socket 调⽤流程是最简单、最基本的，它基本只能⼀对⼀通信，因为使⽤的是同步阻塞的⽅式，当服务端在还没处理完⼀个客户端的⽹络 I/O 时，或者 读写操作发⽣阻塞时，其他客户端是⽆法与服务端连接的。可如果我们服务器只能服务⼀个客户，那这样就太浪费资源了，于是我们要改进这个⽹络 I/O 模型，以⽀持更多的客户端。</p><p>服务器作为服务⽅，通常会在本地固定监听⼀个端⼝，等待客户端的连接。因此服务器的本地 IP 和端⼝是固定的，于是对于服务端 TCP 连接的四元组只有对端 IP 和端⼝是会变化的，所以最⼤ TCP 连接数 = 客户端 IP 数×客户端端⼝数。</p><p>对于 IPv4，客户端的 IP 数最多为 2 的 32 次⽅，客户端的端⼝数最多为 2 的 16 次⽅，也就是服务端单机<br>最⼤ TCP 连接数约为 2 的 48 次⽅。</p><p>这个理论值相当“丰满”，但是服务器肯定承载不了那么⼤的连接数，主要会受两个⽅⾯的限制：</p><p>⽂件描述符，Socket 实际上是⼀个⽂件，也就会对应⼀个⽂件描述符。在 Linux 下，单个进程打开的<br>⽂件描述符数是有限制的，没有经过修改的值⼀般都是 1024，不过我们可以通过 ulimit 增⼤⽂件描<br>述符的数⽬；</p><p>系统内存，每个 TCP 连接在内核中都有对应的数据结构，意味着每个连接都是会占⽤⼀定内存的；</p><p>基于最原始的阻塞⽹络 I/O， 如果服务器要⽀持多个客户端，其中⽐较传统的⽅式，就是使⽤多进程模型，也就是为每个客户端分配⼀个进程来处理请求。</p><p>服务器的主进程负责监听客户的连接，⼀旦与客户端连接完成，accept() 函数就会返回⼀个「已连接<br>Socket」，这时就通过 fork() 函数创建⼀个⼦进程，实际上就把⽗进程所有相关的东⻄都复制⼀份，包<br>括⽂件描述符、内存地址空间、程序计数器、执⾏的代码等。</p><p>这两个进程刚复制完的时候，⼏乎⼀摸⼀样。不过，会根据返回值来区分是⽗进程还是⼦进程，如果返回值是 0，则是⼦进程；如果返回值是其他的整数，就是⽗进程。</p><p>正因为⼦进程会复制⽗进程的⽂件描述符，于是就可以直接使⽤「已连接 Socket 」和客户端通信了，可以发现，⼦进程不需要关⼼「监听 Socket」，只需要关⼼「已连接 Socket」；⽗进程则相反，将客户<br>服务交给⼦进程来处理，因此⽗进程不需要关⼼「已连接 Socket」，只需要关⼼「监听 Socket」。</p><p>另外，当「⼦进程」退出时，实际上内核⾥还会保留该进程的⼀些信息，也是会占⽤内存的，如果不做好<br>“回收”⼯作，就会变成僵⼫进程，随着僵⼫进程越多，会慢慢耗尽我们的系统资源。</p><p>因此，⽗进程要“善后”好⾃⼰的孩⼦，怎么善后呢？那么有两种⽅式可以在⼦进程退出后回收资源，分别<br>是调⽤ wait() 和 waitpid() 函数。</p><p>这种⽤多个进程来应付多个客户端的⽅式，在应对 100 个客户端还是可⾏的，但是当客户端数量⾼达⼀万<br>时，肯定扛不住的，因为每产⽣⼀个进程，必会占据⼀定的系统资源，⽽且进程间上下⽂切换的“包袱”是<br>很重的，性能会⼤打折扣。</p><p>进程的上下⽂切换不仅包含了虚拟内存、栈、全局变量等⽤户空间的资源，还包括了内核堆栈、寄存器等<br>内核空间的资源。</p><p>既然进程间上下⽂切换的“包袱”很重，那我们就搞个⽐较轻量级的模型来应对多⽤户的请求 —— 多线程模型。</p><p>线程是运⾏在进程中的⼀个“逻辑流”，单进程中可以运⾏多个线程，同进程⾥的线程可以共享进程的部分<br>资源的，⽐如⽂件描述符列表、进程空间、代码、全局数据、堆、共享库等，这些共享些资源在上下⽂切<br>换时是不需要切换，⽽只需要切换线程的私有数据、寄存器等不共享的数据，因此同⼀个进程下的线程上<br>下⽂切换的开销要⽐进程⼩得多。</p><p>当服务器与客户端 TCP 完成连接后，通过 pthread_create() 函数创建线程，然后将「已连接 Socket」的<br>⽂件描述符传递给线程函数，接着在线程⾥和客户端进⾏通信，从⽽达到并发处理的⽬的。</p><p>如果每来⼀个连接就创建⼀个线程，线程运⾏完后，还得操作系统还得销毁线程，虽说线程切换的上写⽂<br>开销不⼤，但是如果频繁创建和销毁线程，系统开销也是不⼩的。</p><p>那么，我们可以使⽤线程池的⽅式来避免线程的频繁创建和销毁，所谓的线程池，就是提前创建若⼲个线<br>程，这样当由新连接建⽴时，将这个已连接的 Socket 放⼊到⼀个队列⾥，然后线程池⾥的线程负责从队列<br>中取出已连接 Socket 进程处理。</p><p>需要注意的是，这个队列是全局的，每个线程都会操作，为了避免多线程竞争，线程在操作这个队列前要<br>加锁。</p><p>上⾯基于进程或者线程模型的，其实还是有问题的。新到来⼀个 TCP 连接，就需要分配⼀个进程或者线<br>程，那么如果要达到 C10K，意味着要⼀台机器维护 1 万个连接，相当于要维护 1 万个进程/线程，操作系<br>统就算死扛也是扛不住的。</p><p>既然为每个请求分配⼀个进程/线程的⽅式不合适，那有没有可能只使⽤⼀个进程来维护多个 Socket 呢？<br>答案是有的，那就是 I/O 多路复⽤技术。</p><p>⼀个进程虽然任⼀时刻只能处理⼀个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1<br>秒内就可以处理上千个请求，把时间拉⻓来看，多个请求复⽤了⼀个进程，这就是多路复⽤，这种思想很<br>类似⼀个 CPU 并发多个进程，所以也叫做时分多路复⽤。</p><p>我们熟悉的 select/poll/epoll 内核提供给⽤户态的多路复⽤系统调⽤，进程可以通过⼀个系统调⽤函数从内<br>核中获取多个事件。</p><p>select/poll/epoll 是如何获取⽹络事件的呢？在获取事件时，先把所有连接（⽂件描述符）传给内核，再由<br>内核返回产⽣了事件的连接，然后在⽤户态中再处理这些连接对应的请求即可。</p><p>所以，对于 select 这种⽅式，需要进⾏ 2 次「遍历」⽂件描述符集合，⼀次是在内核态⾥，⼀个次是在⽤<br>户态⾥ ，⽽且还会发⽣ 2 次「拷⻉」⽂件描述符集合，先从⽤户空间传⼊内核空间，由内核修改后，再传<br>出到⽤户空间中。</p><p>select 使⽤固定⻓度的 BitsMap，表示⽂件描述符集合，⽽且所⽀持的⽂件描述符的个数是有限制的，在<br>Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最⼤值为 1024 ，只能监听 0~1023 的⽂件描述符。</p><p>poll 不再⽤ BitsMap 来存储所关注的⽂件描述符，取⽽代之⽤动态数组，以链表形式来组织，突破了<br>select 的⽂件描述符个数限制，当然还会受到系统⽂件描述符限制。</p><p>但是 poll 和 select 并没有太⼤的本质区别，都是使⽤「线性结构」存储进程关注的 Socket 集合，因此都<br>需要遍历⽂件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，⽽且也需要在⽤户态与内核<br>态之间拷⻉⽂件描述符集合，这种⽅式随着并发数上来，性能的损耗会呈指数级增⻓。</p><p>epoll 通过两个⽅⾯，很好解决了 select/poll 的问题。</p><p>第⼀点，epoll 在内核⾥使⽤红⿊树来跟踪进程所有待检测的⽂件描述字，把需要监控的 socket 通过<br>epoll_ctl() 函数加⼊内核中的红⿊树⾥，红⿊树是个⾼效的数据结构，增删查⼀般时间复杂度是<br>O(logn) ，通过对这棵⿊红树进⾏操作，这样就不需要像 select/poll 每次操作时都传⼊整个 socket 集<br>合，只需要传⼊⼀个待检测的 socket，减少了内核和⽤户空间⼤量的数据拷⻉和内存分配。</p><p>第⼆点， epoll 使⽤事件驱动的机制，内核⾥维护了⼀个链表来记录就绪事件，当某个 socket 有事件发⽣<br>时，通过回调函数内核会将其加⼊到这个就绪事件列表中，当⽤户调⽤ epoll_wait() 函数时，只会返回有<br>事件发⽣的⽂件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，⼤⼤提⾼了检测的效<br>率。</p><p> <img src="/images/pasted-156.png" alt="upload successful"></p><p> epoll 的⽅式即使监听的 Socket 数量越多的时候，效率不会⼤幅度降低，能够同时监听的 Socket 的数⽬<br>也⾮常的多了，上限就为系统定义的进程打开的最⼤⽂件描述符个数。因⽽，epoll 被称为解决 C10K 问<br>题的利器。</p><p>（注意：epoll_wait 返回时，对于就绪的事件，epoll使⽤的是共享内存的⽅式，<br>即⽤户态和内核态都指向了就绪链表，所以就避免了内存拷⻉消耗。<br>这是错的！看过 epoll 内核源码的都知道，压根就没有使⽤共享内存这个玩意。你可以从下⾯这份代码看<br>到， epoll_wait 实现的内核代码中调⽤了 __put_user 函数，这个函数就是将数据从内核拷⻉到⽤户空<br>间。）</p><p>epoll ⽀持两种事件触发模式，分别是边缘触发（edge-triggered，ET）和⽔平触发（level-triggered，<br>LT）。</p><p>使⽤边缘触发模式时，当被监控的 Socket 描述符上有可读事件发⽣时，服务器端只会从 epoll_wait<br>中苏醒⼀次，即使进程没有调⽤ read 函数从内核读取数据，也依然只苏醒⼀次，因此我们程序要保<br>证⼀次性将内核缓冲区的数据读取完；</p><p>使⽤⽔平触发模式时，当被监控的 Socket 上有可读事件发⽣时，服务器端不断地从 epoll_wait 中苏<br>醒，直到内核缓冲区数据被 read 函数读完才结束，⽬的是告诉我们有数据需要读取；</p><p>举个例⼦，你的快递被放到了⼀个快递箱⾥，如果快递箱只会通过短信通知你⼀次，即使你⼀直没有去<br>取，它也不会再发送第⼆条短信提醒你，这个⽅式就是边缘触发；如果快递箱发现你的快递没有被取出，<br>它就会不停地发短信通知你，直到你取出了快递，它才消停，这个就是⽔平触发的⽅式。</p><p>这就是两者的区别，⽔平触发的意思是只要满⾜事件的条件，⽐如内核中有数据需要读，就⼀直不断地把<br>这个事件传递给⽤户；⽽边缘触发的意思是只有第⼀次满⾜条件的时候才触发，之后就不会再传递同样的<br>事件了。</p><p>如果使⽤⽔平触发模式，当内核通知⽂件描述符可读写时，接下来还可以继续去检测它的状态，看它是否<br>依然可读或可写。所以在收到通知后，没必要⼀次执⾏尽可能多的读写操作。</p><p>如果使⽤边缘触发模式，I/O 事件发⽣时只会通知⼀次，⽽且我们不知道到底能读写多少数据，所以在收到<br>通知后应尽可能地读写数据，以免错失读写的机会。因此，我们会循环从⽂件描述符读写数据，那么如果<br>⽂件描述符是阻塞的，没有数据可读写时，进程会阻塞在读写函数那⾥，程序就没办法继续往下执⾏。所<br>以，边缘触发模式⼀般和⾮阻塞 I/O 搭配使⽤，程序会⼀直执⾏ I/O 操作，直到系统调⽤（如 read 和<br>write ）返回错误，错误类型为 EAGAIN 或 EWOULDBLOCK 。</p><p>⼀般来说，边缘触发的效率⽐⽔平触发的效率要⾼，因为边缘触发可以减少 epoll_wait 的系统调⽤次数，<br>系统调⽤也是有⼀定的开销的的，毕竟也存在上下⽂的切换。</p><p>select/poll 只有⽔平触发模式，epoll 默认的触发模式是⽔平触发，但是可以根据应⽤场景设置为边缘触发<br>模式。</p><p>另外，使⽤ I/O 多路复⽤时，最好搭配⾮阻塞 I/O ⼀起使⽤，简单点理解，就是多路复⽤ API 返回的事件并不⼀定可读写的，如果使⽤阻塞 I/O， 那么在调⽤read/write 时则会发⽣程序阻塞，因此最好搭配⾮阻塞 I/O，以便应对极少数的特殊情况。</p><p>最基础的 TCP 的 Socket 编程，它是阻塞 I/O 模型，基本上只能⼀对⼀通信，那为了服务更多的客户端，<br>我们需要改进⽹络 I/O 模型。</p><p>⽐较传统的⽅式是使⽤多进程/线程模型，每来⼀个客户端连接，就分配⼀个进程/线程，然后后续的读写都<br>在对应的进程/线程，这种⽅式处理 100 个客户端没问题，但是当客户端增⼤到 10000 个时，10000 个进<br>程/线程的调度、上下⽂切换以及它们占⽤的内存，都会成为瓶颈。</p><p>为了解决上⾯这个问题，就出现了 I/O 的多路复⽤，可以只在⼀个进程⾥处理多个⽂件的 I/O，Linux 下有<br>三种提供 I/O 多路复⽤的 API，分别是： select、poll、epoll。</p><p>select 和 poll 并没有本质区别，它们内部都是使⽤「线性结构」来存储进程关注的 Socket 集合。<br>在使⽤的时候，⾸先需要把关注的 Socket 集合通过 select/poll 系统调⽤从⽤户态拷⻉到内核态，然后由<br>内核检测事件，当有⽹络事件产⽣时，内核需要遍历进程关注 Socket 集合，找到对应的 Socket，并设置<br>其状态为可读/可写，然后把整个 Socket 集合从内核态拷⻉到⽤户态，⽤户态还要继续遍历整个 Socket 集<br>合找到可读/可写的 Socket，然后对其处理。</p><p>很明显发现，select 和 poll 的缺陷在于，当客户端越多，也就是 Socket 集合越⼤，Socket 集合的遍历和拷⻉会带来很⼤的开销，因此也很难应对 C10K。</p><p>epoll 是解决 C10K 问题的利器，通过两个⽅⾯解决了 select/poll 的问题。</p><p>epoll 在内核⾥使⽤「红⿊树」来关注进程所有待检测的 Socket，红⿊树是个⾼效的数据结构，增删<br>查⼀般时间复杂度是 O(logn)，通过对这棵⿊红树的管理，不需要像 select/poll 在每次操作时都传⼊<br>整个 Socket 集合，减少了内核和⽤户空间⼤量的数据拷⻉和内存分配。</p><p>epoll 使⽤事件驱动的机制，内核⾥维护了⼀个「链表」来记录就绪事件，只将有事件发⽣的 Socket<br>集合传递给应⽤程序，不需要像 select/poll 那样轮询扫描整个集合（包含有和⽆事件的 Socket ），<br>⼤⼤提⾼了检测的效率。</p><p>⽽且，epoll ⽀持边缘触发和⽔平触发的⽅式，⽽ select/poll 只⽀持⽔平触发，⼀般⽽⾔，边缘触发的⽅式<br>会⽐⽔平触发的效率⾼。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最基本的 Socket 模型：要想客户端和服务器能在⽹络中通信，那必须得使⽤ Socket 编程，它是进程间通信⾥⽐较特别的⽅式，特别之处在于它是可以跨主机间通信。创建 Socket 的时候，可以指定⽹络层使⽤的是 IPv4 还是 IPv6，传输层使⽤的是 TCP 还是 U</summary>
      
    
    
    
    <category term="I/O多路复用" scheme="http://example.com/categories/I-O%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/"/>
    
    
    <category term="Linux" scheme="http://example.com/tags/Linux/"/>
    
    <category term="IO" scheme="http://example.com/tags/IO/"/>
    
    <category term="I/O多路复用" scheme="http://example.com/tags/I-O%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/"/>
    
    <category term="select/epoll/poll" scheme="http://example.com/tags/select-epoll-poll/"/>
    
  </entry>
  
  <entry>
    <title>Kafka-Kraft 模式</title>
    <link href="http://example.com/2022/03/12/Kafka-Kraft-%E6%A8%A1%E5%BC%8F/"/>
    <id>http://example.com/2022/03/12/Kafka-Kraft-%E6%A8%A1%E5%BC%8F/</id>
    <published>2022-03-12T05:30:00.000Z</published>
    <updated>2022-03-12T05:32:23.547Z</updated>
    
    <content type="html"><![CDATA[<p> <img src="/images/pasted-154.png" alt="upload successful"></p><p> 左图为 Kafka 现有架构，元数据在 zookeeper 中，运行时动态选举 controller，由controller 进行 Kafka 集群管理。右图为 kraft 模式架构（实验性），不再依赖 zookeeper 集群，而是用三台 controller 节点代替 zookeeper，元数据保存在 controller 中，由 controller 直接进行 Kafka 集群管理。</p><p> 这样做的好处有以下几个：</p><p> ⚫ Kafka 不再依赖外部框架，而是能够独立运行； </p><p> ⚫ controller 管理集群时，不再需要从 zookeeper 中先读取数据，集群性能上升； </p><p> ⚫ 由于不依赖 zookeeper，集群扩展时不再受到 zookeeper 读写能力限制； </p><p> ⚫ controller 不再动态选举，而是由配置文件规定。这样我们可以有针对性的加强controller 节点的配置，而不是像以前一样对随机 controller 节点的高负载束手无策。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt; &lt;img src=&quot;/images/pasted-154.png&quot; alt=&quot;upload successful&quot;&gt;&lt;/p&gt;
&lt;p&gt; 左图为 Kafka 现有架构，元数据在 zookeeper 中，运行时动态选举 controller，由controller 进行 Kaf</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="Kraft模式" scheme="http://example.com/tags/Kraft%E6%A8%A1%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>Kafka数据积压（消费者如何提高吞吐量）</title>
    <link href="http://example.com/2022/03/12/Kafka%E6%95%B0%E6%8D%AE%E7%A7%AF%E5%8E%8B%EF%BC%88%E6%B6%88%E8%B4%B9%E8%80%85%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98%E5%90%9E%E5%90%90%E9%87%8F%EF%BC%89/"/>
    <id>http://example.com/2022/03/12/Kafka%E6%95%B0%E6%8D%AE%E7%A7%AF%E5%8E%8B%EF%BC%88%E6%B6%88%E8%B4%B9%E8%80%85%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98%E5%90%9E%E5%90%90%E9%87%8F%EF%BC%89/</id>
    <published>2022-03-12T05:28:00.000Z</published>
    <updated>2022-03-12T05:29:37.815Z</updated>
    
    <content type="html"><![CDATA[<p>1）如果是Kafka消费能力不足，则可以考虑增 加Topic的分区数，并且同时提升消费组的消费者<br>数量，消费者数 = 分区数。（两者缺一不可）</p><p>2）如果是下游的数据处理不及时：提高每批次拉取的数<br>量。批次拉取数据过少（拉取数据/处理时间 &lt; 生产速度），<br>使处理的数据小于生产的数据，也会造成数据积压。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;1）如果是Kafka消费能力不足，则可以考虑增 加Topic的分区数，并且同时提升消费组的消费者&lt;br&gt;数量，消费者数 = 分区数。（两者缺一不可）&lt;/p&gt;
&lt;p&gt;2）如果是下游的数据处理不及时：提高每批次拉取的数&lt;br&gt;量。批次拉取数据过少（拉取数据/处理时间 &amp;lt; </summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="数据积压" scheme="http://example.com/tags/%E6%95%B0%E6%8D%AE%E7%A7%AF%E5%8E%8B/"/>
    
    <category term="消费者" scheme="http://example.com/tags/%E6%B6%88%E8%B4%B9%E8%80%85/"/>
    
  </entry>
  
  <entry>
    <title>Kafka消费者漏消费和重复消费问题</title>
    <link href="http://example.com/2022/03/12/Kafka%E6%B6%88%E8%B4%B9%E8%80%85%E6%BC%8F%E6%B6%88%E8%B4%B9%E5%92%8C%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9%E9%97%AE%E9%A2%98/"/>
    <id>http://example.com/2022/03/12/Kafka%E6%B6%88%E8%B4%B9%E8%80%85%E6%BC%8F%E6%B6%88%E8%B4%B9%E5%92%8C%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9%E9%97%AE%E9%A2%98/</id>
    <published>2022-03-12T05:12:00.000Z</published>
    <updated>2022-03-12T05:27:39.970Z</updated>
    
    <content type="html"><![CDATA[<p>重复消费：已经消费了数据，但是 offset 没提交，下次还会消费到当前数据。</p><p>漏消费：先提交 offset 后消费，有可能会造成数据的漏消费。</p><p> <img src="/images/pasted-153.png" alt="upload successful"></p><p>如果想完成Consumer端的精准一次性消费（既不漏消费也不重复消费），那么需要Kafka消费端将消费过程和提交offset<br>过程做原子绑定。此时我们需要将Kafka的offset保存到支持事务的自定义介质（比 如MySQL）。</p><pre><code>参考：https://blog.csdn.net/qingqing7/article/details/80054281?spm=1001.2101.3001.6650.14&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-14.pc_relevant_default&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-14.pc_relevant_default&amp;utm_relevant_index=25</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;重复消费：已经消费了数据，但是 offset 没提交，下次还会消费到当前数据。&lt;/p&gt;
&lt;p&gt;漏消费：先提交 offset 后消费，有可能会造成数据的漏消费。&lt;/p&gt;
&lt;p&gt; &lt;img src=&quot;/images/pasted-153.png&quot; alt=&quot;upload suc</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="消费者" scheme="http://example.com/tags/%E6%B6%88%E8%B4%B9%E8%80%85/"/>
    
    <category term="漏消费" scheme="http://example.com/tags/%E6%BC%8F%E6%B6%88%E8%B4%B9/"/>
    
    <category term="重复消费" scheme="http://example.com/tags/%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9/"/>
    
  </entry>
  
  <entry>
    <title>Kafka消费者的offset提交</title>
    <link href="http://example.com/2022/03/12/Kafka%E6%B6%88%E8%B4%B9%E8%80%85%E7%9A%84offset%E6%8F%90%E4%BA%A4/"/>
    <id>http://example.com/2022/03/12/Kafka%E6%B6%88%E8%B4%B9%E8%80%85%E7%9A%84offset%E6%8F%90%E4%BA%A4/</id>
    <published>2022-03-12T05:00:00.000Z</published>
    <updated>2022-03-12T05:11:59.292Z</updated>
    
    <content type="html"><![CDATA[<p>offset偏移量表明了该消费者当前消费的数据到哪一步，其存储在系统主题_consumer_offset中（0.9版本之前是存在Zookeeper中），以key,value形式，每隔一段时间kafka都会对其Compact（即保留当前最新的数据）。</p><p>1、自动提交offset：为了能让我们专注于业务处理，Kafka提供了自动提交offset功能，通过参数</p><p>⚫ enable.auto.commit：是否开启自动提交offset功能，默认是true</p><p>⚫ auto.commit.interval.ms：自动提交offset的时间间隔，默认是5s</p><p>2、手动提交：自动提交固然遍历，但基于时间的提交，我们很难把握那个度，因此更多时候，我们可以选择手动提交。</p><p>1）同步提交：同步提交会阻塞当前线程，一直到成功为止，并且失败会自动重试</p><p>2）异步提交：异步提交则不会阻塞当前线程，且没有重试机制，可能提交失败。</p><p>两者都会将本次提交的一批数据最高偏移量提交。</p><p>指定offset消费：auto.offset.reset = earliest | latest | none 默认是 latest。</p><p>当kafka中没有初始偏移量（消费者组第一次消费）或服务器上不存在当前偏移量时（数据被删除）需要指定offset消费。</p><p>1）earliest：自动将偏移量重置为最早的偏移量，–from-beginning。</p><p>2）latest（默认值）：自动将偏移量重置为最新偏移量。</p><p>（3）none：如果未找到消费者组的先前偏移量，则向消费者抛出异常。</p><p>（4）任意指定 offset 位移开始消费</p><p>指定时间消费：在生产环境中，会遇到最近消费的几个小时数据异常，想重新按照时间消费。例如要求按照时间消费前一天的数据</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;offset偏移量表明了该消费者当前消费的数据到哪一步，其存储在系统主题_consumer_offset中（0.9版本之前是存在Zookeeper中），以key,value形式，每隔一段时间kafka都会对其Compact（即保留当前最新的数据）。&lt;/p&gt;
&lt;p&gt;1、自动提</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="消费者" scheme="http://example.com/tags/%E6%B6%88%E8%B4%B9%E8%80%85/"/>
    
    <category term="offset" scheme="http://example.com/tags/offset/"/>
    
  </entry>
  
  <entry>
    <title>Kafka消费者分区的分配以及再平衡</title>
    <link href="http://example.com/2022/03/12/Kafka%E6%B6%88%E8%B4%B9%E8%80%85%E5%88%86%E5%8C%BA%E7%9A%84%E5%88%86%E9%85%8D%E4%BB%A5%E5%8F%8A%E5%86%8D%E5%B9%B3%E8%A1%A1/"/>
    <id>http://example.com/2022/03/12/Kafka%E6%B6%88%E8%B4%B9%E8%80%85%E5%88%86%E5%8C%BA%E7%9A%84%E5%88%86%E9%85%8D%E4%BB%A5%E5%8F%8A%E5%86%8D%E5%B9%B3%E8%A1%A1/</id>
    <published>2022-03-12T04:47:00.000Z</published>
    <updated>2022-03-12T04:53:38.116Z</updated>
    
    <content type="html"><![CDATA[<p>一个consumer group中有多个consumer组成，一个 topic有多个partition组成，现在的问题是，到底由哪个consumer来消费哪个partition的数据。 </p><p>2、Kafka有四种主流的分区分配策略：<br> Range、RoundRobin、Sticky、CooperativeSticky。<br>可以通过配置参数partition.assignment.strategy，修改分区的分配策略。默认策略是Range + CooperativeSticky。Kafka可以同时使用多个分区分配策略。</p><p>1）Range 是对每个 topic 而言的。</p><p>首先对同一个 topic 里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。</p><p>假如现在有 7 个分区，3 个消费者，排序后的分区将会是0,1,2,3,4,5,6；消费者排序完之后将会是C0,C1,C2。例如，7/3 = 2 余 1 ，除不尽，那么 消费者 C0 便会多消费 1 个分区。 8/3=2余2，除不尽，那么C0和C1分别多消费一个。</p><p>通过 partitions数/consumer数 来决定每个消费者应该消费几个分区。如果除不尽，那么前面几个消费者将会多<br>消费 1 个分区。</p><p>注意：如果只是针对 1 个 topic 而言，C0消费者多消费1个分区影响不是很大。但是如果有 N 多个 topic，那么针对个 topic，消费者 C0都将多消费 1 个分区，topic越多，C0消 费的分区会比其他消费者明显多消费 N 个分区。容易产生数据倾斜！</p><p>（注意：说明：某个消费者挂掉后，消费者组需要按照超时时间 45s 来判断它是否退出，所以需要等待，时间到了 45s 后，判断它真的退出就会把任务分配给其他 broker 执行。）</p><p>2）RoundRobin 分区策略原理：</p><p>RoundRobin 针对集群中所有Topic而言。<br>RoundRobin 轮询分区策略，是把所有的 partition 和所有的<br>consumer 都列出来，然后按照 hashcode 进行排序，最后<br>通过轮询算法来分配 partition 给到各个消费者。</p><p>3） Sticky 以及再平衡：</p><p>粘性分区定义：可以理解为分配的结果带有“粘性的”。即在执行一次新的分配之前，<br>考虑上一次分配的结果，尽量少的调整分配的变动，可以节省大量的开销。<br>粘性分区是 Kafka 从 0.11.x 版本开始引入这种分配策略，首先会尽量均衡的放置分区<br>到消费者上面，在出现同一消费者组内消费者出现问题的时候，会尽量保持原有分配的分<br>区不变化。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;一个consumer group中有多个consumer组成，一个 topic有多个partition组成，现在的问题是，到底由哪个consumer来消费哪个partition的数据。 &lt;/p&gt;
&lt;p&gt;2、Kafka有四种主流的分区分配策略：&lt;br&gt; Range、Round</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="消费者" scheme="http://example.com/tags/%E6%B6%88%E8%B4%B9%E8%80%85/"/>
    
    <category term="分区分配策略" scheme="http://example.com/tags/%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/"/>
    
  </entry>
  
  <entry>
    <title>Kafka高效读写数据</title>
    <link href="http://example.com/2022/03/12/Kafka%E9%AB%98%E6%95%88%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE/"/>
    <id>http://example.com/2022/03/12/Kafka%E9%AB%98%E6%95%88%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE/</id>
    <published>2022-03-12T03:23:00.000Z</published>
    <updated>2022-03-12T03:24:21.616Z</updated>
    
    <content type="html"><![CDATA[<p>1）Kafka 本身是分布式集群，可以采用分区技术，并行度高</p><p>2）读数据采用稀疏索引，可以快速定位要消费的数据</p><p>3）顺序写磁盘（Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到 600M/s，而随机写只有 100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。）</p><p>4）页缓存 + 零拷贝技术</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;1）Kafka 本身是分布式集群，可以采用分区技术，并行度高&lt;/p&gt;
&lt;p&gt;2）读数据采用稀疏索引，可以快速定位要消费的数据&lt;/p&gt;
&lt;p&gt;3）顺序写磁盘（Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直追加到文件末端，为顺序写。官网有数</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="高效读写" scheme="http://example.com/tags/%E9%AB%98%E6%95%88%E8%AF%BB%E5%86%99/"/>
    
  </entry>
  
  <entry>
    <title>Kafka文件清理策略</title>
    <link href="http://example.com/2022/03/12/Kafka%E6%96%87%E4%BB%B6%E6%B8%85%E7%90%86%E7%AD%96%E7%95%A5/"/>
    <id>http://example.com/2022/03/12/Kafka%E6%96%87%E4%BB%B6%E6%B8%85%E7%90%86%E7%AD%96%E7%95%A5/</id>
    <published>2022-03-12T03:19:00.000Z</published>
    <updated>2022-03-12T03:23:07.944Z</updated>
    
    <content type="html"><![CDATA[<p>Kafka 中默认的日志保存时间为 7 天，可以通过调整如下参数修改保存时间。<br>⚫ log.retention.hours，最低优先级小时，默认 7 天。</p><p>⚫ log.retention.minutes，分钟。 </p><p>⚫ log.retention.ms，最高优先级毫秒。 </p><p>⚫log.retention.check.interval.ms，负责设置检查周期，默认 5 分钟。</p><p>对于超过设置事件的数据，有两种清楚策略，delete和Compact</p><p>1）delete 日志删除：将过期数据删除</p><p>⚫ log.cleanup.policy = delete 所有数据启用删除策略</p><p>（1）基于时间：默认打开。以 segment 中所有记录中的最大时间戳作为该文件时间戳。</p><p>（2）基于大小：默认关闭。超过设置的所有日志总大小，删除最早segment。log.retention.bytes，默认等于-1，表示无穷大。</p><p>2）compact 日志压缩</p><p>compact日志压缩：对于相同key的不同value值，只保留最后一个版本。</p><p>⚫ log.cleanup.policy = compact 所有数据启用压缩策略</p><p> <img src="/images/pasted-152.png" alt="upload successful"></p><p>压缩后的offset可能是不连续的，比如上图中没有6，当从这些offset消费消息时，将会拿到比这个offset大 的offset对应的消息，实际上会拿到offset为7的消息，并从这个位置开始消费。</p><p>这种策略只适合特殊场景，比如消息的key是用户ID，value是用户的资料，通过这种压缩策略，整个消息<br>集里就保存了所有用户最新的资料。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Kafka 中默认的日志保存时间为 7 天，可以通过调整如下参数修改保存时间。&lt;br&gt;⚫ log.retention.hours，最低优先级小时，默认 7 天。&lt;/p&gt;
&lt;p&gt;⚫ log.retention.minutes，分钟。 &lt;/p&gt;
&lt;p&gt;⚫ log.retenti</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="清楚策略" scheme="http://example.com/tags/%E6%B8%85%E6%A5%9A%E7%AD%96%E7%95%A5/"/>
    
  </entry>
  
  <entry>
    <title>Kafka文件存储机制</title>
    <link href="http://example.com/2022/03/12/Kafka%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6/"/>
    <id>http://example.com/2022/03/12/Kafka%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6/</id>
    <published>2022-03-12T03:16:00.000Z</published>
    <updated>2022-03-12T03:19:20.616Z</updated>
    
    <content type="html"><![CDATA[<p>Topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是Producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引机制， 将每个partition分为多个segment。每个segment包括：“.index”文件、“.log”文件和.timeindex等文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号，例如：first-0。</p><p> <img src="/images/pasted-150.png" alt="upload successful"></p><p> Log文件和Index文件详解：</p><p> <img src="/images/pasted-151.png" alt="upload successful"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是Producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，为防止log文件过大导致数据定位效率低下，Kafka采取了</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="文件存储" scheme="http://example.com/tags/%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"/>
    
  </entry>
  
  <entry>
    <title>Leader Partition 负载平衡</title>
    <link href="http://example.com/2022/03/12/Leader-Partition-%E8%B4%9F%E8%BD%BD%E5%B9%B3%E8%A1%A1/"/>
    <id>http://example.com/2022/03/12/Leader-Partition-%E8%B4%9F%E8%BD%BD%E5%B9%B3%E8%A1%A1/</id>
    <published>2022-03-12T03:11:00.000Z</published>
    <updated>2022-03-12T03:15:02.258Z</updated>
    
    <content type="html"><![CDATA[<p>正常情况下，Kafka本身会自动把Leader Partition均匀分散在各个机器上，来保证每台机器的读写吞吐量都是均匀的。但是如果某些broker宕机，会导致Leader Partition过于集中在其他少部分几台broker上，这会导致少数几台broker的读写请求压力过高，其他宕机的broker重启之后都是follower partition，读写请求很低，造成集群负载不均衡。</p><p>策略：</p><p>1、auto.leader.rebalance.enable，默认是true。（自动Leader Partition 平衡）</p><p>2、leader.imbalance.per.broker.percentage，默认是10%。每个broker允许的不平衡的leader的比率。如果每个broker超过了这个值，控制器会触发leader的平衡。</p><p>3、leader.imbalance.check.interval.seconds，默认值300秒。检查leader负载是否平衡的间隔时间。</p><p>例如：针对broker0节点，分区2的AR优先副本是0节点，但是0节点却不是Leader节点，所以不平衡数加1，AR副本总数是4，所以broker0节点不平衡率为1/4&gt;10%，需要再平衡。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;正常情况下，Kafka本身会自动把Leader Partition均匀分散在各个机器上，来保证每台机器的读写吞吐量都是均匀的。但是如果某些broker宕机，会导致Leader Partition过于集中在其他少部分几台broker上，这会导致少数几台broker的读写请求压</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="Leader" scheme="http://example.com/tags/Leader/"/>
    
    <category term="Partition" scheme="http://example.com/tags/Partition/"/>
    
  </entry>
  
  <entry>
    <title>Leader 和 Follower 故障处理细节</title>
    <link href="http://example.com/2022/03/12/Leader-%E5%92%8C-Follower-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E7%BB%86%E8%8A%82/"/>
    <id>http://example.com/2022/03/12/Leader-%E5%92%8C-Follower-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E7%BB%86%E8%8A%82/</id>
    <published>2022-03-12T03:06:00.000Z</published>
    <updated>2022-03-12T03:10:07.966Z</updated>
    
    <content type="html"><![CDATA[<p>LEO（Log End Offset）：每个副本的最后一个offset，LEO其实就是最新的offset + 1。</p><p>HW（High Watermark）：所有副本中最小的LEO 。</p><p>1）Follower故障：</p><p>（1） Follower发生故障后会被临时踢出ISR</p><p>（2） 这个期间Leader和Follower继续接收数据</p><p>（3）待该Follower恢复后，Follower会读取本地磁盘记录的<br>上次的HW，并将log文件高于HW的部分截取掉，从HW开始向Leader进行同步。</p><p>（4）等该Follower的LEO大于等于该Partition的HW，即<br>Follower追上Leader之后，就可以重新加入ISR了。</p><p>2）Leader故障：</p><p>（1） Leader发生故障之后，会从ISR中选出一个新的Leader</p><p>（2）为保证多个副本之间的数据一致性，其余的Follower会先将各自的log文件高于HW的部分截掉，然后从新的Leader同步数据。</p><p>注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;LEO（Log End Offset）：每个副本的最后一个offset，LEO其实就是最新的offset + 1。&lt;/p&gt;
&lt;p&gt;HW（High Watermark）：所有副本中最小的LEO 。&lt;/p&gt;
&lt;p&gt;1）Follower故障：&lt;/p&gt;
&lt;p&gt;（1） Followe</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="Kafka" scheme="http://example.com/tags/Kafka/"/>
    
    <category term="Leader和Follower故障" scheme="http://example.com/tags/Leader%E5%92%8CFollower%E6%95%85%E9%9A%9C/"/>
    
  </entry>
  
  <entry>
    <title>Kafka Broker总体工作流程</title>
    <link href="http://example.com/2022/03/12/Kafka-Broker%E6%80%BB%E4%BD%93%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/"/>
    <id>http://example.com/2022/03/12/Kafka-Broker%E6%80%BB%E4%BD%93%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/</id>
    <published>2022-03-12T03:00:00.000Z</published>
    <updated>2022-03-12T03:00:21.303Z</updated>
    
    <content type="html"><![CDATA[<p> <img src="/images/pasted-149.png" alt="upload successful"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt; &lt;img src=&quot;/images/pasted-149.png&quot; alt=&quot;upload successful&quot;&gt;&lt;/p&gt;
</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="Kafka" scheme="http://example.com/tags/Kafka/"/>
    
    <category term="工作流程" scheme="http://example.com/tags/%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper中存储的Kafka 信息</title>
    <link href="http://example.com/2022/03/12/Zookeeper%E4%B8%AD%E5%AD%98%E5%82%A8%E7%9A%84Kafka-%E4%BF%A1%E6%81%AF/"/>
    <id>http://example.com/2022/03/12/Zookeeper%E4%B8%AD%E5%AD%98%E5%82%A8%E7%9A%84Kafka-%E4%BF%A1%E6%81%AF/</id>
    <published>2022-03-12T02:52:00.000Z</published>
    <updated>2022-03-12T02:53:19.813Z</updated>
    
    <content type="html"><![CDATA[<p>在zookeeper的服务端存储的Kafka相关信息：</p><p>1）/kafka/brokers/ids [0,1,2] 记录有哪些服务器</p><p>2）/kafka/brokers/topics/first/partitions/0/state<br>{“leader”:1 ,”isr”:[1,0,2] } 记录谁是Leader，有哪些服务器可用</p><p>3）/kafka/controller<br>{“brokerid”:0}<br>辅助选举Leader</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在zookeeper的服务端存储的Kafka相关信息：&lt;/p&gt;
&lt;p&gt;1）/kafka/brokers/ids [0,1,2] 记录有哪些服务器&lt;/p&gt;
&lt;p&gt;2）/kafka/brokers/topics/first/partitions/0/state&lt;br&gt;{“lea</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    <category term="Zookeeper" scheme="http://example.com/categories/Kafka/Zookeeper/"/>
    
    
    <category term="Zookeeper" scheme="http://example.com/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Kafka数据乱序</title>
    <link href="http://example.com/2022/03/12/Kafka%E6%95%B0%E6%8D%AE%E4%B9%B1%E5%BA%8F/"/>
    <id>http://example.com/2022/03/12/Kafka%E6%95%B0%E6%8D%AE%E4%B9%B1%E5%BA%8F/</id>
    <published>2022-03-12T02:47:00.000Z</published>
    <updated>2022-03-12T02:50:26.550Z</updated>
    
    <content type="html"><![CDATA[<p>1）kafka在1.x版本之前保证数据单分区有序，条件如下：<br>max.in.flight.requests.per.connection=1（不需要考虑是否开启幂等性）。 </p><p>2）kafka在1.x及以后版本保证数据单分区有序，条件如下：</p><p>（1）未开启幂等性<br>max.in.flight.requests.per.connection需要设置为1。</p><p>（2）开启幂等性<br>max.in.flight.requests.per.connection需要设置小于等于5。 </p><p>原因说明：因为在kafka1.x以后，启用幂等后，kafka服务端会缓存producer发来的最近5个request的元数据，故无论如何，都可以保证最近5个request的数据都是有序的。</p><p> <img src="/images/pasted-148.png" alt="upload successful"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;1）kafka在1.x版本之前保证数据单分区有序，条件如下：&lt;br&gt;max.in.flight.requests.per.connection=1（不需要考虑是否开启幂等性）。 &lt;/p&gt;
&lt;p&gt;2）kafka在1.x及以后版本保证数据单分区有序，条件如下：&lt;/p&gt;
&lt;p&gt;（</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="Kafka" scheme="http://example.com/tags/Kafka/"/>
    
    <category term="数据有序" scheme="http://example.com/tags/%E6%95%B0%E6%8D%AE%E6%9C%89%E5%BA%8F/"/>
    
  </entry>
  
  <entry>
    <title>Kafka的生产者事务原理</title>
    <link href="http://example.com/2022/03/12/Kafka%E7%9A%84%E7%94%9F%E4%BA%A7%E8%80%85%E4%BA%8B%E5%8A%A1%E5%8E%9F%E7%90%86/"/>
    <id>http://example.com/2022/03/12/Kafka%E7%9A%84%E7%94%9F%E4%BA%A7%E8%80%85%E4%BA%8B%E5%8A%A1%E5%8E%9F%E7%90%86/</id>
    <published>2022-03-12T02:29:00.000Z</published>
    <updated>2022-03-12T02:33:52.261Z</updated>
    
    <content type="html"><![CDATA[<p>注意：开启事务，必须要开启幂等性。另外Procuder在使用事务功能前，必须先自定义一个唯一的transaction.id。有了transaction.id，即使客户端挂掉了，它重启后也能继续处理未完成的事务。</p><p> <img src="/images/pasted-147.png" alt="upload successful"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;注意：开启事务，必须要开启幂等性。另外Procuder在使用事务功能前，必须先自定义一个唯一的transaction.id。有了transaction.id，即使客户端挂掉了，它重启后也能继续处理未完成的事务。&lt;/p&gt;
&lt;p&gt; &lt;img src=&quot;/images/paste</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="Kafka" scheme="http://example.com/tags/Kafka/"/>
    
    <category term="事务" scheme="http://example.com/tags/%E4%BA%8B%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>Kafka保证生产者生产的数据不重复：幂等性+至少一次</title>
    <link href="http://example.com/2022/03/12/Kafka%E4%BF%9D%E8%AF%81%E7%94%9F%E4%BA%A7%E8%80%85%E7%94%9F%E4%BA%A7%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%8D%E9%87%8D%E5%A4%8D%EF%BC%9A%E5%B9%82%E7%AD%89%E6%80%A7-%E8%87%B3%E5%B0%91%E4%B8%80%E6%AC%A1/"/>
    <id>http://example.com/2022/03/12/Kafka%E4%BF%9D%E8%AF%81%E7%94%9F%E4%BA%A7%E8%80%85%E7%94%9F%E4%BA%A7%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%8D%E9%87%8D%E5%A4%8D%EF%BC%9A%E5%B9%82%E7%AD%89%E6%80%A7-%E8%87%B3%E5%B0%91%E4%B8%80%E6%AC%A1/</id>
    <published>2022-03-12T02:25:00.000Z</published>
    <updated>2022-03-12T02:34:13.739Z</updated>
    
    <content type="html"><![CDATA[<p>至少一次：ack级别设置为-1+分区副本大于等于2+ISR里面的应答最小副本大于等于2（保证数据不会丢失）</p><p>幂等性：指Producer不论向Broker发送多少次重复数据，Broker端都只会持久化一条，保证了不重复。（重复数据的判断标准：具有&lt;PID, Partition, SeqNumber&gt;相同主键的消息提交时，Broker只会持久化一条。其 中PID是Kafka每次重启都会分配一个新的；Partition 表示分区号；Sequence Number是单调自增的。）</p><p>因此幂等性只能保证的是在单分区单会话内不重复。</p><p>如何使用幂等性：开启参数 enable.idempotence 默认为 true，false 关闭。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;至少一次：ack级别设置为-1+分区副本大于等于2+ISR里面的应答最小副本大于等于2（保证数据不会丢失）&lt;/p&gt;
&lt;p&gt;幂等性：指Producer不论向Broker发送多少次重复数据，Broker端都只会持久化一条，保证了不重复。（重复数据的判断标准：具有&amp;lt;PID,</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    <category term="消息队列" scheme="http://example.com/categories/Kafka/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
    
    <category term="Kafka" scheme="http://example.com/tags/Kafka/"/>
    
    <category term="数据" scheme="http://example.com/tags/%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Spring Security认证过程</title>
    <link href="http://example.com/2022/03/11/Spring-Security%E8%AE%A4%E8%AF%81%E8%BF%87%E7%A8%8B/"/>
    <id>http://example.com/2022/03/11/Spring-Security%E8%AE%A4%E8%AF%81%E8%BF%87%E7%A8%8B/</id>
    <published>2022-03-11T13:54:00.000Z</published>
    <updated>2022-03-11T14:09:54.174Z</updated>
    
    <content type="html"><![CDATA[<p>我们知道Spring Security的核心就是认证和授权，但是具体它是如何进行认证和授权的呢？下面让我们来聊聊Spring Security的认证过程，具体步骤如下图所示：</p><p> <img src="/images/pasted-146.png" alt="upload successful"></p><p> 在开始之前，我们需要了解一下如下类：</p><p>AuthenticationManager核心验证器，该对象提供了认证方法的入口，接收一个Authentiation对象作为参数。</p><pre><code>      public interface AuthenticationManager &#123;        Authentication authenticate(Authentication authentication)                throws AuthenticationException;    &#125;    </code></pre><p>ProviderManager：它是 AuthenticationManager 的一个实现类，提供了基本的认证逻辑和方法；它包含了一个 List<AuthenticationProvider> 对象，通过 AuthenticationProvider 接口来扩展出不同的认证提供者(当Spring Security默认提供的实现类不能满足需求的时候可以扩展AuthenticationProvider 覆盖supports(Class&lt;?&gt; authentication)方法)；</p><p>具体验证逻辑：</p><p>AuthenticationManager 接收 Authentication 对象作为参数，并通过 authenticate(Authentication) 方法对其进行验证；AuthenticationProvider实现类用来支撑对 Authentication 对象的验证动作；UsernamePasswordAuthenticationToken实现了 Authentication主要是将用户输入的用户名和密码进行封装，并供给 AuthenticationManager 进行验证；验证完成以后将返回一个认证成功的 Authentication 对象；</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我们知道Spring Security的核心就是认证和授权，但是具体它是如何进行认证和授权的呢？下面让我们来聊聊Spring Security的认证过程，具体步骤如下图所示：&lt;/p&gt;
&lt;p&gt; &lt;img src=&quot;/images/pasted-146.png&quot; alt=&quot;up</summary>
      
    
    
    
    <category term="Spring Security" scheme="http://example.com/categories/Spring-Security/"/>
    
    
    <category term="认证" scheme="http://example.com/tags/%E8%AE%A4%E8%AF%81/"/>
    
  </entry>
  
  <entry>
    <title>Spring的AOP是在哪个阶段创建的动态代理？</title>
    <link href="http://example.com/2022/03/10/Spring%E7%9A%84AOP%E6%98%AF%E5%9C%A8%E5%93%AA%E4%B8%AA%E9%98%B6%E6%AE%B5%E5%88%9B%E5%BB%BA%E7%9A%84%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%EF%BC%9F/"/>
    <id>http://example.com/2022/03/10/Spring%E7%9A%84AOP%E6%98%AF%E5%9C%A8%E5%93%AA%E4%B8%AA%E9%98%B6%E6%AE%B5%E5%88%9B%E5%BB%BA%E7%9A%84%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%EF%BC%9F/</id>
    <published>2022-03-10T00:44:00.000Z</published>
    <updated>2022-03-10T00:47:08.519Z</updated>
    
    <content type="html"><![CDATA[<p>1、正常情况下会在bean的生命周期“初始化”后，通过BeanPostProcessor.postProcessAfterInitialization创建AOP的动态代理</p><p>2、特殊情况下，即存在循环依赖的时候，Bean会在生命周期的“属性注入”时，通过MergedBeanDefinitionPostProcessor.postProcessMergedBeanDefinition创建aop动态代理</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;1、正常情况下会在bean的生命周期“初始化”后，通过BeanPostProcessor.postProcessAfterInitialization创建AOP的动态代理&lt;/p&gt;
&lt;p&gt;2、特殊情况下，即存在循环依赖的时候，Bean会在生命周期的“属性注入”时，通过Merg</summary>
      
    
    
    
    <category term="Spring" scheme="http://example.com/categories/Spring/"/>
    
    
    <category term="AOP" scheme="http://example.com/tags/AOP/"/>
    
    <category term="面试题" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>什么情况下AOP会失效,怎么解决？</title>
    <link href="http://example.com/2022/03/10/%E4%BB%80%E4%B9%88%E6%83%85%E5%86%B5%E4%B8%8BAOP%E4%BC%9A%E5%A4%B1%E6%95%88-%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%EF%BC%9F/"/>
    <id>http://example.com/2022/03/10/%E4%BB%80%E4%B9%88%E6%83%85%E5%86%B5%E4%B8%8BAOP%E4%BC%9A%E5%A4%B1%E6%95%88-%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%EF%BC%9F/</id>
    <published>2022-03-10T00:37:00.000Z</published>
    <updated>2022-03-10T00:42:55.549Z</updated>
    
    <content type="html"><![CDATA[<p>1、方法是private</p><p>2、目标类没有配置为Bean</p><p>3、切点表达式没有写正确</p><p>4、jdk动态代理下内部调用不会触发AOP（</p><p>原因：</p><p>内部进行自调用，是走的实例对象，而不是代理对象。</p><p>解决：</p><p>1、在本类中自动注入当前的bean</p><p>2、@EnableAspectJAutoProxy(exposProxy = true)</p><p>设置暴露当前代理对象到本地线程，可以通过AopContent.currentProxy()拿到当前的动态代理对象。<br>）</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;1、方法是private&lt;/p&gt;
&lt;p&gt;2、目标类没有配置为Bean&lt;/p&gt;
&lt;p&gt;3、切点表达式没有写正确&lt;/p&gt;
&lt;p&gt;4、jdk动态代理下内部调用不会触发AOP（&lt;/p&gt;
&lt;p&gt;原因：&lt;/p&gt;
&lt;p&gt;内部进行自调用，是走的实例对象，而不是代理对象。&lt;/p&gt;
&lt;p&gt;解决</summary>
      
    
    
    
    <category term="Spring" scheme="http://example.com/categories/Spring/"/>
    
    
    <category term="AOP" scheme="http://example.com/tags/AOP/"/>
    
    <category term="面试题" scheme="http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AOP有几种实现方式 </title>
    <link href="http://example.com/2022/03/10/AOP%E6%9C%89%E5%87%A0%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F/"/>
    <id>http://example.com/2022/03/10/AOP%E6%9C%89%E5%87%A0%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F/</id>
    <published>2022-03-10T00:34:00.000Z</published>
    <updated>2022-03-10T00:36:27.821Z</updated>
    
    <content type="html"><![CDATA[<p>1、Spring 1.2 基于接口的配置：最早的 Spring AOP 是完全基于几个接口的，想看源码的同学可以从这里起步。</p><p>2、Spring 2.0 schema-based 配置：Spring 2.0 以后使用 XML 的方式来配置，使用 命名空间 <aop ></aop></p><p>3、Spring 2.0 @AspectJ 配置：使用注解的方式来配置，这种方式感觉是最方便的，还有，这里虽然叫<br>做 @AspectJ，但是这个和 AspectJ 其实没啥关系。</p><p>4、AspectJ  方式，这种方式其实和Spring没有关系，采用AspectJ 进行动态织入的方式实现AOP，需要用<br>AspectJ 单独编译。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;1、Spring 1.2 基于接口的配置：最早的 Spring AOP 是完全基于几个接口的，想看源码的同学可以从这里起步。&lt;/p&gt;
&lt;p&gt;2、Spring 2.0 schema-based 配置：Spring 2.0 以后使用 XML 的方式来配置，使用 命名空间 &lt;ao</summary>
      
    
    
    
    <category term="Spring" scheme="http://example.com/categories/Spring/"/>
    
    
    <category term="AOP" scheme="http://example.com/tags/AOP/"/>
    
  </entry>
  
</feed>
