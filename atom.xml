<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-03-13T05:18:08.769Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>时间轮算法</title>
    <link href="http://example.com/2022/03/13/%E6%97%B6%E9%97%B4%E8%BD%AE%E7%AE%97%E6%B3%95/"/>
    <id>http://example.com/2022/03/13/%E6%97%B6%E9%97%B4%E8%BD%AE%E7%AE%97%E6%B3%95/</id>
    <published>2022-03-13T05:08:00.000Z</published>
    <updated>2022-03-13T05:18:08.769Z</updated>
    
    <content type="html"><![CDATA[<p>时间轮就是和手表时钟很相似的存在。时间轮用环形数组实现，数组的每个元素可以称为槽，和 HashMap一样称呼。</p><p>槽的内部用双向链表存着待执行的任务，添加和删除的链表操作时间复杂度都是 O(1)，槽位本身也指代时间精度，比如一秒扫一个槽，那么这个时间轮的最高精度就是 1 秒。</p><p>也就是说延迟 1.2 秒的任务和 1.5 秒的任务会被加入到同一个槽中，然后在 1 秒的时候遍历这个槽中的链表执行任务。</p><p> <img src="/images/pasted-157.png" alt="upload successful"></p><p>从图中可以看到此时指针指向的是第一个槽，一共有八个槽0~7，假设槽的时间单位为 1 秒，现在要加入一个延时 5 秒的任务，计算方式就是 5 % 8 + 1 = 6，即放在槽位为 6，下标为 5 的那个槽中。</p><p>更具体的就是拼到槽的双向链表的尾部。然后每秒指针顺时针移动一格，这样就扫到了下一格，遍历这格中的双向链表执行任务。然后再循环继续。</p><p>可以看到插入任务从计算槽位到插入链表，时间复杂度都是O(1)。那假设现在要加入一个50秒后执行的任务怎么办？这槽好像不够啊？难道要加槽嘛？和HashMap一样扩容？</p><p>不是的，常见有两种方式，一种是通过增加轮次的概念。50 % 8 + 1 = 3，即应该放在槽位是 3，下标是2 的位置。然后 (50 - 1) / 8 = 6，即轮数记为 6。也就是说当循环 6 轮之后扫到下标的 2 的这个槽位会触发这个任务。Netty中HashedWheelTimer 使用的就是这种方式。</p><p>还有一种是通过多层次的时间轮，这个和我们的手表就更像了，像我们秒针走一圈，分针走一格，分针走一圈，时针走一格。</p><p>多层次时间轮就是这样实现的。假设上图就是第一层，那么第一层走了一圈，第二层就走一格，可以得知第二层的一格就是8秒，假设第二层也是 8 个槽，那么第二层走一圈，第三层走一格，可以得知第三层一格就是 64 秒。那么一格三层，每层8个槽，一共 24 个槽时间轮就可以处理最多延迟 512 秒的任<br>务。</p><p>而多层次时间轮还会有降级的操作，假设一个任务延迟 500 秒执行，那么刚开始加进来肯定是放在第三层的，当时间过了 436 秒后，此时还需要 64 秒就会触发任务的执行，而此时相对而言它就是个延迟 64秒后的任务，因此它会被降低放在第二层中，第一层还放不下它。再过个 56 秒，相对而言它就是个延迟 8 秒后执行的任务，因此它会再被降级放在第一层中，等待执行。</p><p>降级是为了保证时间精度一致性Kafka内部用的就是多层次的时间轮算法。</p><p>Kafka 就利用了空间换时间的思想，通过 DelayQueue，来保存每个槽，通过每个槽的过期时间排序。这样拥有最早需要执行任务的槽会有优先获取。如果时候未到，那么 delayQueue.poll 就会阻塞着，这样就不会有空推进的情况发送。</p><p>总的来说Kafka用了多层次时间轮来实现，并且是按需创建时间轮，采用任务的绝对时间来判断延期，并且对于每个槽(槽内存放的也是任务的双向链表)都会维护一个过期时间，利用 DelayQueue 来对每个槽的过期<br>时间排序，来进行时间的推进，防止空推进的存在。</p><p>每次推进都会更新 currentTime 为当前时间戳，当然做了点微调使得 currentTime 是 tickMs 的整数<br>倍。并且每次推进都会把能降级的任务重新插入降级。</p><p>可以看到这里的 DelayQueue 的元素是每个槽，而不是任务，因此数量就少很多了，这应该是权衡了对于槽操作的延时队列的时间复杂度与空推进的影响。</p><p>总结：</p><p>Timer、DelayQueue 和 ScheduledThreadPool，它们都是基于优先队列实现的，O(logn)<br>的时间复杂度在任务数多的情况下频繁的入队出队对性能来说有损耗。因此适合于任务数不多的情况。</p><p>Timer 是单线程的会有阻塞的风险，并且对异常没有做处理，一个任务出错 Timer 就挂了。而<br>ScheduledThreadPool 相比于 Timer 首先可以多线程来执行任务，并且线程池对异常做了处理，使得任务之间不会有影响。</p><p>并且 Timer和ScheduledThreadPool 可以周期性执行任务。 而 DelayQueue 就是个具有优先级的阻塞队列。</p><p>对比而言时间轮更适合任务数很大的延时场景，它的任务插入和删除时间复杂度都为O(1)。对于延迟超过时间轮所能表示的范围有两种处理方式，一是通过增加一个字段-轮数，Netty 就是这样实现的。二是多层次时间轮，Kakfa 是这样实现的。</p><p>相比而言 Netty 的实现会有空推进的问题，而 Kafka 采用 DelayQueue 以槽为单位，利用空间换时间的思想解决了空推进的问题。</p><p>可以看出延迟任务的实现都不是很精确的，并且或多或少都会有阻塞的情况，即使你异步执行，线程不够的情况下还是会阻塞。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;时间轮就是和手表时钟很相似的存在。时间轮用环形数组实现，数组的每个元素可以称为槽，和 HashMap一样称呼。&lt;/p&gt;
&lt;p&gt;槽的内部用双向链表存着待执行的任务，添加和删除的链表操作时间复杂度都是 O(1)，槽位本身也指代时间精度，比如一秒扫一个槽，那么这个时间轮的最高精度</summary>
      
    
    
    
    <category term="算法" scheme="http://example.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    <category term="Kafka" scheme="http://example.com/categories/%E7%AE%97%E6%B3%95/Kafka/"/>
    
    
    <category term="算法" scheme="http://example.com/tags/%E7%AE%97%E6%B3%95/"/>
    
    <category term="时间轮" scheme="http://example.com/tags/%E6%97%B6%E9%97%B4%E8%BD%AE/"/>
    
  </entry>
  
  <entry>
    <title>延迟队列 DelayQueue</title>
    <link href="http://example.com/2022/03/13/%E5%BB%B6%E8%BF%9F%E9%98%9F%E5%88%97-DelayQueue/"/>
    <id>http://example.com/2022/03/13/%E5%BB%B6%E8%BF%9F%E9%98%9F%E5%88%97-DelayQueue/</id>
    <published>2022-03-13T05:05:00.000Z</published>
    <updated>2022-03-13T05:07:25.947Z</updated>
    
    <content type="html"><![CDATA[<p>Java 中还有个延迟队列 DelayQueue，加入延迟队列的元素都必须实现 Delayed 接口。延迟队列内部是利用 PriorityQueue 实现的，所以还是利用优先队列！Delayed 接口继承了Comparable 因此优先队<br>列是通过 delay 来排序的。</p><p>延迟队列是利用优先队列实现的，元素通过实现 Delayed 接口来返回延迟的时间。不过延迟队列就是个容<br>器，需要其他线程来获取和执行任务。</p><p>对于 Timer 、ScheduledThreadPool 和 DelayQueue，总结的说下它们都是通过优先队列来获取最早需要执行的任务，因此插入和删除任务的时间复杂度都为O(logn)，并且 Timer 、<br>ScheduledThreadPool 的周期性任务是通过重置任务的下一次执行时间来完成的。</p><p>问题就出在时间复杂度上，插入删除时间复杂度是O(logn)，那么假设频繁插入删除次数为 m，总的时<br>间复杂度就是O(mlogn)，这种时间复杂度满足不了 Kafka 这类中间件对性能的要求，而时间轮算法的插入删除时间复杂度是O(1)。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Java 中还有个延迟队列 DelayQueue，加入延迟队列的元素都必须实现 Delayed 接口。延迟队列内部是利用 PriorityQueue 实现的，所以还是利用优先队列！Delayed 接口继承了Comparable 因此优先队&lt;br&gt;列是通过 delay 来排序</summary>
      
    
    
    
    <category term="Java" scheme="http://example.com/categories/Java/"/>
    
    
    <category term="DelayQueue" scheme="http://example.com/tags/DelayQueue/"/>
    
    <category term="队列" scheme="http://example.com/tags/%E9%98%9F%E5%88%97/"/>
    
  </entry>
  
  <entry>
    <title>ScheduledThreadPoolExecutor，更多功能的Timer</title>
    <link href="http://example.com/2022/03/13/ScheduledThreadPoolExecutor%EF%BC%8C%E6%9B%B4%E5%A4%9A%E5%8A%9F%E8%83%BD%E7%9A%84Timer/"/>
    <id>http://example.com/2022/03/13/ScheduledThreadPoolExecutor%EF%BC%8C%E6%9B%B4%E5%A4%9A%E5%8A%9F%E8%83%BD%E7%9A%84Timer/</id>
    <published>2022-03-13T05:02:00.000Z</published>
    <updated>2022-03-13T05:05:25.870Z</updated>
    
    <content type="html"><![CDATA[<p>jdk1.5 引入了 ScheduledThreadPoolExecutor，它是一个具有更多功能的 Timer 的替代品，允许多个服务线程。如果设置一个服务线程和 Timer 没啥差别。</p><p>从注释看出相对于 Timer ，可能就是单线程跑任务和多线程跑任务的区别。但ScheduledThreadPoolExecutor继承了 ThreadPoolExecutor，实现了 ScheduledExecutorService。可以定性操作就是正常线程池差不<br>多了。</p><p>区别就在于两点，一个是 ScheduledFutureTask ，一个是 DelayedWorkQueue。<br>其实 DelayedWorkQueue 就是优先队列，也是利用数组实现的小顶堆。而 ScheduledFutureTask 继<br>承自 FutureTask 重写了 run 方法，实现了周期性任务的需求。</p><p>ScheduledThreadPoolExecutor 大致的流程和 Timer 差不多，也是维护一个优先队列，然后通过重写<br>task 的 run 方法来实现周期性任务，主要差别在于能多线程运行任务，不会单线程阻塞。<br>并且 Java 线程池的设定是 task 出错会把错误吃了，无声无息的。因此一个任务出错也不会影响之后的<br>任务。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;jdk1.5 引入了 ScheduledThreadPoolExecutor，它是一个具有更多功能的 Timer 的替代品，允许多个服务线程。如果设置一个服务线程和 Timer 没啥差别。&lt;/p&gt;
&lt;p&gt;从注释看出相对于 Timer ，可能就是单线程跑任务和多线程跑任务的区</summary>
      
    
    
    
    <category term="Java" scheme="http://example.com/categories/Java/"/>
    
    
    <category term="ScheduledThreadPoolExecutor" scheme="http://example.com/tags/ScheduledThreadPoolExecutor/"/>
    
  </entry>
  
  <entry>
    <title>JDK中的Timer</title>
    <link href="http://example.com/2022/03/13/JDK%E4%B8%AD%E7%9A%84Timer/"/>
    <id>http://example.com/2022/03/13/JDK%E4%B8%AD%E7%9A%84Timer/</id>
    <published>2022-03-13T04:57:00.000Z</published>
    <updated>2022-03-13T05:01:58.460Z</updated>
    
    <content type="html"><![CDATA[<p>java提供了延时操作的timer，里面由一个小根堆数组和执行线程构成。小根堆数组堆顶是当前最先需要执行的任务。执行线程通过不断轮询询问该任务（同系统当前时间做比对）是否需要执行。当需要执行时，看是否是周期性任务，是则将任务执行时间改到下一个周期，然后执行，不是则删除，执行任务。</p><p>可以看出 Timer 实际就是根据任务的执行时间维护了一个优先队列，并且起了一个线程不断地拉取任务执行。</p><p>有什么弊端呢？</p><p>首先优先队列的插入和删除的时间复杂度是O(logn)，当数据量大的时候，频繁的入堆出堆性能有待考虑。</p><p>并且是单线程执行，那么如果一个任务执行的时间过久则会影响下一个任务的执行时间(当然你任务的run要是异步执行也行)。并且从代码可以看到对异常没有做什么处理，那么一个任务出错的时候会导致之后的任务都无法执行。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;java提供了延时操作的timer，里面由一个小根堆数组和执行线程构成。小根堆数组堆顶是当前最先需要执行的任务。执行线程通过不断轮询询问该任务（同系统当前时间做比对）是否需要执行。当需要执行时，看是否是周期性任务，是则将任务执行时间改到下一个周期，然后执行，不是则删除，执行</summary>
      
    
    
    
    <category term="Java" scheme="http://example.com/categories/Java/"/>
    
    
    <category term="Timer" scheme="http://example.com/tags/Timer/"/>
    
    <category term="JDK" scheme="http://example.com/tags/JDK/"/>
    
  </entry>
  
  <entry>
    <title>I/O 多路复⽤：select/poll/epoll</title>
    <link href="http://example.com/2022/03/12/I-O-%E5%A4%9A%E8%B7%AF%E5%A4%8D%E2%BD%A4%EF%BC%9Aselect-poll-epoll/"/>
    <id>http://example.com/2022/03/12/I-O-%E5%A4%9A%E8%B7%AF%E5%A4%8D%E2%BD%A4%EF%BC%9Aselect-poll-epoll/</id>
    <published>2022-03-12T11:22:00.000Z</published>
    <updated>2022-03-12T11:44:57.876Z</updated>
    
    <content type="html"><![CDATA[<p>最基本的 Socket 模型：要想客户端和服务器能在⽹络中通信，那必须得使⽤ Socket 编程，它是进程间通信⾥⽐较特别的⽅式，特别之处在于它是可以跨主机间通信。创建 Socket 的时候，可以指定⽹络层使⽤的是 IPv4 还是 IPv6，传输层使⽤的是 TCP 还是 UDP。</p><p>服务端⾸先调⽤ socket() 函数，创建⽹络协议为 IPv4，以及传输协议为 TCP 的 Socket ，接着调⽤<br>bind() 函数，给这个 Socket 绑定⼀个 IP 地址和端⼝。</p><p>绑定端⼝的⽬的：当内核收到 TCP 报⽂，通过 TCP 头⾥⾯的端⼝号，来找到我们的应⽤程序，然后把数据传递给我们。</p><p>绑定 IP 地址的⽬的：⼀台机器是可以有多个⽹卡的，每个⽹卡都有对应的 IP 地址，当绑定⼀个⽹卡时，内核在收到该⽹卡上的包，才会发给我们；</p><p>绑定完 IP 地址和端⼝后，就可以调⽤ listen() 函数进⾏监听，此时对应 TCP 状态图中的 listen ，如果<br>我们要判定服务器中⼀个⽹络程序有没有启动，可以通过 netstat 命令查看对应的端⼝号是否有被监听。</p><p>服务端进⼊了监听状态后，通过调⽤ accept() 函数，来从内核获取客户端的连接，如果没有客户端连<br>接，则会阻塞等待客户端连接的到来。</p><p>客户端在创建好 Socket 后，调⽤ connect() 函数发起连接，该函数的参数要指明服务端的 IP 地址和端⼝号，然后万众期待的 TCP 三次握⼿就开始了。</p><p>在 TCP 连接的过程中，服务器的内核实际上为每个 Socket 维护了两个队列：</p><p>⼀个是还没完全建⽴连接的队列，称为 TCP 半连接队列，这个队列都是没有完成三次握⼿的连接，</p><p>此时服务端处于 syn_rcvd 的状态；<br>⼀个是⼀件建⽴连接的队列，称为 TCP 全连接队列，这个队列都是完成了三次握⼿的连接，此时服务端处于 established 状态；</p><p>当 TCP 全连接队列不为空后，服务端的 accept() 函数，就会从内核中的 TCP 全连接队列⾥拿出⼀个已<br>经完成连接的 Socket 返回应⽤程序，后续数据传输都⽤这个 Socket。</p><p>（注意，监听的 Socket 和真正⽤来传数据的 Socket 是两个：<br>⼀个叫作监听 Socket；<br>⼀个叫作已连接 Socket；）</p><p>连接建⽴后，客户端和服务端就开始相互传输数据了，双⽅都可以通过 read() 和 write() 函数来读写数<br>据。</p><p> <img src="/images/pasted-155.png" alt="upload successful"></p><p> 基于 Linux ⼀切皆⽂件的理念，在内核中 Socket 也是以「⽂件」的形式存在的，也是有对应的⽂件<br>描述符。</p><p>上面提到的TCP Socket 调⽤流程是最简单、最基本的，它基本只能⼀对⼀通信，因为使⽤的是同步阻塞的⽅式，当服务端在还没处理完⼀个客户端的⽹络 I/O 时，或者 读写操作发⽣阻塞时，其他客户端是⽆法与服务端连接的。可如果我们服务器只能服务⼀个客户，那这样就太浪费资源了，于是我们要改进这个⽹络 I/O 模型，以⽀持更多的客户端。</p><p>服务器作为服务⽅，通常会在本地固定监听⼀个端⼝，等待客户端的连接。因此服务器的本地 IP 和端⼝是固定的，于是对于服务端 TCP 连接的四元组只有对端 IP 和端⼝是会变化的，所以最⼤ TCP 连接数 = 客户端 IP 数×客户端端⼝数。</p><p>对于 IPv4，客户端的 IP 数最多为 2 的 32 次⽅，客户端的端⼝数最多为 2 的 16 次⽅，也就是服务端单机<br>最⼤ TCP 连接数约为 2 的 48 次⽅。</p><p>这个理论值相当“丰满”，但是服务器肯定承载不了那么⼤的连接数，主要会受两个⽅⾯的限制：</p><p>⽂件描述符，Socket 实际上是⼀个⽂件，也就会对应⼀个⽂件描述符。在 Linux 下，单个进程打开的<br>⽂件描述符数是有限制的，没有经过修改的值⼀般都是 1024，不过我们可以通过 ulimit 增⼤⽂件描<br>述符的数⽬；</p><p>系统内存，每个 TCP 连接在内核中都有对应的数据结构，意味着每个连接都是会占⽤⼀定内存的；</p><p>基于最原始的阻塞⽹络 I/O， 如果服务器要⽀持多个客户端，其中⽐较传统的⽅式，就是使⽤多进程模型，也就是为每个客户端分配⼀个进程来处理请求。</p><p>服务器的主进程负责监听客户的连接，⼀旦与客户端连接完成，accept() 函数就会返回⼀个「已连接<br>Socket」，这时就通过 fork() 函数创建⼀个⼦进程，实际上就把⽗进程所有相关的东⻄都复制⼀份，包<br>括⽂件描述符、内存地址空间、程序计数器、执⾏的代码等。</p><p>这两个进程刚复制完的时候，⼏乎⼀摸⼀样。不过，会根据返回值来区分是⽗进程还是⼦进程，如果返回值是 0，则是⼦进程；如果返回值是其他的整数，就是⽗进程。</p><p>正因为⼦进程会复制⽗进程的⽂件描述符，于是就可以直接使⽤「已连接 Socket 」和客户端通信了，可以发现，⼦进程不需要关⼼「监听 Socket」，只需要关⼼「已连接 Socket」；⽗进程则相反，将客户<br>服务交给⼦进程来处理，因此⽗进程不需要关⼼「已连接 Socket」，只需要关⼼「监听 Socket」。</p><p>另外，当「⼦进程」退出时，实际上内核⾥还会保留该进程的⼀些信息，也是会占⽤内存的，如果不做好<br>“回收”⼯作，就会变成僵⼫进程，随着僵⼫进程越多，会慢慢耗尽我们的系统资源。</p><p>因此，⽗进程要“善后”好⾃⼰的孩⼦，怎么善后呢？那么有两种⽅式可以在⼦进程退出后回收资源，分别<br>是调⽤ wait() 和 waitpid() 函数。</p><p>这种⽤多个进程来应付多个客户端的⽅式，在应对 100 个客户端还是可⾏的，但是当客户端数量⾼达⼀万<br>时，肯定扛不住的，因为每产⽣⼀个进程，必会占据⼀定的系统资源，⽽且进程间上下⽂切换的“包袱”是<br>很重的，性能会⼤打折扣。</p><p>进程的上下⽂切换不仅包含了虚拟内存、栈、全局变量等⽤户空间的资源，还包括了内核堆栈、寄存器等<br>内核空间的资源。</p><p>既然进程间上下⽂切换的“包袱”很重，那我们就搞个⽐较轻量级的模型来应对多⽤户的请求 —— 多线程模型。</p><p>线程是运⾏在进程中的⼀个“逻辑流”，单进程中可以运⾏多个线程，同进程⾥的线程可以共享进程的部分<br>资源的，⽐如⽂件描述符列表、进程空间、代码、全局数据、堆、共享库等，这些共享些资源在上下⽂切<br>换时是不需要切换，⽽只需要切换线程的私有数据、寄存器等不共享的数据，因此同⼀个进程下的线程上<br>下⽂切换的开销要⽐进程⼩得多。</p><p>当服务器与客户端 TCP 完成连接后，通过 pthread_create() 函数创建线程，然后将「已连接 Socket」的<br>⽂件描述符传递给线程函数，接着在线程⾥和客户端进⾏通信，从⽽达到并发处理的⽬的。</p><p>如果每来⼀个连接就创建⼀个线程，线程运⾏完后，还得操作系统还得销毁线程，虽说线程切换的上写⽂<br>开销不⼤，但是如果频繁创建和销毁线程，系统开销也是不⼩的。</p><p>那么，我们可以使⽤线程池的⽅式来避免线程的频繁创建和销毁，所谓的线程池，就是提前创建若⼲个线<br>程，这样当由新连接建⽴时，将这个已连接的 Socket 放⼊到⼀个队列⾥，然后线程池⾥的线程负责从队列<br>中取出已连接 Socket 进程处理。</p><p>需要注意的是，这个队列是全局的，每个线程都会操作，为了避免多线程竞争，线程在操作这个队列前要<br>加锁。</p><p>上⾯基于进程或者线程模型的，其实还是有问题的。新到来⼀个 TCP 连接，就需要分配⼀个进程或者线<br>程，那么如果要达到 C10K，意味着要⼀台机器维护 1 万个连接，相当于要维护 1 万个进程/线程，操作系<br>统就算死扛也是扛不住的。</p><p>既然为每个请求分配⼀个进程/线程的⽅式不合适，那有没有可能只使⽤⼀个进程来维护多个 Socket 呢？<br>答案是有的，那就是 I/O 多路复⽤技术。</p><p>⼀个进程虽然任⼀时刻只能处理⼀个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1<br>秒内就可以处理上千个请求，把时间拉⻓来看，多个请求复⽤了⼀个进程，这就是多路复⽤，这种思想很<br>类似⼀个 CPU 并发多个进程，所以也叫做时分多路复⽤。</p><p>我们熟悉的 select/poll/epoll 内核提供给⽤户态的多路复⽤系统调⽤，进程可以通过⼀个系统调⽤函数从内<br>核中获取多个事件。</p><p>select/poll/epoll 是如何获取⽹络事件的呢？在获取事件时，先把所有连接（⽂件描述符）传给内核，再由<br>内核返回产⽣了事件的连接，然后在⽤户态中再处理这些连接对应的请求即可。</p><p>所以，对于 select 这种⽅式，需要进⾏ 2 次「遍历」⽂件描述符集合，⼀次是在内核态⾥，⼀个次是在⽤<br>户态⾥ ，⽽且还会发⽣ 2 次「拷⻉」⽂件描述符集合，先从⽤户空间传⼊内核空间，由内核修改后，再传<br>出到⽤户空间中。</p><p>select 使⽤固定⻓度的 BitsMap，表示⽂件描述符集合，⽽且所⽀持的⽂件描述符的个数是有限制的，在<br>Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最⼤值为 1024 ，只能监听 0~1023 的⽂件描述符。</p><p>poll 不再⽤ BitsMap 来存储所关注的⽂件描述符，取⽽代之⽤动态数组，以链表形式来组织，突破了<br>select 的⽂件描述符个数限制，当然还会受到系统⽂件描述符限制。</p><p>但是 poll 和 select 并没有太⼤的本质区别，都是使⽤「线性结构」存储进程关注的 Socket 集合，因此都<br>需要遍历⽂件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，⽽且也需要在⽤户态与内核<br>态之间拷⻉⽂件描述符集合，这种⽅式随着并发数上来，性能的损耗会呈指数级增⻓。</p><p>epoll 通过两个⽅⾯，很好解决了 select/poll 的问题。</p><p>第⼀点，epoll 在内核⾥使⽤红⿊树来跟踪进程所有待检测的⽂件描述字，把需要监控的 socket 通过<br>epoll_ctl() 函数加⼊内核中的红⿊树⾥，红⿊树是个⾼效的数据结构，增删查⼀般时间复杂度是<br>O(logn) ，通过对这棵⿊红树进⾏操作，这样就不需要像 select/poll 每次操作时都传⼊整个 socket 集<br>合，只需要传⼊⼀个待检测的 socket，减少了内核和⽤户空间⼤量的数据拷⻉和内存分配。</p><p>第⼆点， epoll 使⽤事件驱动的机制，内核⾥维护了⼀个链表来记录就绪事件，当某个 socket 有事件发⽣<br>时，通过回调函数内核会将其加⼊到这个就绪事件列表中，当⽤户调⽤ epoll_wait() 函数时，只会返回有<br>事件发⽣的⽂件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，⼤⼤提⾼了检测的效<br>率。</p><p> <img src="/images/pasted-156.png" alt="upload successful"></p><p> epoll 的⽅式即使监听的 Socket 数量越多的时候，效率不会⼤幅度降低，能够同时监听的 Socket 的数⽬<br>也⾮常的多了，上限就为系统定义的进程打开的最⼤⽂件描述符个数。因⽽，epoll 被称为解决 C10K 问<br>题的利器。</p><p>（注意：epoll_wait 返回时，对于就绪的事件，epoll使⽤的是共享内存的⽅式，<br>即⽤户态和内核态都指向了就绪链表，所以就避免了内存拷⻉消耗。<br>这是错的！看过 epoll 内核源码的都知道，压根就没有使⽤共享内存这个玩意。你可以从下⾯这份代码看<br>到， epoll_wait 实现的内核代码中调⽤了 __put_user 函数，这个函数就是将数据从内核拷⻉到⽤户空<br>间。）</p><p>epoll ⽀持两种事件触发模式，分别是边缘触发（edge-triggered，ET）和⽔平触发（level-triggered，<br>LT）。</p><p>使⽤边缘触发模式时，当被监控的 Socket 描述符上有可读事件发⽣时，服务器端只会从 epoll_wait<br>中苏醒⼀次，即使进程没有调⽤ read 函数从内核读取数据，也依然只苏醒⼀次，因此我们程序要保<br>证⼀次性将内核缓冲区的数据读取完；</p><p>使⽤⽔平触发模式时，当被监控的 Socket 上有可读事件发⽣时，服务器端不断地从 epoll_wait 中苏<br>醒，直到内核缓冲区数据被 read 函数读完才结束，⽬的是告诉我们有数据需要读取；</p><p>举个例⼦，你的快递被放到了⼀个快递箱⾥，如果快递箱只会通过短信通知你⼀次，即使你⼀直没有去<br>取，它也不会再发送第⼆条短信提醒你，这个⽅式就是边缘触发；如果快递箱发现你的快递没有被取出，<br>它就会不停地发短信通知你，直到你取出了快递，它才消停，这个就是⽔平触发的⽅式。</p><p>这就是两者的区别，⽔平触发的意思是只要满⾜事件的条件，⽐如内核中有数据需要读，就⼀直不断地把<br>这个事件传递给⽤户；⽽边缘触发的意思是只有第⼀次满⾜条件的时候才触发，之后就不会再传递同样的<br>事件了。</p><p>如果使⽤⽔平触发模式，当内核通知⽂件描述符可读写时，接下来还可以继续去检测它的状态，看它是否<br>依然可读或可写。所以在收到通知后，没必要⼀次执⾏尽可能多的读写操作。</p><p>如果使⽤边缘触发模式，I/O 事件发⽣时只会通知⼀次，⽽且我们不知道到底能读写多少数据，所以在收到<br>通知后应尽可能地读写数据，以免错失读写的机会。因此，我们会循环从⽂件描述符读写数据，那么如果<br>⽂件描述符是阻塞的，没有数据可读写时，进程会阻塞在读写函数那⾥，程序就没办法继续往下执⾏。所<br>以，边缘触发模式⼀般和⾮阻塞 I/O 搭配使⽤，程序会⼀直执⾏ I/O 操作，直到系统调⽤（如 read 和<br>write ）返回错误，错误类型为 EAGAIN 或 EWOULDBLOCK 。</p><p>⼀般来说，边缘触发的效率⽐⽔平触发的效率要⾼，因为边缘触发可以减少 epoll_wait 的系统调⽤次数，<br>系统调⽤也是有⼀定的开销的的，毕竟也存在上下⽂的切换。</p><p>select/poll 只有⽔平触发模式，epoll 默认的触发模式是⽔平触发，但是可以根据应⽤场景设置为边缘触发<br>模式。</p><p>另外，使⽤ I/O 多路复⽤时，最好搭配⾮阻塞 I/O ⼀起使⽤，简单点理解，就是多路复⽤ API 返回的事件并不⼀定可读写的，如果使⽤阻塞 I/O， 那么在调⽤read/write 时则会发⽣程序阻塞，因此最好搭配⾮阻塞 I/O，以便应对极少数的特殊情况。</p><p>最基础的 TCP 的 Socket 编程，它是阻塞 I/O 模型，基本上只能⼀对⼀通信，那为了服务更多的客户端，<br>我们需要改进⽹络 I/O 模型。</p><p>⽐较传统的⽅式是使⽤多进程/线程模型，每来⼀个客户端连接，就分配⼀个进程/线程，然后后续的读写都<br>在对应的进程/线程，这种⽅式处理 100 个客户端没问题，但是当客户端增⼤到 10000 个时，10000 个进<br>程/线程的调度、上下⽂切换以及它们占⽤的内存，都会成为瓶颈。</p><p>为了解决上⾯这个问题，就出现了 I/O 的多路复⽤，可以只在⼀个进程⾥处理多个⽂件的 I/O，Linux 下有<br>三种提供 I/O 多路复⽤的 API，分别是： select、poll、epoll。</p><p>select 和 poll 并没有本质区别，它们内部都是使⽤「线性结构」来存储进程关注的 Socket 集合。<br>在使⽤的时候，⾸先需要把关注的 Socket 集合通过 select/poll 系统调⽤从⽤户态拷⻉到内核态，然后由<br>内核检测事件，当有⽹络事件产⽣时，内核需要遍历进程关注 Socket 集合，找到对应的 Socket，并设置<br>其状态为可读/可写，然后把整个 Socket 集合从内核态拷⻉到⽤户态，⽤户态还要继续遍历整个 Socket 集<br>合找到可读/可写的 Socket，然后对其处理。</p><p>很明显发现，select 和 poll 的缺陷在于，当客户端越多，也就是 Socket 集合越⼤，Socket 集合的遍历和拷⻉会带来很⼤的开销，因此也很难应对 C10K。</p><p>epoll 是解决 C10K 问题的利器，通过两个⽅⾯解决了 select/poll 的问题。</p><p>epoll 在内核⾥使⽤「红⿊树」来关注进程所有待检测的 Socket，红⿊树是个⾼效的数据结构，增删<br>查⼀般时间复杂度是 O(logn)，通过对这棵⿊红树的管理，不需要像 select/poll 在每次操作时都传⼊<br>整个 Socket 集合，减少了内核和⽤户空间⼤量的数据拷⻉和内存分配。</p><p>epoll 使⽤事件驱动的机制，内核⾥维护了⼀个「链表」来记录就绪事件，只将有事件发⽣的 Socket<br>集合传递给应⽤程序，不需要像 select/poll 那样轮询扫描整个集合（包含有和⽆事件的 Socket ），<br>⼤⼤提⾼了检测的效率。</p><p>⽽且，epoll ⽀持边缘触发和⽔平触发的⽅式，⽽ select/poll 只⽀持⽔平触发，⼀般⽽⾔，边缘触发的⽅式<br>会⽐⽔平触发的效率⾼。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最基本的 Socket 模型：要想客户端和服务器能在⽹络中通信，那必须得使⽤ Socket 编程，它是进程间通信⾥⽐较特别的⽅式，特别之处在于它是可以跨主机间通信。创建 Socket 的时候，可以指定⽹络层使⽤的是 IPv4 还是 IPv6，传输层使⽤的是 TCP 还是 U</summary>
      
    
    
    
    <category term="I/O多路复用" scheme="http://example.com/categories/I-O%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/"/>
    
    
    <category term="Linux" scheme="http://example.com/tags/Linux/"/>
    
    <category term="IO" scheme="http://example.com/tags/IO/"/>
    
    <category term="I/O多路复用" scheme="http://example.com/tags/I-O%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/"/>
    
    <category term="select/epoll/poll" scheme="http://example.com/tags/select-epoll-poll/"/>
    
  </entry>
  
  <entry>
    <title>Kafka-Kraft 模式</title>
    <link href="http://example.com/2022/03/12/Kafka-Kraft-%E6%A8%A1%E5%BC%8F/"/>
    <id>http://example.com/2022/03/12/Kafka-Kraft-%E6%A8%A1%E5%BC%8F/</id>
    <published>2022-03-12T05:30:00.000Z</published>
    <updated>2022-03-12T05:32:23.547Z</updated>
    
    <content type="html"><![CDATA[<p> <img src="/images/pasted-154.png" alt="upload successful"></p><p> 左图为 Kafka 现有架构，元数据在 zookeeper 中，运行时动态选举 controller，由controller 进行 Kafka 集群管理。右图为 kraft 模式架构（实验性），不再依赖 zookeeper 集群，而是用三台 controller 节点代替 zookeeper，元数据保存在 controller 中，由 controller 直接进行 Kafka 集群管理。</p><p> 这样做的好处有以下几个：</p><p> ⚫ Kafka 不再依赖外部框架，而是能够独立运行； </p><p> ⚫ controller 管理集群时，不再需要从 zookeeper 中先读取数据，集群性能上升； </p><p> ⚫ 由于不依赖 zookeeper，集群扩展时不再受到 zookeeper 读写能力限制； </p><p> ⚫ controller 不再动态选举，而是由配置文件规定。这样我们可以有针对性的加强controller 节点的配置，而不是像以前一样对随机 controller 节点的高负载束手无策。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt; &lt;img src=&quot;/images/pasted-154.png&quot; alt=&quot;upload successful&quot;&gt;&lt;/p&gt;
&lt;p&gt; 左图为 Kafka 现有架构，元数据在 zookeeper 中，运行时动态选举 controller，由controller 进行 Kaf</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="Kraft模式" scheme="http://example.com/tags/Kraft%E6%A8%A1%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>Kafka数据积压（消费者如何提高吞吐量）</title>
    <link href="http://example.com/2022/03/12/Kafka%E6%95%B0%E6%8D%AE%E7%A7%AF%E5%8E%8B%EF%BC%88%E6%B6%88%E8%B4%B9%E8%80%85%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98%E5%90%9E%E5%90%90%E9%87%8F%EF%BC%89/"/>
    <id>http://example.com/2022/03/12/Kafka%E6%95%B0%E6%8D%AE%E7%A7%AF%E5%8E%8B%EF%BC%88%E6%B6%88%E8%B4%B9%E8%80%85%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98%E5%90%9E%E5%90%90%E9%87%8F%EF%BC%89/</id>
    <published>2022-03-12T05:28:00.000Z</published>
    <updated>2022-03-12T05:29:37.815Z</updated>
    
    <content type="html"><![CDATA[<p>1）如果是Kafka消费能力不足，则可以考虑增 加Topic的分区数，并且同时提升消费组的消费者<br>数量，消费者数 = 分区数。（两者缺一不可）</p><p>2）如果是下游的数据处理不及时：提高每批次拉取的数<br>量。批次拉取数据过少（拉取数据/处理时间 &lt; 生产速度），<br>使处理的数据小于生产的数据，也会造成数据积压。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;1）如果是Kafka消费能力不足，则可以考虑增 加Topic的分区数，并且同时提升消费组的消费者&lt;br&gt;数量，消费者数 = 分区数。（两者缺一不可）&lt;/p&gt;
&lt;p&gt;2）如果是下游的数据处理不及时：提高每批次拉取的数&lt;br&gt;量。批次拉取数据过少（拉取数据/处理时间 &amp;lt; </summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="数据积压" scheme="http://example.com/tags/%E6%95%B0%E6%8D%AE%E7%A7%AF%E5%8E%8B/"/>
    
    <category term="消费者" scheme="http://example.com/tags/%E6%B6%88%E8%B4%B9%E8%80%85/"/>
    
  </entry>
  
  <entry>
    <title>Kafka消费者漏消费和重复消费问题</title>
    <link href="http://example.com/2022/03/12/Kafka%E6%B6%88%E8%B4%B9%E8%80%85%E6%BC%8F%E6%B6%88%E8%B4%B9%E5%92%8C%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9%E9%97%AE%E9%A2%98/"/>
    <id>http://example.com/2022/03/12/Kafka%E6%B6%88%E8%B4%B9%E8%80%85%E6%BC%8F%E6%B6%88%E8%B4%B9%E5%92%8C%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9%E9%97%AE%E9%A2%98/</id>
    <published>2022-03-12T05:12:00.000Z</published>
    <updated>2022-03-12T05:27:39.970Z</updated>
    
    <content type="html"><![CDATA[<p>重复消费：已经消费了数据，但是 offset 没提交，下次还会消费到当前数据。</p><p>漏消费：先提交 offset 后消费，有可能会造成数据的漏消费。</p><p> <img src="/images/pasted-153.png" alt="upload successful"></p><p>如果想完成Consumer端的精准一次性消费（既不漏消费也不重复消费），那么需要Kafka消费端将消费过程和提交offset<br>过程做原子绑定。此时我们需要将Kafka的offset保存到支持事务的自定义介质（比 如MySQL）。</p><pre><code>参考：https://blog.csdn.net/qingqing7/article/details/80054281?spm=1001.2101.3001.6650.14&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-14.pc_relevant_default&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-14.pc_relevant_default&amp;utm_relevant_index=25</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;重复消费：已经消费了数据，但是 offset 没提交，下次还会消费到当前数据。&lt;/p&gt;
&lt;p&gt;漏消费：先提交 offset 后消费，有可能会造成数据的漏消费。&lt;/p&gt;
&lt;p&gt; &lt;img src=&quot;/images/pasted-153.png&quot; alt=&quot;upload suc</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="消费者" scheme="http://example.com/tags/%E6%B6%88%E8%B4%B9%E8%80%85/"/>
    
    <category term="漏消费" scheme="http://example.com/tags/%E6%BC%8F%E6%B6%88%E8%B4%B9/"/>
    
    <category term="重复消费" scheme="http://example.com/tags/%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9/"/>
    
  </entry>
  
  <entry>
    <title>Kafka消费者的offset提交</title>
    <link href="http://example.com/2022/03/12/Kafka%E6%B6%88%E8%B4%B9%E8%80%85%E7%9A%84offset%E6%8F%90%E4%BA%A4/"/>
    <id>http://example.com/2022/03/12/Kafka%E6%B6%88%E8%B4%B9%E8%80%85%E7%9A%84offset%E6%8F%90%E4%BA%A4/</id>
    <published>2022-03-12T05:00:00.000Z</published>
    <updated>2022-03-12T05:11:59.292Z</updated>
    
    <content type="html"><![CDATA[<p>offset偏移量表明了该消费者当前消费的数据到哪一步，其存储在系统主题_consumer_offset中（0.9版本之前是存在Zookeeper中），以key,value形式，每隔一段时间kafka都会对其Compact（即保留当前最新的数据）。</p><p>1、自动提交offset：为了能让我们专注于业务处理，Kafka提供了自动提交offset功能，通过参数</p><p>⚫ enable.auto.commit：是否开启自动提交offset功能，默认是true</p><p>⚫ auto.commit.interval.ms：自动提交offset的时间间隔，默认是5s</p><p>2、手动提交：自动提交固然遍历，但基于时间的提交，我们很难把握那个度，因此更多时候，我们可以选择手动提交。</p><p>1）同步提交：同步提交会阻塞当前线程，一直到成功为止，并且失败会自动重试</p><p>2）异步提交：异步提交则不会阻塞当前线程，且没有重试机制，可能提交失败。</p><p>两者都会将本次提交的一批数据最高偏移量提交。</p><p>指定offset消费：auto.offset.reset = earliest | latest | none 默认是 latest。</p><p>当kafka中没有初始偏移量（消费者组第一次消费）或服务器上不存在当前偏移量时（数据被删除）需要指定offset消费。</p><p>1）earliest：自动将偏移量重置为最早的偏移量，–from-beginning。</p><p>2）latest（默认值）：自动将偏移量重置为最新偏移量。</p><p>（3）none：如果未找到消费者组的先前偏移量，则向消费者抛出异常。</p><p>（4）任意指定 offset 位移开始消费</p><p>指定时间消费：在生产环境中，会遇到最近消费的几个小时数据异常，想重新按照时间消费。例如要求按照时间消费前一天的数据</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;offset偏移量表明了该消费者当前消费的数据到哪一步，其存储在系统主题_consumer_offset中（0.9版本之前是存在Zookeeper中），以key,value形式，每隔一段时间kafka都会对其Compact（即保留当前最新的数据）。&lt;/p&gt;
&lt;p&gt;1、自动提</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="消费者" scheme="http://example.com/tags/%E6%B6%88%E8%B4%B9%E8%80%85/"/>
    
    <category term="offset" scheme="http://example.com/tags/offset/"/>
    
  </entry>
  
  <entry>
    <title>Kafka消费者分区的分配以及再平衡</title>
    <link href="http://example.com/2022/03/12/Kafka%E6%B6%88%E8%B4%B9%E8%80%85%E5%88%86%E5%8C%BA%E7%9A%84%E5%88%86%E9%85%8D%E4%BB%A5%E5%8F%8A%E5%86%8D%E5%B9%B3%E8%A1%A1/"/>
    <id>http://example.com/2022/03/12/Kafka%E6%B6%88%E8%B4%B9%E8%80%85%E5%88%86%E5%8C%BA%E7%9A%84%E5%88%86%E9%85%8D%E4%BB%A5%E5%8F%8A%E5%86%8D%E5%B9%B3%E8%A1%A1/</id>
    <published>2022-03-12T04:47:00.000Z</published>
    <updated>2022-03-12T04:53:38.116Z</updated>
    
    <content type="html"><![CDATA[<p>一个consumer group中有多个consumer组成，一个 topic有多个partition组成，现在的问题是，到底由哪个consumer来消费哪个partition的数据。 </p><p>2、Kafka有四种主流的分区分配策略：<br> Range、RoundRobin、Sticky、CooperativeSticky。<br>可以通过配置参数partition.assignment.strategy，修改分区的分配策略。默认策略是Range + CooperativeSticky。Kafka可以同时使用多个分区分配策略。</p><p>1）Range 是对每个 topic 而言的。</p><p>首先对同一个 topic 里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。</p><p>假如现在有 7 个分区，3 个消费者，排序后的分区将会是0,1,2,3,4,5,6；消费者排序完之后将会是C0,C1,C2。例如，7/3 = 2 余 1 ，除不尽，那么 消费者 C0 便会多消费 1 个分区。 8/3=2余2，除不尽，那么C0和C1分别多消费一个。</p><p>通过 partitions数/consumer数 来决定每个消费者应该消费几个分区。如果除不尽，那么前面几个消费者将会多<br>消费 1 个分区。</p><p>注意：如果只是针对 1 个 topic 而言，C0消费者多消费1个分区影响不是很大。但是如果有 N 多个 topic，那么针对个 topic，消费者 C0都将多消费 1 个分区，topic越多，C0消 费的分区会比其他消费者明显多消费 N 个分区。容易产生数据倾斜！</p><p>（注意：说明：某个消费者挂掉后，消费者组需要按照超时时间 45s 来判断它是否退出，所以需要等待，时间到了 45s 后，判断它真的退出就会把任务分配给其他 broker 执行。）</p><p>2）RoundRobin 分区策略原理：</p><p>RoundRobin 针对集群中所有Topic而言。<br>RoundRobin 轮询分区策略，是把所有的 partition 和所有的<br>consumer 都列出来，然后按照 hashcode 进行排序，最后<br>通过轮询算法来分配 partition 给到各个消费者。</p><p>3） Sticky 以及再平衡：</p><p>粘性分区定义：可以理解为分配的结果带有“粘性的”。即在执行一次新的分配之前，<br>考虑上一次分配的结果，尽量少的调整分配的变动，可以节省大量的开销。<br>粘性分区是 Kafka 从 0.11.x 版本开始引入这种分配策略，首先会尽量均衡的放置分区<br>到消费者上面，在出现同一消费者组内消费者出现问题的时候，会尽量保持原有分配的分<br>区不变化。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;一个consumer group中有多个consumer组成，一个 topic有多个partition组成，现在的问题是，到底由哪个consumer来消费哪个partition的数据。 &lt;/p&gt;
&lt;p&gt;2、Kafka有四种主流的分区分配策略：&lt;br&gt; Range、Round</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="消费者" scheme="http://example.com/tags/%E6%B6%88%E8%B4%B9%E8%80%85/"/>
    
    <category term="分区分配策略" scheme="http://example.com/tags/%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/"/>
    
  </entry>
  
  <entry>
    <title>Kafka高效读写数据</title>
    <link href="http://example.com/2022/03/12/Kafka%E9%AB%98%E6%95%88%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE/"/>
    <id>http://example.com/2022/03/12/Kafka%E9%AB%98%E6%95%88%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE/</id>
    <published>2022-03-12T03:23:00.000Z</published>
    <updated>2022-03-12T03:24:21.616Z</updated>
    
    <content type="html"><![CDATA[<p>1）Kafka 本身是分布式集群，可以采用分区技术，并行度高</p><p>2）读数据采用稀疏索引，可以快速定位要消费的数据</p><p>3）顺序写磁盘（Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到 600M/s，而随机写只有 100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。）</p><p>4）页缓存 + 零拷贝技术</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;1）Kafka 本身是分布式集群，可以采用分区技术，并行度高&lt;/p&gt;
&lt;p&gt;2）读数据采用稀疏索引，可以快速定位要消费的数据&lt;/p&gt;
&lt;p&gt;3）顺序写磁盘（Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直追加到文件末端，为顺序写。官网有数</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="高效读写" scheme="http://example.com/tags/%E9%AB%98%E6%95%88%E8%AF%BB%E5%86%99/"/>
    
  </entry>
  
  <entry>
    <title>Kafka文件清理策略</title>
    <link href="http://example.com/2022/03/12/Kafka%E6%96%87%E4%BB%B6%E6%B8%85%E7%90%86%E7%AD%96%E7%95%A5/"/>
    <id>http://example.com/2022/03/12/Kafka%E6%96%87%E4%BB%B6%E6%B8%85%E7%90%86%E7%AD%96%E7%95%A5/</id>
    <published>2022-03-12T03:19:00.000Z</published>
    <updated>2022-03-12T03:23:07.944Z</updated>
    
    <content type="html"><![CDATA[<p>Kafka 中默认的日志保存时间为 7 天，可以通过调整如下参数修改保存时间。<br>⚫ log.retention.hours，最低优先级小时，默认 7 天。</p><p>⚫ log.retention.minutes，分钟。 </p><p>⚫ log.retention.ms，最高优先级毫秒。 </p><p>⚫log.retention.check.interval.ms，负责设置检查周期，默认 5 分钟。</p><p>对于超过设置事件的数据，有两种清楚策略，delete和Compact</p><p>1）delete 日志删除：将过期数据删除</p><p>⚫ log.cleanup.policy = delete 所有数据启用删除策略</p><p>（1）基于时间：默认打开。以 segment 中所有记录中的最大时间戳作为该文件时间戳。</p><p>（2）基于大小：默认关闭。超过设置的所有日志总大小，删除最早segment。log.retention.bytes，默认等于-1，表示无穷大。</p><p>2）compact 日志压缩</p><p>compact日志压缩：对于相同key的不同value值，只保留最后一个版本。</p><p>⚫ log.cleanup.policy = compact 所有数据启用压缩策略</p><p> <img src="/images/pasted-152.png" alt="upload successful"></p><p>压缩后的offset可能是不连续的，比如上图中没有6，当从这些offset消费消息时，将会拿到比这个offset大 的offset对应的消息，实际上会拿到offset为7的消息，并从这个位置开始消费。</p><p>这种策略只适合特殊场景，比如消息的key是用户ID，value是用户的资料，通过这种压缩策略，整个消息<br>集里就保存了所有用户最新的资料。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Kafka 中默认的日志保存时间为 7 天，可以通过调整如下参数修改保存时间。&lt;br&gt;⚫ log.retention.hours，最低优先级小时，默认 7 天。&lt;/p&gt;
&lt;p&gt;⚫ log.retention.minutes，分钟。 &lt;/p&gt;
&lt;p&gt;⚫ log.retenti</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="清楚策略" scheme="http://example.com/tags/%E6%B8%85%E6%A5%9A%E7%AD%96%E7%95%A5/"/>
    
  </entry>
  
  <entry>
    <title>Kafka文件存储机制</title>
    <link href="http://example.com/2022/03/12/Kafka%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6/"/>
    <id>http://example.com/2022/03/12/Kafka%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6/</id>
    <published>2022-03-12T03:16:00.000Z</published>
    <updated>2022-03-12T03:19:20.616Z</updated>
    
    <content type="html"><![CDATA[<p>Topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是Producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引机制， 将每个partition分为多个segment。每个segment包括：“.index”文件、“.log”文件和.timeindex等文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号，例如：first-0。</p><p> <img src="/images/pasted-150.png" alt="upload successful"></p><p> Log文件和Index文件详解：</p><p> <img src="/images/pasted-151.png" alt="upload successful"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是Producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，为防止log文件过大导致数据定位效率低下，Kafka采取了</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="文件存储" scheme="http://example.com/tags/%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"/>
    
  </entry>
  
  <entry>
    <title>Leader Partition 负载平衡</title>
    <link href="http://example.com/2022/03/12/Leader-Partition-%E8%B4%9F%E8%BD%BD%E5%B9%B3%E8%A1%A1/"/>
    <id>http://example.com/2022/03/12/Leader-Partition-%E8%B4%9F%E8%BD%BD%E5%B9%B3%E8%A1%A1/</id>
    <published>2022-03-12T03:11:00.000Z</published>
    <updated>2022-03-12T03:15:02.258Z</updated>
    
    <content type="html"><![CDATA[<p>正常情况下，Kafka本身会自动把Leader Partition均匀分散在各个机器上，来保证每台机器的读写吞吐量都是均匀的。但是如果某些broker宕机，会导致Leader Partition过于集中在其他少部分几台broker上，这会导致少数几台broker的读写请求压力过高，其他宕机的broker重启之后都是follower partition，读写请求很低，造成集群负载不均衡。</p><p>策略：</p><p>1、auto.leader.rebalance.enable，默认是true。（自动Leader Partition 平衡）</p><p>2、leader.imbalance.per.broker.percentage，默认是10%。每个broker允许的不平衡的leader的比率。如果每个broker超过了这个值，控制器会触发leader的平衡。</p><p>3、leader.imbalance.check.interval.seconds，默认值300秒。检查leader负载是否平衡的间隔时间。</p><p>例如：针对broker0节点，分区2的AR优先副本是0节点，但是0节点却不是Leader节点，所以不平衡数加1，AR副本总数是4，所以broker0节点不平衡率为1/4&gt;10%，需要再平衡。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;正常情况下，Kafka本身会自动把Leader Partition均匀分散在各个机器上，来保证每台机器的读写吞吐量都是均匀的。但是如果某些broker宕机，会导致Leader Partition过于集中在其他少部分几台broker上，这会导致少数几台broker的读写请求压</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="Leader" scheme="http://example.com/tags/Leader/"/>
    
    <category term="Partition" scheme="http://example.com/tags/Partition/"/>
    
  </entry>
  
  <entry>
    <title>Leader 和 Follower 故障处理细节</title>
    <link href="http://example.com/2022/03/12/Leader-%E5%92%8C-Follower-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E7%BB%86%E8%8A%82/"/>
    <id>http://example.com/2022/03/12/Leader-%E5%92%8C-Follower-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E7%BB%86%E8%8A%82/</id>
    <published>2022-03-12T03:06:00.000Z</published>
    <updated>2022-03-12T03:10:07.966Z</updated>
    
    <content type="html"><![CDATA[<p>LEO（Log End Offset）：每个副本的最后一个offset，LEO其实就是最新的offset + 1。</p><p>HW（High Watermark）：所有副本中最小的LEO 。</p><p>1）Follower故障：</p><p>（1） Follower发生故障后会被临时踢出ISR</p><p>（2） 这个期间Leader和Follower继续接收数据</p><p>（3）待该Follower恢复后，Follower会读取本地磁盘记录的<br>上次的HW，并将log文件高于HW的部分截取掉，从HW开始向Leader进行同步。</p><p>（4）等该Follower的LEO大于等于该Partition的HW，即<br>Follower追上Leader之后，就可以重新加入ISR了。</p><p>2）Leader故障：</p><p>（1） Leader发生故障之后，会从ISR中选出一个新的Leader</p><p>（2）为保证多个副本之间的数据一致性，其余的Follower会先将各自的log文件高于HW的部分截掉，然后从新的Leader同步数据。</p><p>注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;LEO（Log End Offset）：每个副本的最后一个offset，LEO其实就是最新的offset + 1。&lt;/p&gt;
&lt;p&gt;HW（High Watermark）：所有副本中最小的LEO 。&lt;/p&gt;
&lt;p&gt;1）Follower故障：&lt;/p&gt;
&lt;p&gt;（1） Followe</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="Kafka" scheme="http://example.com/tags/Kafka/"/>
    
    <category term="Leader和Follower故障" scheme="http://example.com/tags/Leader%E5%92%8CFollower%E6%95%85%E9%9A%9C/"/>
    
  </entry>
  
  <entry>
    <title>Kafka Broker总体工作流程</title>
    <link href="http://example.com/2022/03/12/Kafka-Broker%E6%80%BB%E4%BD%93%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/"/>
    <id>http://example.com/2022/03/12/Kafka-Broker%E6%80%BB%E4%BD%93%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/</id>
    <published>2022-03-12T03:00:00.000Z</published>
    <updated>2022-03-12T03:00:21.303Z</updated>
    
    <content type="html"><![CDATA[<p> <img src="/images/pasted-149.png" alt="upload successful"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt; &lt;img src=&quot;/images/pasted-149.png&quot; alt=&quot;upload successful&quot;&gt;&lt;/p&gt;
</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="Kafka" scheme="http://example.com/tags/Kafka/"/>
    
    <category term="工作流程" scheme="http://example.com/tags/%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper中存储的Kafka 信息</title>
    <link href="http://example.com/2022/03/12/Zookeeper%E4%B8%AD%E5%AD%98%E5%82%A8%E7%9A%84Kafka-%E4%BF%A1%E6%81%AF/"/>
    <id>http://example.com/2022/03/12/Zookeeper%E4%B8%AD%E5%AD%98%E5%82%A8%E7%9A%84Kafka-%E4%BF%A1%E6%81%AF/</id>
    <published>2022-03-12T02:52:00.000Z</published>
    <updated>2022-03-12T02:53:19.813Z</updated>
    
    <content type="html"><![CDATA[<p>在zookeeper的服务端存储的Kafka相关信息：</p><p>1）/kafka/brokers/ids [0,1,2] 记录有哪些服务器</p><p>2）/kafka/brokers/topics/first/partitions/0/state<br>{“leader”:1 ,”isr”:[1,0,2] } 记录谁是Leader，有哪些服务器可用</p><p>3）/kafka/controller<br>{“brokerid”:0}<br>辅助选举Leader</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在zookeeper的服务端存储的Kafka相关信息：&lt;/p&gt;
&lt;p&gt;1）/kafka/brokers/ids [0,1,2] 记录有哪些服务器&lt;/p&gt;
&lt;p&gt;2）/kafka/brokers/topics/first/partitions/0/state&lt;br&gt;{“lea</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    <category term="Zookeeper" scheme="http://example.com/categories/Kafka/Zookeeper/"/>
    
    
    <category term="Zookeeper" scheme="http://example.com/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Kafka数据乱序</title>
    <link href="http://example.com/2022/03/12/Kafka%E6%95%B0%E6%8D%AE%E4%B9%B1%E5%BA%8F/"/>
    <id>http://example.com/2022/03/12/Kafka%E6%95%B0%E6%8D%AE%E4%B9%B1%E5%BA%8F/</id>
    <published>2022-03-12T02:47:00.000Z</published>
    <updated>2022-03-12T02:50:26.550Z</updated>
    
    <content type="html"><![CDATA[<p>1）kafka在1.x版本之前保证数据单分区有序，条件如下：<br>max.in.flight.requests.per.connection=1（不需要考虑是否开启幂等性）。 </p><p>2）kafka在1.x及以后版本保证数据单分区有序，条件如下：</p><p>（1）未开启幂等性<br>max.in.flight.requests.per.connection需要设置为1。</p><p>（2）开启幂等性<br>max.in.flight.requests.per.connection需要设置小于等于5。 </p><p>原因说明：因为在kafka1.x以后，启用幂等后，kafka服务端会缓存producer发来的最近5个request的元数据，故无论如何，都可以保证最近5个request的数据都是有序的。</p><p> <img src="/images/pasted-148.png" alt="upload successful"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;1）kafka在1.x版本之前保证数据单分区有序，条件如下：&lt;br&gt;max.in.flight.requests.per.connection=1（不需要考虑是否开启幂等性）。 &lt;/p&gt;
&lt;p&gt;2）kafka在1.x及以后版本保证数据单分区有序，条件如下：&lt;/p&gt;
&lt;p&gt;（</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="Kafka" scheme="http://example.com/tags/Kafka/"/>
    
    <category term="数据有序" scheme="http://example.com/tags/%E6%95%B0%E6%8D%AE%E6%9C%89%E5%BA%8F/"/>
    
  </entry>
  
  <entry>
    <title>Kafka的生产者事务原理</title>
    <link href="http://example.com/2022/03/12/Kafka%E7%9A%84%E7%94%9F%E4%BA%A7%E8%80%85%E4%BA%8B%E5%8A%A1%E5%8E%9F%E7%90%86/"/>
    <id>http://example.com/2022/03/12/Kafka%E7%9A%84%E7%94%9F%E4%BA%A7%E8%80%85%E4%BA%8B%E5%8A%A1%E5%8E%9F%E7%90%86/</id>
    <published>2022-03-12T02:29:00.000Z</published>
    <updated>2022-03-12T02:33:52.261Z</updated>
    
    <content type="html"><![CDATA[<p>注意：开启事务，必须要开启幂等性。另外Procuder在使用事务功能前，必须先自定义一个唯一的transaction.id。有了transaction.id，即使客户端挂掉了，它重启后也能继续处理未完成的事务。</p><p> <img src="/images/pasted-147.png" alt="upload successful"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;注意：开启事务，必须要开启幂等性。另外Procuder在使用事务功能前，必须先自定义一个唯一的transaction.id。有了transaction.id，即使客户端挂掉了，它重启后也能继续处理未完成的事务。&lt;/p&gt;
&lt;p&gt; &lt;img src=&quot;/images/paste</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    
    <category term="Kafka" scheme="http://example.com/tags/Kafka/"/>
    
    <category term="事务" scheme="http://example.com/tags/%E4%BA%8B%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>Kafka保证生产者生产的数据不重复：幂等性+至少一次</title>
    <link href="http://example.com/2022/03/12/Kafka%E4%BF%9D%E8%AF%81%E7%94%9F%E4%BA%A7%E8%80%85%E7%94%9F%E4%BA%A7%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%8D%E9%87%8D%E5%A4%8D%EF%BC%9A%E5%B9%82%E7%AD%89%E6%80%A7-%E8%87%B3%E5%B0%91%E4%B8%80%E6%AC%A1/"/>
    <id>http://example.com/2022/03/12/Kafka%E4%BF%9D%E8%AF%81%E7%94%9F%E4%BA%A7%E8%80%85%E7%94%9F%E4%BA%A7%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%8D%E9%87%8D%E5%A4%8D%EF%BC%9A%E5%B9%82%E7%AD%89%E6%80%A7-%E8%87%B3%E5%B0%91%E4%B8%80%E6%AC%A1/</id>
    <published>2022-03-12T02:25:00.000Z</published>
    <updated>2022-03-12T02:34:13.739Z</updated>
    
    <content type="html"><![CDATA[<p>至少一次：ack级别设置为-1+分区副本大于等于2+ISR里面的应答最小副本大于等于2（保证数据不会丢失）</p><p>幂等性：指Producer不论向Broker发送多少次重复数据，Broker端都只会持久化一条，保证了不重复。（重复数据的判断标准：具有&lt;PID, Partition, SeqNumber&gt;相同主键的消息提交时，Broker只会持久化一条。其 中PID是Kafka每次重启都会分配一个新的；Partition 表示分区号；Sequence Number是单调自增的。）</p><p>因此幂等性只能保证的是在单分区单会话内不重复。</p><p>如何使用幂等性：开启参数 enable.idempotence 默认为 true，false 关闭。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;至少一次：ack级别设置为-1+分区副本大于等于2+ISR里面的应答最小副本大于等于2（保证数据不会丢失）&lt;/p&gt;
&lt;p&gt;幂等性：指Producer不论向Broker发送多少次重复数据，Broker端都只会持久化一条，保证了不重复。（重复数据的判断标准：具有&amp;lt;PID,</summary>
      
    
    
    
    <category term="Kafka" scheme="http://example.com/categories/Kafka/"/>
    
    <category term="消息队列" scheme="http://example.com/categories/Kafka/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
    
    <category term="Kafka" scheme="http://example.com/tags/Kafka/"/>
    
    <category term="数据" scheme="http://example.com/tags/%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
</feed>
