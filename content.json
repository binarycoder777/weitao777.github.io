{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[{"title":"关于","date":"2022-03-05T01:29:53.000Z","updated":"2022-03-05T01:31:10.414Z","comments":false,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"对待工作认真负责，富有上进心，热爱学习，喜欢专研新技术，能很快掌握新技术。为人诚恳乐观，心里素质较好，抗压能力较强， 可以适应较强的压力。具备模块设计及开发能力，能独立完成开发任务。此外也喜欢阅读相关技术的书籍文档。平时编码严格遵守相关 编码规范。"},{"title":"404 Not Found：该页无法显示","date":"2022-01-05T06:15:01.841Z","updated":"2022-01-05T06:10:01.885Z","comments":false,"path":"/404.html","permalink":"http://example.com/404.html","excerpt":"","text":""},{"title":"书单","date":"2022-03-05T01:29:35.000Z","updated":"2022-03-05T01:33:11.946Z","comments":false,"path":"books/index.html","permalink":"http://example.com/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2022-01-05T08:23:46.960Z","updated":"2022-01-05T06:10:01.887Z","comments":false,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2022-01-05T08:22:49.166Z","updated":"2022-01-05T06:10:01.888Z","comments":true,"path":"links/index.html","permalink":"http://example.com/links/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-01-05T08:24:21.310Z","updated":"2022-01-05T06:10:01.888Z","comments":false,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2022-01-05T06:15:01.947Z","updated":"2022-01-05T06:10:01.888Z","comments":false,"path":"repository/index.html","permalink":"http://example.com/repository/index.html","excerpt":"","text":""}],"posts":[{"title":"Spring编程式事务","slug":"Spring编程式事务","date":"2022-06-17T08:55:00.000Z","updated":"2022-06-17T12:43:17.962Z","comments":true,"path":"2022/06/17/Spring编程式事务/","link":"","permalink":"http://example.com/2022/06/17/Spring%E7%BC%96%E7%A8%8B%E5%BC%8F%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"描述：今天在做项目实现活动领取功能模块时，考虑到在系统中，用户的领取次数表和领取活动表的后续开销问题，在设计时通过自研的分库分表db-router-Spring-boot-starter组件对该功能涉及的两张表进行了分库操作。即让不同uid的用户产生的领取活动信息和活动次数信息打到不同的数据库上，减轻数据库的压力。但在后续开发时，发现涉及到两张表的修改，而这里又用到了分库操作，对数据源进行了切换。导致常用的@trancation注解在这里并不适用。为此，去温习了一下编程式事务，用于解决此处的问题。在这里记录一下。 编程式事务主要有2种用法 方式1：通过PlatformTransactionManager控制事务 方式2：通过TransactionTemplate控制事务 方式1：PlatformTransactionManager 定义事务管理器PlatformTransactionManager（事务管理器相当于一个管理员，这个管理员就是用来帮你控制事务的，比如开启事务，提交事务，回滚事务等等。spring中使用PlatformTransactionManager这个接口来表示事务管理器，PlatformTransactionManager多个实现类，用来应对不同的环境） JpaTransactionManager：如果你用jpa来操作db，那么需要用这个管理器来帮你控制事务。 DataSourceTransactionManager：如果你用是指定数据源的方式，比如操作数据库用的是：JdbcTemplate、mybatis、ibatis，那么需要用这个管理器来帮你控制事务。 HibernateTransactionManager：如果你用hibernate来操作db，那么需要用这个管理器来帮你控制事务。 JtaTransactionManager：如果你用的是java中的jta来操作db，这种通常是分布式事务，此时需要用这种管理器来控制事务。 定义事务属性TransactionDefinition：定义事务属性，比如事务隔离级别、事务超时时间、事务传播方式、是否是只读事务等等。spring中使用TransactionDefinition接口来表示事务的定义信息，有个子类比较常用：DefaultTransactionDefinition。 开启事务：调用事务管理器的getTransaction方法，即可以开启一个事务，这个方法会返回一个TransactionStatus表示事务状态的一个对象，通过TransactionStatus提供的一些方法可以用来控制事务的一些状态，比如事务最终是需要回滚还是需要提交。（执行了getTransaction后，spring内部会执行一些操作。将数据源datasource和connection映射起来放在了ThreadLocal中。通过resources这个ThreadLocal获取datasource其对应的connection对象） 执行业务操作：用同一个dataSource，而事务管理器开启事务的时候，会创建一个连接，将datasource和connection映射之后丢在了ThreadLocal中，而JdbcTemplate内部执行db操作的时候，也需要获取连接，JdbcTemplate会以自己内部的datasource去上面的threadlocal中找有没有关联的连接，如果有直接拿来用，若没找到将重新创建一个连接，而此时是可以找到的，那么JdbcTemplate就参与到spring的事务中了。 提交或回滚 分析： TransactionTemplate，主要有2个方法： executeWithoutResult：无返回值场景 executeWithoutResult(Consumer action)：没有返回值的，需传递一个Consumer对象，在accept方法中做业务操作 execute：有返回值场景 T execute(TransactionCallback action)：有返回值的，需要传递一个TransactionCallback对象，在doInTransaction方法中做业务操作 通过上面2个方法，事务管理器会自动提交事务或者回滚事务。 什么时候事务会回滚，有2种方式 方式1 在execute或者executeWithoutResult内部执行transactionStatus.setRollbackOnly();将事务状态标注为回滚状态，spring会自动让事务回滚 方式2 execute方法或者executeWithoutResult方法内部抛出任意异常即可回滚。 总结：平时我们用的最多的是声明式事务，声明式事务的底层还是使用上面这种方式来控制事务的，只不过对其进行了封装，让我们用起来更容易些。了解不同的方法有助于我们在不同的场景的应用。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"事务","slug":"事务","permalink":"http://example.com/tags/%E4%BA%8B%E5%8A%A1/"},{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"}],"author":"John Doe"},{"title":"误删数据库后除了跑路，还能怎么办？","slug":"误删数据库后除了跑路，还能怎么办？","date":"2022-06-15T14:23:00.000Z","updated":"2022-06-15T15:43:59.602Z","comments":true,"path":"2022/06/15/误删数据库后除了跑路，还能怎么办？/","link":"","permalink":"http://example.com/2022/06/15/%E8%AF%AF%E5%88%A0%E6%95%B0%E6%8D%AE%E5%BA%93%E5%90%8E%E9%99%A4%E4%BA%86%E8%B7%91%E8%B7%AF%EF%BC%8C%E8%BF%98%E8%83%BD%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9F/","excerpt":"","text":"传统的高可用架构是不能预防误删除数据的，因为主库的一个drop table命令，会通过binlog传给所有从库和级联从库，进而导致整个集群的实例都会执行这个命令。 为了找到解决误删数据库的更高效的方法，我们需要先对和MySQL相关的误删数据，做下分类： 使用delete语句误删数据行 使用drop table或者truncate table语句误删数据库 使用drop database语句误删数据库 使用rm命令误删整个MySQL实例 误删行方案：使用delete语句误删了数据行，可以用Flashback工具通过闪回把数据恢复回来。 原理：修改binlog的内容，拿回原库重放。而能够使用这个方案的前提是，需要确保binlog_format=row和binlog_row_image=FULL。 具体： 对于insert语句，对应的binlog event类型是Write_rows event，把它改成Delete_rows event即可。 同理，对于delete语句，也是将Delete_rows event改为Write_rows event。 而如果是Update_rows的话,binlog里面记录了数据行修改前和修改后的值,对调这两行的位置即可。 而对于误删数据涉及到了多个事务的话，需要将事务的顺序调过来再执行。 当然恢复过程并不建议在主库上进行，而是恢复出一个备份，或者找一个从库作为临时库，在临时库上执行恢复操作，确认过数据无误后，在恢复回主库。（原因：一个在执行线上逻辑的主库，数据状态往往是有关联的。可能由于发现数据问题的实际晚了一点，就导致已经在之前误删的基础上，业务代码逻辑又继续修改了其他数据，如果这时进行数据恢复，未经确认，会导致出现对数据的二次破坏） 注意：不止要了解误删数据的事后处理方法，更重要的是做到事前预防。 建议： 1. 把sql_safe_updates参数设置成on，在delete或者update语句中写where，或者where条件不包含索引字段，这条语句就会报错。 2. 代码上线前，必须经过SQL审计 误删库/表这种情况下，想要恢复数据就需要使用全量备份了，加增量日志的方式。这个方案要求线上定期的全量备份，并且实时备份binlog（因为对于drop/turncate table/database binlog记录的是statement格式，没法通过flasback进行恢复） 恢复数据的流程： 取最近一次全量备份（假设是一天一备，上次备份是当天0点） 用备份恢复出一个临时库 从日志备份里面，取出凌晨0点之后的日志 把这些日志，除了误删除数据的语句外，全部应用到临时库 说明：为了加速数据恢复，可以在mysqlbinlog加上-database，用来指定误删除表所在的库，但是这样还是不够快（1.误删表，不能指定解析一个表的日志；2.用mysqlbinlog解析出日志应用，应用日志的过程就只能是单线程） 误删库或表后，恢复数据的思路主要就是通过备份，再加上应用binlog的方式。最好把数据恢复功能做成自动化工具。 延迟复制备库延迟复制备库是一种特殊的备库，通过CHANGE MASTER TO_DELAY = N命令，可以指定这个备库持续保持主库有N秒的延迟。 预防误删库表的方法 账号分离，权限控制 制定操作规范 定期给开发进行培训 搭建延迟备库 做好sql审计 做好备份","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"author":"John Doe"},{"title":"关于ZAB协议","slug":"关于ZAB协议","date":"2022-05-25T14:02:00.000Z","updated":"2022-05-25T14:19:02.540Z","comments":true,"path":"2022/05/25/关于ZAB协议/","link":"","permalink":"http://example.com/2022/05/25/%E5%85%B3%E4%BA%8EZAB%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"Zab 借鉴了 Paxos 算法，是特别为 Zookeeper 设计的支持崩溃恢复的原子广播协议。基于该协议，Zookeeper 设计为只有一台客户端（Leader）负责处理外部的写事务请求，然后Leader 客户端将数据同步到其他 Follower 节点。Zookeeper 只有一个 Leader 可以发起提案 Zab协议包括两种基本的模式：消息广播、崩溃恢复。 消息广播ZAB协议针对事务请求的处理过程类似于一个两阶段提交过程 （1）广播事务阶段 （2）广播提交操作这两阶段提交模型如下，有可能因为Leader宕机带来数据不一致，比如 （1） Leader 发 起 一 个 事 务Proposal1 后 就 宕 机 ， Follower 都 没有Proposal1 （2）Leader收到半数ACK宕 机，没来得及向Follower发送Commit怎么解决呢？ZAB引入了崩溃恢复模式。 （1）客户端发起一个写操作请求。 （2）Leader服务器将客户端的请求转化为事务Proposal 提案，同时为每个Proposal 分配一个全局的ID，即zxid。 （3）Leader服务器为每个Follower服务器分配一个单独的队列，然后将需要广播的 Proposal依次放到队列中去，并且根据FIFO策略进行消息发送。 （4）Follower接收到Proposal后，会首先将其以事务日志的方式写入本地磁盘中，写入成功后向Leader反馈一个Ack响应消息。 （5）Leader接收到超过半数以上Follower的Ack响应消息后，即认为消息发送成功，可以发送commit消息。 （6）Leader向所有Follower广播commit消息，同时自身也会完成事务提交。Follower 接收到commit消息后，会将上一条事务提交。 （7）Zookeeper采用Zab协议的核心，就是只要有一台服务器提交了Proposal，就要确保所有的服务器最终都能正确提交Proposal。 崩溃恢复——异常假设 一旦Leader服务器出现崩溃或者由于网络原因导致Leader服务器失去了与过半 Follower的联系，那么就会进入崩溃恢复模式。 假设两种服务器异常情况 假设一个事务在Leader提出之后，Leader挂了 一个事务在Leader上提交了，并且过半的Follower都响应ACK了，但是Leader在Commit消息发出之前挂了。 Zab协议崩溃恢复要满足以下两个要求 1、 确保已经被Leader提交的提案Proposal，必须最终被所有的Follower服务器提交。 （已经产生的提案，Follower必须执行） 2、 确保丢弃已经被Leader提出的，但是没有被提交的Proposal。（丢弃胎死腹中的提案） 崩溃恢复主要包括两部分：Leader选举和数据恢复。 崩溃恢复—Leader选举 Leader选举：根据上述要求，Zab协议需要保证选举出来的Leader需要满足以下条件： （1）新选举出来的Leader不能包含未提交的Proposal。即新Leader必须都是已经提交了Proposal的Follower服务器节点。 （2）新选举的Leader节点中含有最大的zxid。这样做的好处是可以避免Leader服务器检查Proposal的提交和丢弃工作。 崩溃恢复——数据恢复Zab如何数据同步： （1）完成Leader选举后，在正式开始工作之前（接收事务请求，然后提出新的Proposal），Leader服务器会首先确认事务日志中的所有的Proposal 是否已经被集群中过半的服务器Commit。 （2）Leader服务器需要确保所有的Follower服务器能够接收到每一条事务的Proposal，并且能将所有已经提交的事务Proposal应用到内存数据中。等到Follower将所有尚未同步的事务Proposal都从Leader服务器上同步过，并且应用到内存数据中以后，Leader才会把该Follower加入到真正可用的Follower列表中。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"ZAB协议","slug":"ZAB协议","permalink":"http://example.com/tags/ZAB%E5%8D%8F%E8%AE%AE/"},{"name":"数据一致性","slug":"数据一致性","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7/"}],"author":"John Doe"},{"title":"关于Paxos算法","slug":"关于Paxos算法","date":"2022-05-25T13:38:00.000Z","updated":"2022-05-25T13:59:20.676Z","comments":true,"path":"2022/05/25/关于Paxos算法/","link":"","permalink":"http://example.com/2022/05/25/%E5%85%B3%E4%BA%8EPaxos%E7%AE%97%E6%B3%95/","excerpt":"","text":"Paxos算法是一种基于消息传递且具有高度容错特性的一致性算法。其通常用于快速正确的在一个分布式系统中对某个数据值达成一致，并且保证无论发送任何异常都不会破坏整个系统的一致性。 在一个Paxos系统中，首先将所有节点划分为Proposer（提议者）、Acceptor（接受者）和Learner（学习者） 注意：每个节点可以身兼数职 一个完整的Paxos算法流程分为三个阶段 Prepare准备阶段 Proposer向多个Acceptor发出Propose请求Promise（承诺） Acceptor针对收到的Propose请求进行Promise（承诺） Accept接受阶段 Proposer收到多数Acceptor承诺的Promise后，向Acceptor发出Propose请求 Acceptor针对收到的Propose请求进行Accept处理 Learn学习阶段 Proposer将形成的决议发送给所有Learners 1、 Prepare: Proposer生成全局唯一且递增的Proposal ID，向所有Acceptor发送Propose请求，这里无需携带提案内容，只携带Proposal ID即可。 2、 Promise: Acceptor收到Propose请求后，做出“两个承诺，一个应答”。 - 不再接受Proposal ID小于等于（注意：这里是&lt;= ）当前请求的Propose请求。 - 不再接受Proposal ID小于（注意：这里是&lt; ）当前请求的Accept请求。 - 不违背以前做出的承诺下，回复已经Accept过的提案中Proposal ID最大的那个提案的Value和Proposal ID，没有则返回空值。 3、 Propose: Proposer收到多数Acceptor的Promise应答后，从应答中选择Proposal ID最大的提案的Value，作为本次要发起的提案。如果所有应答的提案Value均为空值，则可以自己随意决定提案Value。然后携带当前Proposal ID，向所有Acceptor发送Propose请求。 4、 Accept: Acceptor收到Propose请求后，在不违背自己之前做出的承诺下，接受并持久化当前Proposal ID和提案Value。 5、 Learn: Proposer收到多数Acceptor的Accept后，决议形成，将形成的决议发送给所有Learner。 下面举例以说明： 情况1： 有A1, A2, A3, A4, A5 5位议员，就税率问题进行决议。 A1发起1号Proposal的Propose，等待Promise承诺； A2-A5回应Promise； A1在收到两份回复时就会发起税率10%的Proposal； A2-A5回应Accept； 通过Proposal，税率10%。 情况2： - 现在我们假设在A1提出提案的同时, A5决定将税率定为20% - A2承诺A1，A4承诺A5，A3行为成为关键 - 情况1：A3先收到A1消息，承诺A1。 - A1发起Proposal（1，10%），A2，A3接受。 - 之后A3又收到A5消息，回复A1：（1，10%），并承诺A5 - A5发起Proposal（2，20%），A3，A4接受。之后A1，A5同时广播决议。 由此可见Paxos 算法缺陷：在网络复杂的情况下，一个应用 Paxos 算法的分布式系统，可能很久无法收敛，甚至陷入活锁的情况。 情况3： - 现在我们假设在A1提出提案的同时, A5决定将税率定为20% - A1，A5同时发起Propose（序号分别为1，2） - A2承诺A1，A4承诺A5，A3行为成为关键 - 情况2：A3先收到A1消息，承诺A1。之后立刻收到A5消息，承诺A5。 - A1发起Proposal（1，10%），无足够响应，A1重新Propose （序号3），A3再次承诺A1。 - A5发起Proposal（2，20%），无足够相应。A5重新Propose （序号4），A3再次承诺A5。 造成这种情况的原因是系统中有一个以上的 Proposer，多个 Proposers 相互争夺 Acceptor，造成迟迟无法达成一致的情况。针对这种情况，一种改进的 Paxos 算法被提出：从系统中选出一个节点作为 Leader，只有 Leader 能够发起提案。这样，一次 Paxos 流程中只有一个Proposer，不会出现活锁的情况，此时只会出现例子中第一种情况。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"Paxos","slug":"Paxos","permalink":"http://example.com/tags/Paxos/"},{"name":"一致性算法","slug":"一致性算法","permalink":"http://example.com/tags/%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/"}],"author":"John Doe"},{"title":"Zookeeper选举机制","slug":"Zookeeper选举机制","date":"2022-05-25T12:24:00.000Z","updated":"2022-05-25T12:40:27.471Z","comments":true,"path":"2022/05/25/Zookeeper选举机制/","link":"","permalink":"http://example.com/2022/05/25/Zookeeper%E9%80%89%E4%B8%BE%E6%9C%BA%E5%88%B6/","excerpt":"","text":"在了解Zookeeper的选举机制之前，首先需要了解 SID(服务器ID，用于唯一标识一台Zookeeper集群中的机器，每台机器不能重复，和myid一致) ZXID（事务ID。用来标识一次服务器状态的变更。在某一时刻，集群中的每台机器的ZXID值不一定完全一致，这和Zookeeper服务器对于客户端“更新请求”的处理逻辑有关） Epoch(每个Leader任期的代号。没有Leader时同一轮投票过程中的逻辑时钟是相同的。每投完一次票，这个数据会增加) 了解上述三个参数之后，后续Zookeeper的选举机制便和其有关。关于其选举机制可以分为两种情况。 第一种情况是第一次启动集群时，Leader的选举是根据其myid的大小进行选举的。 例如：目前有一个五台Zookeeper的集群。当启动id为1的Zookeeper时，它投自己一票。此时不满足Leader成立的条件。票数半数以上。因此进入looking状态。然后id为2的服务器启动，id1和id2的服务器都投自己一票，然后交换选票信息。此时服务器1发现服务器2的id比自己大，因此服务器1转投服务器2一票。服务器1只有0票，而服务器2有2票。但此时还是不满足半数以上，因此两者都进入looking状态。然后服务器3启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数，服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING；服务器4启动，发起一次选举。此时服务器1，2，3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为 1票。此时服务器4服从多数，更改选票信息为服务器3，并更改状态为FOLLOWING；服务器5启动，同4一样当小弟。 而第二种情况是非第一次启动，而是在集群运行中，Leader宕机了，需要重新选举Leader。 例如：假设ZooKeeper由5台服务器组成，SID分别为1、2、3、4、5，ZXID分别为8、8、8、7、7，并且此时SID为3的服务器是Leader。某一时刻，3和5服务器出现故障，因此开始进行Leader选举。 选举Leader规则： ①EPOCH大的直接胜出 ②EPOCH相同，事务id大的胜出 ③事务id相同，服务器id大的胜出","categories":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"http://example.com/categories/Zookeeper/"}],"tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"http://example.com/tags/Zookeeper/"},{"name":"选举机制","slug":"选举机制","permalink":"http://example.com/tags/%E9%80%89%E4%B8%BE%E6%9C%BA%E5%88%B6/"}],"author":"John Doe"},{"title":"为什么要使用泛型程序设计？","slug":"为什么要使用泛型程序设计？","date":"2022-05-15T13:49:00.000Z","updated":"2022-05-18T11:13:31.973Z","comments":true,"path":"2022/05/15/为什么要使用泛型程序设计？/","link":"","permalink":"http://example.com/2022/05/15/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BD%BF%E7%94%A8%E6%B3%9B%E5%9E%8B%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%EF%BC%9F/","excerpt":"","text":"最近一直忙于编译原理的学习，难得空闲下来，重新巩固了一下泛型的知识。在此小记一下。 泛型程序设计意味着编写的代码可以被很多不同类型的对象重用 泛型类在java中增加泛型类前，泛型程序设计时通过继承等方式实现的。例如：ArrayList维护一个Object数组的引用。 这种方式存在着两个问题： 当获取一个值时，必须进行强制类型转换。 没有进行任何检测，可以向其中添加任何类的对象。 因此，对于调用，编译和运行都不会出错。但在某些情况下进行了错误的强制类型转换使用。就会报错。 对此，泛型提供了更好的解决方案：类型参数。这不仅使得代码更具可读性，也使得代码更加安全。因为编译器可以根据这个信息推断出get时的类型，不需要进行强制类型转换。在编译期间检查出类型错误，而不是在运行时才检测出。 一个泛型类就是具有一个或多个类型变量的类。（使用大写形式，且比较短。在java中，E表示集合类型，K和V则是键值对，T表示任意类型） 泛型方法泛型方法的类型变量放在修饰符后，返回值前面。泛型方法可以定义在普通类中，也可以定义在泛型类中。 当调用一个泛型方法时，在方法名前的尖括号中放入具体的类型。当然在大多数情况下，编译器可以推导出类型，意味着我们可以省去。（在某些情况下，编译器无法推导出，此时需要指明） 类型变量的限定有时候类或方法需要对类型变量加以约束，&lt; T extends Object &gt; T 当做出这样的限定后，泛型的变量类型便被约束了。当然一个类型变量或通配符可以有多个限定，只需要用&amp;隔开即可。&lt; T extends Object1 &amp; Object2 &gt; T值得注意的是，在java中可以根据需要拥有多个接口超类型，但限定中至多有一个类。如果用一个类作为限定，它必须位于限定列表的第一个 extends决定了泛型变量的上限。 泛型代码和虚拟机虚拟机没有泛型类对象，所有对象都是属于普通对象。 类型擦除：无论何时定义一个泛型类型，都自动提供了一个相应的原始类型。原始类型的名字就是删去类型参数后的泛型类型名。擦除类型变量，并替换为限定类型类型。（无限定的变量用Object） 翻译泛型表达式：当程序调用泛型方法时，如果擦除返回类型，编译器会插入强制类型转换。 翻译泛型方法：类型擦除也会出现在泛型方法中，只留下限定类型。但这可能会导致类型擦除和多态发生冲突。 约束与局限性 不能用基本数据类型实例化类型参数，即不能有&lt; double &gt;，但可以有&lt; Double &gt;,原因是类型擦除。 运行时类型查询只适用于原始类型，而不适用于泛型类型。当试图查询一个对象是否属于某个泛型类型时，倘若使用instanceof会得到一个编译器错误。同样的道理，getClass方法总是返回原始类型。 不能创建参数化类型的数组，例如：Pair&lt; String &gt;[] table = new Pair&lt; String &gt; [10];类型擦除之后，table类型时Pair[],，可以把它转化为Object[],数组会记住其元素类型，如果试图存其他类型，则会报错。不过对于泛型，这种机制会使之无效。不过仍会导致一个类型错误。因此，不能创建参数化类型的数组。当然可以声明通配类型的数组，然后进行类型转换。 不能构造泛型数组，因为数组本身也有类型，用来监控存在虚拟机中的数组。 泛型类的静态上下文中类型变量无效：不能在静态域或方法中使用类型变量。 不能抛出或捕获泛型类的实例：可以消除对受查异常的检查 注意擦除后的冲突：要想支持擦除的转换，就需要强制限制一个类或类型变量不能同时成为两个接口类型的子类，而这两个接口时同一接口的不同参数。 泛型类型的继承规则 无论S和T有什么关系，通常calssName &lt; S &gt; 和 calssName &lt; T &gt; 没有任何关系 待续…","categories":[{"name":"Java","slug":"Java","permalink":"http://example.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"泛型","slug":"泛型","permalink":"http://example.com/tags/%E6%B3%9B%E5%9E%8B/"}],"author":"John Doe"},{"title":"关于lambda表达式","slug":"关于lambda表达式","date":"2022-05-02T14:46:00.000Z","updated":"2022-05-02T15:19:37.907Z","comments":true,"path":"2022/05/02/关于lambda表达式/","link":"","permalink":"http://example.com/2022/05/02/%E5%85%B3%E4%BA%8Elambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/","excerpt":"","text":"简单介绍lambda表达式是一个可传递的代码块，可以在以后执行一次或多次。 表达形式：(参数)-&gt;{表达式} 如果可以推导出lambda表达式的参数类型，则可以忽略 如果只有一个参数，且参数类型可以推断，则（）可以省略 无需指定其返回类型，lambda表达式的返回类型可以根据上下文推断得出 注意：如果一个lambda表达式在某些分支返回值，而在另一些分支不返还，这是错误的 函数式接口对于只有一个抽象方法的接口，需要这种接口的对象时，就可以通过提供一个lambda表达式。这种接口称为函数式接口。 java.util.function包中定义了不少非常通用的函数式接口。例如其中一个接口BiFunction&lt;T,U,R&gt;描述了参数类型为T和U且返回类型为R的函数。（比如可以把比较的lambda表达式保存在这个类型的变量中。当然，没多少人喜欢在sort的时候接收一个BiFunction。） 方法引用对象或类型::方法名 object::instanceMethod Class::staticMethod Class::instanceMethod 前两种方法引用等价于提供方法参数的lambda表达式，而对于第三种，第一个参数会成为方法的目标 例子：Arrays.sort(strings,String::compareToIgnoreCase)不考虑字母的大小写进行排序 构造器引用构造器引用同方法引用类似，不过方法名为new。如：String:new 变量的作用域lambda表达式的三个部分： 代码块 参数 自由变量的值（非参数，且不在代码块中定义的变量） 注意：即对于自由变量，lambda表达式需要数据结构对其进行存储，而为了明确其捕获到的自由变量的值，lambda表达式中只能引用值不会改变的变量（常量） 补充：关于代码块和自由变量值有一个术语：闭包 在lambda表达式中使用this关键字，值创建这个lambda表达式的方法的this参数处理lambda表达式使用lambda表达式的重点是延迟执行。而希望一个代码延迟执行的原因有很多： 在一个单独的线程中运行的代码 多次运行代码 在算法的适当位置运行代码（如排序比较） 发生某种事件时执行代码（如点击一个按钮之类）… 常用函数式接口 基本类型函数式接口可以使用这些来减少装箱拆箱 补充：如果自己设计接口，其中只有一个抽象方法，可以使用@Functionallnterface注解来标记这个接口（好处：一方面无意增加了另一个抽象方法编译器会提示报错，另一方面javadoc页会指出这是一个函数式接口）","categories":[{"name":"Java","slug":"Java","permalink":"http://example.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"lambda表达式","slug":"lambda表达式","permalink":"http://example.com/tags/lambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/"}],"author":"John Doe"},{"title":"SLF4J那些事","slug":"SLF4J那些事","date":"2022-04-29T07:21:00.000Z","updated":"2022-04-29T08:49:21.775Z","comments":true,"path":"2022/04/29/SLF4J那些事/","link":"","permalink":"http://example.com/2022/04/29/SLF4J%E9%82%A3%E4%BA%9B%E4%BA%8B/","excerpt":"","text":"Java中的日志框架 门面日志框架 不同的日志框架，有着不同的api和配置文件，为了统一应用中不同的日志框架所带来的统一管理问题，引入了门面日志框架，向上提供了统一的接口管理，向下对接不同的日志框架实现。 具体流程 关于SL4J的适配 总结 引入jar包slf4-api.jar 引入适配层jar包（如果需要的话） 引入底层日志框架的jar包 确认是否版本安全补充","categories":[{"name":"Java","slug":"Java","permalink":"http://example.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"日志","slug":"日志","permalink":"http://example.com/tags/%E6%97%A5%E5%BF%97/"},{"name":"SL4J","slug":"SL4J","permalink":"http://example.com/tags/SL4J/"}],"author":"John Doe"},{"title":"Java SPI机制","slug":"Java-SPI机制","date":"2022-04-29T01:06:00.000Z","updated":"2022-04-29T01:35:01.943Z","comments":true,"path":"2022/04/29/Java-SPI机制/","link":"","permalink":"http://example.com/2022/04/29/Java-SPI%E6%9C%BA%E5%88%B6/","excerpt":"","text":"基本概念SPI（Serivce Provider Interface）：它是从java6开始引入的，一种基于classloader来发现并加载服务的机制。 一个标准的SPI有三个组件构成： - Service：是一个公开的接口或抽象类，定义了一个抽象的功能模块 - Service Provider：service的一个实现类 - ServiceLoader：核心组件，负责在运行时发现并加载service provider SPI运行流程 Application无需关注service的具体实现，只需面向接口编程 Java SPI在JDBC中的应用 在Java SPI前，我们需要编码去注册驱动Class.forName(“com.mysql.jdbc.Driver”) 在引入Java SPI后，我们只需要日引入对应的依赖jar包即可 Java SPI的三大规范要素 小结 作用：提供了一种组件发现和注册的方式，可以用于实现各种插件，或者灵活替换所使用的组件 优点：面向接口编程，优雅的实现模块之间的解耦 设计思想：面向接口+配置文件+反射技术 应用场景：JDBC、SLF4J等 补充：Java SPI和SPringBoot自动装配","categories":[{"name":"Java","slug":"Java","permalink":"http://example.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"SPI","slug":"SPI","permalink":"http://example.com/tags/SPI/"}],"author":"John Doe"},{"title":"MySQL行转列，列转行","slug":"MySQL行转列，列转行","date":"2022-04-26T12:09:00.000Z","updated":"2022-04-26T12:41:41.561Z","comments":true,"path":"2022/04/26/MySQL行转列，列转行/","link":"","permalink":"http://example.com/2022/04/26/MySQL%E8%A1%8C%E8%BD%AC%E5%88%97%EF%BC%8C%E5%88%97%E8%BD%AC%E8%A1%8C/","excerpt":"","text":"行转列 建表 CREATE TABLE tb_score( id INT(11) NOT NULL auto_increment, userid VARCHAR(20) NOT NULL COMMENT &#39;用户id&#39;, subject VARCHAR(20) COMMENT &#39;科目&#39;, score DOUBLE COMMENT &#39;成绩&#39;, PRIMARY KEY(id) )ENGINE = INNODB DEFAULT CHARSET = utf8; 插入数据 INSERT INTO tb_score(userid,subject,score) VALUES (&#39;001&#39;,&#39;语文&#39;,90); INSERT INTO tb_score(userid,subject,score) VALUES (&#39;001&#39;,&#39;数学&#39;,92); INSERT INTO tb_score(userid,subject,score) VALUES (&#39;001&#39;,&#39;英语&#39;,80); INSERT INTO tb_score(userid,subject,score) VALUES (&#39;002&#39;,&#39;语文&#39;,88); INSERT INTO tb_score(userid,subject,score) VALUES (&#39;002&#39;,&#39;数学&#39;,90); INSERT INTO tb_score(userid,subject,score) VALUES (&#39;002&#39;,&#39;英语&#39;,75.5); INSERT INTO tb_score(userid,subject,score) VALUES (&#39;003&#39;,&#39;语文&#39;,70); INSERT INTO tb_score(userid,subject,score) VALUES (&#39;003&#39;,&#39;数学&#39;,85); INSERT INTO tb_score(userid,subject,score) VALUES (&#39;003&#39;,&#39;英语&#39;,90); INSERT INTO tb_score(userid,subject,score) VALUES (&#39;003&#39;,&#39;政治&#39;,82); 查询数据表中的内容（即转换前的结果） select * from tb_score; 使用case…when….then 进行行转列 SELECT userid, SUM(CASE `subject` WHEN &#39;语文&#39; THEN score ELSE 0 END) AS &#39;语文&#39;, SUM(CASE `subject` WHEN &#39;数学&#39; THEN score ELSE 0 END) AS &#39;数学&#39;, SUM(CASE `subject` WHEN &#39;英语&#39; THEN score ELSE 0 END) AS &#39;英语&#39;, SUM(CASE `subject` WHEN &#39;政治&#39; THEN score ELSE 0 END) AS &#39;政治&#39; FROM tb_score GROUP BY userid 使用IF() 进行行转列：SELECT userid, SUM(IF(`subject`=&#39;语文&#39;,score,0)) AS &#39;语文&#39;, SUM(IF(`subject`=&#39;数学&#39;,score,0)) AS &#39;数学&#39;, SUM(IF(`subject`=&#39;英语&#39;,score,0)) AS &#39;英语&#39;, SUM(IF(`subject`=&#39;政治&#39;,score,0)) AS &#39;政治&#39; FROM tb_score GROUP BY userid 注意点： （1）SUM() 是为了能够使用GROUP BY根据userid进行分组，因为每一个userid对应的subject=”语文”的记录只有一条，所以SUM() 的值就等于对应那一条记录的score的值。假如userid =’001’ and subject=’语文’ 的记录有两条，则此时SUM() 的值将会是这两条记录的和，同理，使用Max()的值将会是这两条记录里面值最大的一个。但是正常情况下，一个user对应一个subject只有一个分数，因此可以使用SUM()、MAX()、MIN()、AVG()等聚合函数都可以达到行转列的效果。 （2）IF(subject=’语文’,score,0) 作为条件，即对所有subject=’语文’的记录的score字段进行SUM()、MAX()、MIN()、AVG()操作，如果score没有值则默认为0。 计算行列和 SELECT IFNULL(userid,&#39;TOTAL&#39;) AS userid, SUM(IF(`subject`=&#39;语文&#39;,score,0)) AS 语文, SUM(IF(`subject`=&#39;数学&#39;,score,0)) AS 数学, SUM(IF(`subject`=&#39;英语&#39;,score,0)) AS 英语, SUM(IF(`subject`=&#39;政治&#39;,score,0)) AS 政治, SUM(score) AS TOTAL FROM tb_score GROUP BY userid WITH ROLLUP; 合并字段显示：利用group_concat() SELECT userid,GROUP_CONCAT(`subject`,&quot;:&quot;,score)AS 成绩 FROM tb_score GROUP BY userid 列转行将每个userid对应的多个科目的成绩查出来，通过UNION ALL将结果集加起来 附：UNION与UNION ALL的区别（摘）： 1.对重复结果的处理：UNION会去掉重复记录，UNION ALL不会； 2.对排序的处理：UNION会排序，UNION ALL只是简单地将两个结果集合并； 3.效率方面的区别：因为UNION 会做去重和排序处理，因此效率比UNION ALL慢很多；","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"查询","slug":"查询","permalink":"http://example.com/tags/%E6%9F%A5%E8%AF%A2/"},{"name":"行列转换","slug":"行列转换","permalink":"http://example.com/tags/%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2/"}],"author":"John Doe"},{"title":"记一次安装Zookeeper启动失败的坑","slug":"记一次安装Zookeeper启动失败的坑","date":"2022-04-21T13:36:00.000Z","updated":"2022-04-21T13:43:12.434Z","comments":true,"path":"2022/04/21/记一次安装Zookeeper启动失败的坑/","link":"","permalink":"http://example.com/2022/04/21/%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%AE%89%E8%A3%85Zookeeper%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5%E7%9A%84%E5%9D%91/","excerpt":"","text":"今天在阿里云上安装ZooKeeper，然后启动时一直报: Starting zookeeper … FAILED TO START。折腾了一会，经查阅资料发现，原来和版本有莫大关系… 1. 下载的版本问题（&gt;= 3.5.5）实际上只要 &gt;= 3.5.5 版本都会出现这种问题。 问题原因：下载了错误的版本文件，Zookeeper 从3.5.5后开始拆分为两个版本，而且他们的结构还很类似。 标准版本（Apache ZooKeeper x.y.z ），下载的文件名为：apache-zookeeper-x.y.z-bin.tar.gz 另一个是源码版本（Apache ZooKeeper x.y.z Source Release），下载的文件名为：apache-zookeeper-x.y.z.tar.gz 所以下载 Zookeeper 的时候要注意，应该下载第一个(本人头铁，下载了第二个)。 2. 端口冲突问题（&gt;=3.5.0） 在3.5.5版本及以上，Zookeeper 提供了一个内嵌的Jetty容器来运行 AdminServer，默认占用的是 8080端口，AdminServer 主要是来查看 Zookeeper 的一些状态，如果机器上有其他程序（比如：Tomcat）占用了 8080 端口，也会导致 Starting zookeeper … FAILED TO START 的问题。 可以通过以下几种方式去解决： 禁用 AdminServer 服务 admin.enableServer=false 修改器端口号： admin.serverPort=9000 转载自：https://blog.csdn.net/peng2hui1314/article/details/107255142","categories":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"http://example.com/categories/Zookeeper/"}],"tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"http://example.com/tags/Zookeeper/"}],"author":"John Doe"},{"title":"重排序","slug":"重排序","date":"2022-04-17T02:01:00.000Z","updated":"2022-04-17T02:13:06.463Z","comments":true,"path":"2022/04/17/重排序/","link":"","permalink":"http://example.com/2022/04/17/%E9%87%8D%E6%8E%92%E5%BA%8F/","excerpt":"","text":"重排序是指编译器和处理器为了优化程序性能而对指令序列进行重新排序的一种手段。 数据依赖性如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间 就存在数据依赖性。数据依赖分为下列3种类型 上面3种情况，只要重排序两个操作的执行顺序，程序的执行结果就会被改变。 而编译器和处理器可能会对操作做重排序。编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序。 这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作， 不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑。 as-if-serial语义 as-if-serial语义的意思是：不管怎么重排序（编译器和处理器为了提高并行度），（单线程） 程序的执行结果不能被改变。编译器、runtime和处理器都必须遵守as-if-serial语义。 为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因 为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作就可能被 编译器和处理器重排序。 as-if-serial语义把单线程程序保护了起来，遵守as-if-serial语义的编译器、runtime和处理器 共同为编写单线程程序的程序员创建了一个幻觉：单线程程序是按程序的顺序来执行的。as- if-serial语义使单线程程序员无需担心重排序会干扰他们，也无需担心内存可见性问题。 程序顺序规则 1）A happens-before B。 2）B happens-before C。 3）A happens-before C。 这里的第3个happens-before关系，是根据happens-before的传递性推导出来的。 这里A happens-before B，但实际执行时B却可以排在A之前执行（看上面的重排序后的执 行顺序）。如果A happens-before B，JMM并不要求A一定要在B之前执行。JMM仅仅要求前一个 操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前。这里操作A 的执行结果不需要对操作B可见；而且重排序操作A和操作B后的执行结果，与操作A和操作B 按happens-before顺序执行的结果一致。在这种情况下，JMM会认为这种重排序并不非法（not illegal），JMM允许这种重排序。 在计算机中，软件技术和硬件技术有一个共同的目标：在不改变程序执行结果的前提下， 尽可能提高并行度。编译器和处理器遵从这一目标，从happens-before的定义我们可以看出， JMM同样遵从这一目标。","categories":[{"name":"并发编程","slug":"并发编程","permalink":"http://example.com/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"JMM","slug":"JMM","permalink":"http://example.com/tags/JMM/"},{"name":"并发编程","slug":"并发编程","permalink":"http://example.com/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"重排序","slug":"重排序","permalink":"http://example.com/tags/%E9%87%8D%E6%8E%92%E5%BA%8F/"}],"author":"John Doe"},{"title":"Java内存模式的基础","slug":"Java内存模式的基础","date":"2022-04-17T01:11:00.000Z","updated":"2022-04-17T01:52:43.054Z","comments":true,"path":"2022/04/17/Java内存模式的基础/","link":"","permalink":"http://example.com/2022/04/17/Java%E5%86%85%E5%AD%98%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%9F%BA%E7%A1%80/","excerpt":"","text":"并发编程模型的两个关键问题在并发编程中，需要处理两个关键问题：线程之间如何通信及线程之间如何同步（这里的线程是指并发执行的活动实体）。 通信是指线程之间以何种机制来交换信息。在命令式编程中，线程之间的通信机制有两种：共享内存和消息传递。 在共享内存的并发模型里，线程之间共享程序的公共状态，通过写-读内存中的公共状态 进行隐式通信。 在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过发送消 息来显式进行通信。 同步是指程序中用于控制不同线程间操作发生相对顺序的机制。 在共享内存并发模型里，同步是显式进行的。程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行。 在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。 Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对 程序员完全透明。如果编写多线程程序的Java程序员不理解隐式进行的线程之间通信的工作 机制，很可能会遇到各种奇怪的内存可见性问题。 Java内存模型的抽象结构在Java中，所有实例域、静态域和数组元素都存储在堆内存中，堆内存在线程之间共享 （本章用“共享变量”这个术语代指实例域，静态域和数组元素）。局部变量（Local Variables），方 法定义参数（Java语言规范称之为Formal Method Parameters）和异常处理器参数（Exception Handler Parameters）不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影 响。 Java线程之间的通信由Java内存模型（本文简称为JMM）控制，JMM决定一个线程对共享 变量的写入何时对另一个线程可见。从抽象的角度来看，JMM定义了线程和主内存之间的抽 象关系：线程之间的共享变量存储在主内存（Main Memory）中，每个线程都有一个私有的本地 内存（Local Memory），本地内存中存储了该线程以读/写共享变量的副本。本地内存是JMM的 一个抽象概念，并不真实存在。它涵盖了缓存、写缓冲区、寄存器以及其他的硬件和编译器优化。 从源代码到指令序列的重排序在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序。重排序分3种类型。 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句 的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism，ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应 机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上 去可能是在乱序执行。 这些重排序可能会导致多线程程序 出现内存可见性问题。对于编译器，JMM的编译器重排序规则会禁止特定类型的编译器重排 序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM的处理器重排序规则会要 求Java编译器在生成指令序列时，插入特定类型的内存屏障（Memory Barriers，Intel称之为 Memory Fence）指令，通过内存屏障指令来禁止特定类型的处理器重排序。 JMM属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。 并发编程模型的分类现代的处理器使用写缓冲区临时保存向内存写入的数据。写缓冲区可以保证指令流水线 持续运行，它可以避免由于处理器停顿下来等待向内存写入数据而产生的延迟。同时，通过以 批处理的方式刷新写缓冲区，以及合并写缓冲区中对同一内存地址的多次写，减少对内存总 线的占用。虽然写缓冲区有这么多好处，但每个处理器上的写缓冲区，仅仅对它所在的处理器 可见。这个特性会对内存操作的执行顺序产生重要的影响：处理器对内存的读/写操作的执行 顺序，不一定与内存实际发生的读/写操作顺序一致！ 这里处理器A和处理器B可以同时把共享变量写入自己的写缓冲区（A1，B1），然后从内存 中读取另一个共享变量（A2，B2），最后才把自己写缓存区中保存的脏数据刷新到内存中（A3， B3）。当以这种时序执行时，程序就可以得到x=y=0的结果。 从内存操作实际发生的顺序来看，直到处理器A执行A3来刷新自己的写缓存区，写操作 A1才算真正执行了。虽然处理器A执行内存操作的顺序为：A1→A2，但内存操作实际发生的顺 序却是A2→A1。此时，处理器A的内存操作顺序被重排序了（处理器B的情况和处理器A一样， 这里就不赘述了）。 这里的关键是，由于写缓冲区仅对自己的处理器可见，它会导致处理器执行内存操作的 顺序可能会与内存实际的操作执行顺序不一致。由于现代的处理器都会使用写缓冲区，因此现代的处理器都会允许对写-读操作进行重排序。 常见的处理器都允许Store-Load重排序；常见的处理器都不允许对 存在数据依赖的操作做重排序。sparc-TSO和X86拥有相对较强的处理器内存模型，它们仅允 许对写-读操作做重排序（因为它们都使用了写缓冲区）。 为了保证内存可见性，Java编译器在生成指令序列的适当位置会插入内存屏障指令来禁 止特定类型的处理器重排序。JMM把内存屏障指令分为4类 StoreLoad Barriers是一个“全能型”的屏障，它同时具有其他3个屏障的效果。现代的多处 理器大多支持该屏障（其他类型的屏障不一定被所有处理器支持）。执行该屏障开销会很昂 贵，因为当前处理器通常要把写缓冲区中的数据全部刷新到内存中（Buffer Fully Flush）。 happens-before简介 从JDK 5开始，Java使用新的JSR-133内存模型（除非特别说明，本文针对的都是JSR-133内 存模型）。JSR-133使用happens-before的概念来阐述操作之间的内存可见性。在JMM中，如果一 个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须要存在happens-before关 系。这里提到的两个操作既可以是在一个线程之内，也可以是在不同线程之间。 与程序员密切相关的happens-before规则如下。 程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。 volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。 注意：两个操作之间具有happens-before关系，并不意味着前一个操作必须要在后一个 操作之前执行！happens-before仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一 个操作按顺序排在第二个操作之前 一个happens-before规则对应于一个或多个编译器和处理器重排序规则。对 于Java程序员来说，happens-before规则简单易懂，它避免Java程序员为了理解JMM提供的内存 可见性保证而去学习复杂的重排序规则以及这些规则的具体实现方法。","categories":[{"name":"JMM","slug":"JMM","permalink":"http://example.com/categories/JMM/"},{"name":"并发编程","slug":"JMM/并发编程","permalink":"http://example.com/categories/JMM/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"JMM","slug":"JMM","permalink":"http://example.com/tags/JMM/"},{"name":"并发编程","slug":"并发编程","permalink":"http://example.com/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"author":"John Doe"},{"title":"First集和Follow集的构造","slug":"First集和Follow集的构造","date":"2022-04-14T10:57:00.000Z","updated":"2022-04-14T11:11:37.751Z","comments":true,"path":"2022/04/14/First集和Follow集的构造/","link":"","permalink":"http://example.com/2022/04/14/First%E9%9B%86%E5%92%8CFollow%E9%9B%86%E7%9A%84%E6%9E%84%E9%80%A0/","excerpt":"","text":"在编译原理语法分析学习中，关于First集和Follow集的构造令我比较费解（课件上的白话太晦涩了，恕我看得一知半解），消化了好一会才明白，特此记录一下。 关于First集对于 X -&gt; … 这条产生式而言： 若右边第一个符号是终结符或 ε ，则直接将其加入 First（X） 若右边第一个符号是非终结符，则将其 First 集的的非 ε 元素加入 First（X） 若右边第一个符号是非终结符而且紧随其后的是很多个非终结符，这个时候就要注意是否有 ε 。 【3.1】若第 i 个非终结符的 First 集有 ε ，则可将第 i+1 个非终结符去除 ε 的 First 集加入 First（X）。 【3.2】若所有的非终结符都能够推导出 ε ，则将 ε 也加入到 First（X） E.G. G[S]: S -&gt; ABCD A -&gt; a | ε B -&gt; b | ε C -&gt; c D -&gt; d 解： First(S) = {a, b, c}，其中 c 是由上面所说的第二、三条规则所推得出来的，因为此时 A 和 B 都可以等于空串（ ε ），所以非终结符 C 的 first 集合就被加入 G[S] 了。 如果这里 C，D 也能够产生 ε 的话，根据第三条规则中的第二小点，此时 First（S） = {a, b, c, d, ε} 关于Follow集 将所有产生式的候选式（即产生式右部）的非终结符都找到，定位到你想要求解 Follow 集的非终结符的位置，从当前位置往后挨个检查。设 A -&gt; aBC 是一个产生式，在这个产生式中， B 和 C 是非终结符，a 是终结符 先检验这个非终结符的右边还有没有别的符号（终结符或非终结符都可以），在例子中 B 是需要检查的第一个非终结符，它的右边是有非终结符 C 的。 若右边有符号 -&gt; 将 First（右侧第一个符号）的非 ε 集合加入到 Follow（当前符号）中，如果 First（右侧第一个符号）含有 ε ，即有 … -&gt; ε ，则将 Follow（产生式左部符号）加入 Follow（当前符号）中。 E.G. 用 A -&gt; aBC 来说就是，当前扫描到 B 了，而 B 的右侧有非终结符 C，则将去掉 ε 的 First（C）加入 Follow（B）中。若存在 C -&gt; ε ，则将 Follow（A）也加入 Follow（B）中。 若右边没有符号了，例如这里的 C，那么可以将 Follow（A）中的元素全部加入到 Follow（C）中。 判断当前符号是不是文法的开始符号，比如 G[A] 中的非终结符 A 就是 G[A] 文法的开始符号，如果是的话就将“#”也加入到 Follow（当前符号）中去 预测表的构造首先构造出预测分析表的第一行与第一列，第一行为文法出现的所有终结符以及‘#’（注意：没有 ε ，因为 Follow 集不含 ε），第一列为文法出现的所有终结符。 然后对文法 G 的每个产生式 A -&gt; ab 都执行如下操作： 【1】对于每个属于 First(ab) 的终结符 m ，都把 A -&gt; ab 添加到预测表中的 [A, m] 中去 【2】如果 ε 也属于 First(ab)，那么对于任何属于 Follow(A) 的字符 x，都把 A -&gt; ε 加入到 [A, x] 中去","categories":[{"name":"编译原理","slug":"编译原理","permalink":"http://example.com/categories/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"}],"tags":[{"name":"上下文无关文法","slug":"上下文无关文法","permalink":"http://example.com/tags/%E4%B8%8A%E4%B8%8B%E6%96%87%E6%97%A0%E5%85%B3%E6%96%87%E6%B3%95/"},{"name":"First","slug":"First","permalink":"http://example.com/tags/First/"},{"name":"Follow","slug":"Follow","permalink":"http://example.com/tags/Follow/"}],"author":"John Doe"},{"title":"初识LSM树","slug":"初识LSM树","date":"2022-04-14T01:09:00.000Z","updated":"2022-04-14T01:37:31.362Z","comments":true,"path":"2022/04/14/初识LSM树/","link":"","permalink":"http://example.com/2022/04/14/%E5%88%9D%E8%AF%86LSM%E6%A0%91/","excerpt":"","text":"今天在看MYSQL45讲的时候，看见一个新词LSM树，抱着好奇和求学的心态，上网查询了一下。对LMS树有了一个简单的认识。特以此篇来记录一下。摘抄自：https://zhuanlan.zhihu.com/p/181498475 简介LSM树并不像B+树、红黑树一样是一颗严格的树状数据结构，它其实是一种存储结构，目前HBase,LevelDB,RocksDB这些NoSQL存储都是采用的LSM树。 对于传统关系型数据库使用btree或一些变体作为存储结构，其能高效进行查找。但保存在磁盘中时它也有一个明显的缺陷，那就是逻辑上相离很近但物理却可能相隔很远，这就可能造成大量的磁盘随机读写。随机读写比顺序读写慢很多，为了提升IO性能，我们需要一种能将随机操作变为顺序操作的机制，于是便有了LSM树。LSM树能让我们进行顺序写磁盘，从而大幅提升写操作，作为代价的是牺牲了一些读性能。 LMS树的核心思想 由上图可知，LMS主要由三个部分组成： MemTableMemTable是在内存中的数据结构，用于保存最近更新的数据，会按照Key有序地组织这些数据，LSM树对于具体如何组织有序地组织数据并没有明确的数据结构定义，例如Hbase使跳跃表来保证内存中key的有序。 因为数据暂时保存在内存中，内存并不是可靠存储，如果断电会丢失数据，因此通常会通过WAL(Write-ahead logging，预写式日志)的方式来保证数据的可靠性。 Immutable MemTable 当 MemTable达到一定大小后，会转化成Immutable MemTable。Immutable MemTable是将转MemTable变为SSTable的一种中间状态。写操作由新的MemTable处理，在转存过程中不阻塞数据更新操作。 SSTable(Sorted String Table) 有序键值对集合，是LSM树组在磁盘中的数据结构。为了加快SSTable的读取，可以通过建立key的索引以及布隆过滤器来加快key的查找。 这里需要关注一个重点，LSM树(Log-Structured-Merge-Tree)正如它的名字一样，LSM树会将所有的数据插入、修改、删除等操作记录(注意是操作记录)保存在内存之中，当此类操作达到一定的数据量后，再批量地顺序写入到磁盘当中。这与B+树不同，B+树数据的更新会直接在原数据所在处修改对应的值，但是LSM数的数据更新是日志式的，当一条数据更新是直接append一条更新记录完成的。这样设计的目的就是为了顺序写，不断地将Immutable MemTable flush到持久化存储即可，而不用去修改之前的SSTable中的key，保证了顺序写。 因此当MemTable达到一定大小flush到持久化存储变成SSTable后，在不同的SSTable中，可能存在相同Key的记录，当然最新的那条记录才是准确的。这样设计的虽然大大提高了写性能，但同时也会带来一些问题： 1）冗余存储，对于某个key，实际上除了最新的那条记录外，其他的记录都是冗余无用的，但是仍然占用了存储空间。因此需要进行Compact操作(合并多个SSTable)来清除冗余的记录。 2）读取时需要从最新的倒着查询，直到找到某个key的记录。最坏情况需要查询完所有的SSTable，这里可以通过前面提到的索引/布隆过滤器来优化查找速度。 LSM树的Compact策略从上面可以看出，Compact操作是十分关键的操作，否则SSTable数量会不断膨胀。在Compact策略上，主要介绍两种基本策略：size-tiered和leveled。 不过在介绍这两种策略之前，先介绍三个比较重要的概念，事实上不同的策略就是围绕这三个概念之间做出权衡和取舍。 读放大:读取数据时实际读取的数据量大于真正的数据量。例如在LSM树中需要先在MemTable查看当前key是否存在，不存在继续从SSTable中寻找。 写放大:写入数据时实际写入的数据量大于真正的数据量。例如在LSM树中写入时可能触发Compact操作，导致实际写入的数据量远大于该key的数据量。 空间放大:数据实际占用的磁盘空间比数据的真正大小更多。上面提到的冗余存储，对于一个key来说，只有最新的那条记录是有效的，而之前的记录都是可以被清理回收的。 size-tiered 策略 size-tiered策略保证每层SSTable的大小相近，同时限制每一层SSTable的数量。如上图，每层限制SSTable为N，当每层SSTable达到N后，则触发Compact操作合并这些SSTable，并将合并后的结果写入到下一层成为一个更大的sstable。 由此可以看出，当层数达到一定数量时，最底层的单个SSTable的大小会变得非常大。并且size-tiered策略会导致空间放大比较严重。即使对于同一层的SSTable，每个key的记录是可能存在多份的，只有当该层的SSTable执行compact操作才会消除这些key的冗余记录。 leveled策略 leveled策略也是采用分层的思想，每一层限制总文件的大小。 但是跟size-tiered策略不同的是，leveled会将每一层切分成多个大小相近的SSTable。这些SSTable是这一层是全局有序的，意味着一个key在每一层至多只有1条记录，不存在冗余记录。之所以可以保证全局有序，是因为合并策略和size-tiered不同，接下来会详细提到。 假设存在以下这样的场景: L1的总大小超过L1本身大小限制： 此时会从L1中选择至少一个文件，然后把它跟L2有交集的部分(非常关键)进行合并。生成的文件会放在L2: 如上图所示，此时L1第二SSTable的key的范围覆盖了L2中前三个SSTable，那么就需要将L1中第二个SSTable与L2中前三个SSTable执行Compact操作。 如果L2合并后的结果仍旧超出L5的阈值大小，需要重复之前的操作 —— 选至少一个文件然后把它合并到下一层: 需要注意的是，多个不相干的合并是可以并发进行的 leveled策略相较于size-tiered策略来说，每层内key是不会重复的，即使是最坏的情况，除开最底层外，其余层都是重复key，按照相邻层大小比例为10来算，冗余占比也很小。因此空间放大问题得到缓解。但是写放大问题会更加突出。举一个最坏场景，如果LevelN层某个SSTable的key的范围跨度非常大，覆盖了LevelN+1层所有key的范围，那么进行Compact时将涉及LevelN+1层的全部数据。 小结从Btree到LSM，其设计思想都和底层息息相关，了解学习不同的思想和底层。有助于我们对不同的框架的学习和思考。","categories":[{"name":"LSM","slug":"LSM","permalink":"http://example.com/categories/LSM/"}],"tags":[{"name":"LSM","slug":"LSM","permalink":"http://example.com/tags/LSM/"}],"author":"John Doe"},{"title":"MySQL EXPLAIN 详解","slug":"MySQL-EXPLAIN-详解","date":"2022-04-10T08:23:00.000Z","updated":"2022-04-10T08:44:36.967Z","comments":true,"path":"2022/04/10/MySQL-EXPLAIN-详解/","link":"","permalink":"http://example.com/2022/04/10/MySQL-EXPLAIN-%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"MySQL 提供了一个EXPALIN 命令，可以用于对SQL语句的执行计划进行分析，并详细的输出分析结果，供开发人员进行针对性的优化。 我们想要查询一条sql有没有用上索引，有没有全表查询，这些都可以通过explain这个命令来查看。 通过explain命令，我们可以深入了解到MySQL的基于开销的优化器，还可以获得很多被优化器考虑到的访问策略的细节以及运行sql语句时哪种策略预计会被优化器采用。 xplain的使用十分简单，通过在查询语句前面加一个explain关键字即可。 参数说明explain 命令一共返回12列信息，分别是： id、select_type、table、partitions、type、possible_keys、key、key_len、ref、rows、filtered、Extra id 列 每个select语句都会自动分配的一个唯一标识符 表示查询中，操作表的顺序，有三种情况id相同，执行顺序从上到下 id不同，如果是子查询，id号会自增，id越大，优先级越高 id相同的不相同的同时存在 id列为null表示为结果集，不需要使用这个语句来查询 select_type 列（很重要）查询类型，主要用于区别 普通查询、联合查询（union、union all）、子查询等复杂查询。 simple：表示不需要union操作或者不包含子查询的简单select查询。有连接查询时，外层的查询为simple，且只有一个。 primary：一个需要使用union的操作或者含有子查询的select，位于最外层的单位查询的select_type即为primary。且只有一个 subquery:除了from子句中包含的子查询外，其它地方出现的子查询都可能时subquery dependent subquery:子查询的结果受到外层的影响 union、union result:union 连接的多表查询，第一个查询primary，后面的是union, 结果集是 union result dependent union:和union一样，出现在union或者union all中，但是这个查询要受到外部查询的影响 derived:在from子句后面的子查询，也叫派生表,注意，在MySql5.6 对于此查询没有优化，所以查询类型是derived.在mysql 5.7 使用了 Merge Derived table 优化，查询类型变为SIMPLE。通过控制参数: optimizer_switch=’derived=on|off’ 决定开始还是优化。默认开启。 table列 显示的查询表名，如果查询使用了别名，那么这里显示的就是别名 如果不涉及对数据表的操作，那么这里就是null 如果显示为尖括号括起来就表示这是一个临时表，N就是执行计划的id，表示结果来自这个查询 如果显示为尖括号括起来的&lt;union n,m&gt;也表示一个临时表，表示来自union查询id为n、m的结果集 partitions 列分区信息 type 列 （重要）依次从好到差： system、const、eq_ref、ref、full_text、ref_or_null、unique_subquery、 index_subquery、range、index_merge、index、all 除了 All 以外，其它的类型都可以用到索引，除了index_merge可以使用多个索引之外，其它的类型最多只能使用到一个索引。 system：表中只有一行数据或者是空表 const：使用唯一索引或者主键，返回记录一定是一条的等值where条件时，通常type是const。 eq_ref:连接字段为主键或者唯一索引，此类型通常出现于多表的join查询，表示对于前表的每一个结果，都对应后表的唯一一条结果。并且查询的比较是=操作，查询效率比较高。 ref: 非主键或者唯一键的等值查询 join连接字段是非主键或者唯一键 最左前缀索引匹配 fulltext:全文检索索引。 ref_or_null:和ref类似，增加了null值判断 unique_subquery、 index_subquery:都是子查询，前者返回唯一值，后者返回可能有重复。 range (重要):索引范围扫描，常用于 &gt;&lt;,is null,between,in,like等 index_merge(索引合并):表示查询使用了两个或者以上的索引数量，常见于and或者or查询匹配上了多个不同索引的字段 index(辅助索引):减少回表次数,因为要查询的索引都在一颗索引树上 all: 全表扫描 possible_keys 列此次查询中，可能选用的索引 key列查询实际使用的索引，select_type为index_merge时，key列可能有多个索引，其它时候这里只会有一个 key_len 列 用于处理查询的索引长度，如果是单列索引，那么整个索引长度都会计算进去，如果是多列索引，那么查询不一定能使用到所有的列，具体使用了多少个列的索引，这里就会计算进去，没有使用到的索引，这里不会计算进去。 留意一下这个长度，计算一下就知道这个索引使用了多少列 另外，key_len 只计算 where 条件使用到索引长度，而排序和分组就算用到了索引也不会计算key_len ref 如果是使用的常数等值查询，这里会显示const 如果是连接查询，被驱动表的执行计划这里会显示驱动表的关联字段 如果是条件使用了表达式或者函数，或者条件列发生了内部隐式转换，这里可能会显示func rows执行计划估算的扫描行数，不是精确值（innodb不是精确值，myisam是精确值，主要是因为innodb使用了mvcc） extra这个列包含很多不适合在其它列显示的重要信息，有很多种，常用的有： using temporary 表示使用了临时表存储中间结果 MySQL在对 order by和group by 时使用临时表 临时表可以是内存临时表和磁盘临时表，执行计划中看不出来，需要查看status变量：used_tmp_table、used_tmp_disk_table才可以看出来 no table used 不带from字句的查询或者from dual查询（explain select 1;） 使用 not in() 形式的子查询查询或者not exists运算符的连接查询，这种叫做反链接 即：一般连接先查询内表再查询外表，反链接就是先查询外表再查询内表 using filesort 排序时无法使用到所以就会出现这个，常见于order by和group by 说明MySQL会使用一个外部的索引进行排序，而不是按照索引顺序进行读取 MySQL中无法利用索引完成的排序就叫“文件排序” using index 查询时候不需要回表 表示相应的select查询中使用到了覆盖索引(Covering index)，避免访问表的数据行 如果同时出现了using where，说明索引被用来执行查询键值如果没有using where，表示读取数据而不是执行查找操作 using where 表示存储引擎返回的记录并不都是符合条件的，需要在server层进行筛选过滤，性能很低 using index condition 索引下推，不需要再在server层进行过滤,5.6.x开始支持 first match 5.6.x 开始出现的优化子查询的新特性之一，常见于where字句含有in()类型的子查询，如果内表数据量过大，可能出现 loosescan 5.6.x 开始出现的优化子查询的新特性之一，常见于where字句含有in()类型的子查询，如果内表返回有重复值，可能出现 filtered 列5.7之后的版本默认就有这个字段，不需要使用explain extended了。这个字段表示存储引擎返回的数据在server层过滤后，剩下多少满足查询的记录数量的比例，注意是百分比，不是具体记录数。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"Explain","slug":"Explain","permalink":"http://example.com/tags/Explain/"}],"author":"John Doe"},{"title":"MySQL日志和索引相关问题","slug":"MySQL日志和索引相关问题","date":"2022-04-10T06:50:00.000Z","updated":"2022-04-10T07:15:47.220Z","comments":true,"path":"2022/04/10/MySQL日志和索引相关问题/","link":"","permalink":"http://example.com/2022/04/10/MySQL%E6%97%A5%E5%BF%97%E5%92%8C%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/","excerpt":"","text":"MySQL怎么知道binlog是完整的?一个事务的binlog是有完整格式的： statement格式的binlog，最后会有COMMIT； row格式的binlog，最后会有一个XID event。另外，在MySQL 5.6.2版本以后，还引入了binlog-checksum参数，用来验证binlog内容的正确性。对于binlog日志由于磁盘原因，可能会在日志中间出错的情况，MySQL可以通过校验checksum的结果来发现。所以，MySQL还是有办法验证事务binlog的完整性的。 redo log 和binlog是怎么关联起来的?它们有一个共同的数据字段，叫XID。崩溃恢复的时候，会按顺序扫描redo log： 如果碰到既有prepare、又有commit的redo log，就直接提交； 如果碰到只有parepare、而没有commit的redo log，就拿着XID去binlog找对应的事务。 为什么需要两阶段提交？先写redo log，崩溃恢复时，必须两个日志都完整才可以，不是一样的吗？回答：其实，两阶段提交是经典的分布式系统问题，并不是MySQL独有的。 如果必须要举一个场景，来说明这么做的必要性的话，那就是事务的持久性问题。 对于InnoDB引擎来说，如果redo log提交完成了，事务就不能回滚（如果这还允许回滚，就可能覆盖掉别的事务的更新）。而如果redo log直接提交，然后binlog写入的时候失败，InnoDB又回滚不了，数据和binlog日志又不一致了。 两阶段提交就是为了给所有人一个机会，当每个人都说“我ok”的时候，再一起提交。 不引入两个日志，也就没有两阶段提交。只需要用redolog支持崩溃恢复和归档不是也可以吗？回答： 如果只从崩溃恢复的角度来讲是可以的。你可以把binlog关掉，这样就没有两阶段提交了，但系统依然是crash-safe的。 但是，如果你了解一下业界各个公司的使用场景的话，就会发现在正式的生产库上，binlog都是开着的。因为binlog有着redo log无法替代的功能。 一个是归档。redo log是循环写，写到末尾是要回到开头继续写的。这样历史日志没法保留，redo log也就起不到归档的作用。 一个就是MySQL系统依赖于binlog。binlog作为MySQL一开始就有的功能，被用在了很多地方。 其中，MySQL系统高可用的基础，就是binlog复制。 还有很多公司有异构系统（比如一些数据分析系统），这些系统就靠消费MySQL的binlog来更新 自己的数据。关掉binlog的话，这些下游系统就没法输入了。 总之，由于现在包括MySQL高可用在内的很多系统机制都依赖于binlog，所以“鸠占鹊巢”redo log还做不到。你看，发展生态是多么重要。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"author":"John Doe"},{"title":"自增id用完了怎么办？","slug":"自增id用完了怎么办？","date":"2022-04-10T05:40:00.000Z","updated":"2022-04-10T06:29:48.479Z","comments":true,"path":"2022/04/10/自增id用完了怎么办？/","link":"","permalink":"http://example.com/2022/04/10/%E8%87%AA%E5%A2%9Eid%E7%94%A8%E5%AE%8C%E4%BA%86%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9F/","excerpt":"","text":"MySQL里有很多自增的id，每个自增id都是定义了初始值，然后不停地往上加步长。虽然自然数 是没有上限的，但是在计算机里，只要定义了表示这个数的字节长度，那它就有上限。比如，无 符号整型(unsigned int)是4个字节，上限就是2 -1。既然自增id有上限，就有可能被用完。但是，自增id用完了会怎么样呢？ 表定义自增id表定义的自增值达到上限后的逻辑是：再申请下一个id时，得到的值保持不变。即当自增id用完，在插入新数据会报错（主键冲突） 2^32 -1（4294967295）不是一个特别大的数，对于一个频繁插入删除数据的表来说，是可能会被 用完的。因此在建表的时候你需要考察你的表是否有可能达到这个上限，如果有可能，就应该创 建成8个字节的bigint unsigned。 InnoDB系统自自增row__id如果你创建的InnoDB表没有指定主键，那么InnoDB会给你创建一个不可见的，长度为6个字节 的row_id。InnoDB维护了一个全局的dict_sys.row_id值，所有无主键的InnoDB表，每插入一行 数据，都将当前的dict_sys.row_id值作为要插入数据的row_id，然后把dict_sys.row_id的值加1。 实际上，在代码实现时row_id是一个长度为8字节的无符号长整型(bigint unsigned)。但 是，InnoDB在设计时，给row_id留的只是6个字节的长度，这样写到数据表中时只放了最后6个 字节，所以row_id能写到数据表中的值，就有两个特征： row_id写入表中的值范围，是从0到2^48 -1； 当dict_sys.row_id=2 时，如果再有插入数据的行为要来申请row_id，拿到以后再取最后6个 字节的话就是0。 也就是说，写入表的row_id是从0开始到2^48 -1。达到上限后，下一个值就是0，然后继续循环。 当然，2^48 -1这个值本身已经很大了，但是如果一个MySQL实例跑得足够久的话，还是可能达到这个上限的。在InnoDB逻辑里，申请到row_id=N后，就将这行数据写入表中；如果表中已经存在row_id=N的行，新写入的行就会覆盖原有的行。 从这个角度看，我们还是应该在InnoDB表中主动创建自增主键。因为，表自增id到达上限后， 再插入数据时报主键冲突错误，是更能被接受的。 毕竟覆盖数据，就意味着数据丢失，影响的是数据可靠性；报主键冲突，是插入失败，影响的是 可用性。而一般情况下，可靠性优先于可用性。 XidMySQL内部维护了一个全局变量global_query_id，每次执行语句的时候将它赋值给Query_id， 然后给这个变量加1。如果当前语句是这个事务执行的第一条语句，那么MySQL还会同时把 Query_id赋值给这个事务的Xid。 而global_query_id是一个纯内存变量，重启之后就清零了。所以你就知道了，在同一个数据库实 例中，不同事务的Xid也是有可能相同的。 但是MySQL重启之后会重新生成新的binlog文件，这就保证了，同一个binlog文件里，Xid一定是 惟一的。 虽然MySQL重启不会导致同一个binlog里面出现两个相同的Xid，但是如果global_query_id达到 上限后，就会继续从0开始计数。从理论上讲，还是就会出现同一个binlog里面出现相同Xid的场景。 因为global_query_id定义的长度是8个字节，这个自增值的上限是2^64 -1。要出现这种情况，必须是下面这样的过程： 执行一个事务，假设Xid是A； 接下来执行2 次查询语句，让global_query_id回到A； 再启动一个事务，这个事务的Xid也是A。不过，2 这个值太大了，大到你可以认为这个可能性只会存在于理论上。 Innodb trx__idXid和InnoDB的trx_id是两个容易混淆的概念。 Xid是由server层维护的。InnoDB内部使用Xid，就是为了能够在InnoDB事务和server之间做关 联。但是，InnoDB自己的trx_id，是另外维护的。 InnoDB内部维护了一个max_trx_id全局变量，每次需要申请一个新的trx_id时，就获得 max_trx_id的当前值，然后并将max_trx_id加1。 InnoDB数据可见性的核心思想是：每一行数据都记录了更新它的trx_id，当一个事务读到一行数据的时候，判断这个数据是否可见的方法，就是通过事务的一致性视图与这行数据的trx_id做对 比。 对于正在执行的事务，你可以从information_schema.innodb_trx表中看到事务的trx_id。 看下面这个例子： session B里，我从innodb_trx表里查出的这两个字段，第二个字段trx_mysql_thread_id就是线程 id。显示线程id，是为了说明这两次查询看到的事务对应的线程id都是5，也就是session A所在的线程。 可以看到，T2时刻显示的trx_id是一个很大的数；T4时刻显示的trx_id是1289，看上去是一个比 较正常的数字。这是什么原因呢？ 实际上，在T1时刻，session A还没有涉及到更新，是一个只读事务。而对于只读事务，InnoDB 并不会分配trx_id。也就是说： 在T1时刻，trx_id的值其实就是0。而这个很大的数，只是显示用的。一会儿我会再和你说说这个数据的生成逻辑。 直到session A 在T3时刻执行insert语句的时候，InnoDB才真正分配了trx_id。所以，T4时刻，session B查到的这个trx_id的值就是1289。 需要注意的是，除了显而易见的修改类语句外，如果在select 语句后面加上for update，这个事 务也不是只读事务。 另外注意： 1. update 和 delete语句除了事务本身，还涉及到标记删除旧数据，也就是要把数据放到purge 队列里等待后续物理删除，这个操作也会把max_trx_id+1， 因此在一个事务中至少加2； 2. InnoDB的后台操作，比如表的索引信息统计这类操作，也是会启动内部事务的，因此你可能看到，trx_id值并不是按照加1递增的。 那么，T2时刻查到的这个很大的数字是怎么来的呢？ 其实，这个数字是每次查询的时候由系统临时计算出来的。它的算法是：把当前事务的trx变量的 指针地址转成整数，再加上2 。使用这个算法，就可以保证以下两点： 因为同一个只读事务在执行期间，它的指针地址是不会变的，所以不论是在 innodb_trx还是 在innodb_locks表里，同一个只读事务查出来的trx_id就会是一样的。 如果有并行的多个只读事务，每个事务的trx变量的指针地址肯定不同。这样，不同的并发只读事务，查出来的trx_id就是不同的。那么，为什么还要再加上2^48呢？在显示值里面加上2 ，目的是要保证只读事务显示的trx_id值比较大，正常情况下就会区别于读 写事务的id。但是，trx_id跟row_id的逻辑类似，定义长度也是8个字节。因此，在理论上还是可 能出现一个读写事务与一个只读事务显示的trx_id相同的情况。不过这个概率很低，并且也没有 什么实质危害，可以不管它。 另一个问题是，只读事务不分配trx__id，有什么好处呢？ 一个好处是，这样做可以减小事务视图里面活跃事务数组的大小。因为当前正在运行的只读事务，是不影响数据的可见性判断的。所以，在创建事务的一致性视图时，InnoDB就只需要拷贝读写事务的trx_id。 另一个好处是，可以减少trx_id的申请次数。在InnoDB里，即使你只是执行一个普通的select语句，在执行过程中，也是要对应一个只读事务的。所以只读事务优化后，普通的查询语句 不需要申请trx_id，就大大减少了并发事务申请trx_id的锁冲突。 由于只读事务不分配trx_id，一个自然而然的结果就是trx_id的增加速度变慢了。 但是，max_trx_id会持久化存储，重启也不会重置为0，那么从理论上讲，只要一个MySQL服务 跑得足够久，就可能出现max_trx_id达到2^48-1的上限，然后从0开始的情况。 当达到这个状态后，MySQL就会持续出现一个脏读的bug，我们来复现一下这个bug。 首先我们需要把当前的max_trx_id先修改成248-1。注意：这个case里使用的是可重复读隔离级 别。具体的操作流程如下： 由于我们已经把系统的max_trx_id设置成了2^48-1，所以在session A启动的事务TA的低水位就是2^48-1 在T2时刻，session B执行第一条update语句的事务id就是2 -1，而第二条update语句的事务id 就是0了，这条update语句执行后生成的数据版本上的trx_id就是0。 在T3时刻，session A执行select语句的时候，判断可见性发现，c=3这个数据版本的trx_id，小于 事务TA的低水位，因此认为这个数据可见。 但，这个是脏读。 由于低水位值会持续增加，而事务id从0开始计数，就导致了系统在这个时刻之后，所有的查询 都会出现脏读的。 并且，MySQL重启时max_trx_id也不会清0，也就是说重启MySQL，这个bug仍然存在。 那么，这个bug也是只存在于理论上吗？ 假设一个MySQL实例的TPS是每秒50万，持续这个压力的话，在17.8年后，就会出现这个情 况。如果TPS更高，这个年限自然也就更短了。但是，从MySQL的真正开始流行到现在，恐怕 都还没有实例跑到过这个上限。不过，这个bug是只要MySQL实例服务时间够长，就会必然出现的。 thread_id接下来，我们再看看线程id（thread_id）。其实，线程id才是MySQL中最常见的一种自增id。平 时我们在查各种现场的时候，showprocesslist里面的第一列，就是thread_id。 thread_id的逻辑很好理解：系统保存了一个全局变量thread_id_counter，每新建一个连接，就 将thread_id_counter赋值给这个新连接的线程变量。 thread_id_counter定义的大小是4个字节，因此达到2 -1后，它就会重置为0，然后继续增加。 但是，你不会在showprocesslist里看到两个相同的thread_id。 这，是因为MySQL设计了一个唯一数组的逻辑，给新线程分配thread_id的时候，逻辑代码是这样的： 小结MySQL不同的自增id达到上限以后的行为。数据库系统作为一个可能需要7*24小时全年无休的服务，考虑这些边界是非常有必要的。 每种自增id有各自的应用场景，在达到上限后的表现也不同： 表的自增id达到上限后，再申请时它的值就不会改变，进而导致继续插入数据时报主键冲突 的错误。 row_id达到上限后，则会归0再重新递增，如果出现相同的row_id，后写的数据会覆盖之前 的数据。 Xid只需要不在同一个binlog文件中出现重复值即可。虽然理论上会出现重复值，但是概率极 小，可以忽略不计。 InnoDB的max_trx_id 递增值每次MySQL重启都会被保存起来，所以我们文章中提到的脏读 的例子就是一个必现的bug，好在留给我们的时间还很充裕。 thread_id是我们使用中最常见的，而且也是处理得最好的一个自增id逻辑了。 不同的自增id有不同的上限值，上限值的大小取决于声明的类型长度。 注：学习自MYSQL45讲","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"自增id","slug":"自增id","permalink":"http://example.com/tags/%E8%87%AA%E5%A2%9Eid/"}],"author":"John Doe"},{"title":"关于Java SE 8 的流库","slug":"关于Java-SE-8-的流库","date":"2022-04-09T06:58:00.000Z","updated":"2022-04-09T08:39:23.181Z","comments":true,"path":"2022/04/09/关于Java-SE-8-的流库/","link":"","permalink":"http://example.com/2022/04/09/%E5%85%B3%E4%BA%8EJava-SE-8-%E7%9A%84%E6%B5%81%E5%BA%93/","excerpt":"","text":"流提供了一种让我们可以在比集合更高的概念级别上去指定计算的数据视图。通过使用流，我们可以说明想要完成什么任务，而不是说明如何去实现它。我们将操作的调度留给具体实现去解决。 1.1 从迭代到流的操作在处理集合时，我们通常会迭代遍历它的元素，并在每个元素上执行某项操作。 而在使用流时，相同的操作是这样的 流的版本比循环版本更易于阅读。将stream修改为parallelstream就可以让流库以并行方式来执行过滤和计数。 流遵循了“做什么,而非怎么做”的原则。在上述例子中，我们描述了需要做什么：获取单词长度，对其计数。我们没有指定该操作应该以什么顺序或者在哪个线程中执行。 流表面上看起来和集合类似，但实际上期存在较大差异： 流并不存储其元素。这些元素可能存储在底层的集合中，或者是按需生成的。 流的操作不会修改器数据源。 流的操作是尽可能的惰性执行。即直至需要结果时，操作才会执行。 还是以上述例子：stream会产生一个用于words的stream，filter会返回另一个流，其中只包含长度大于12的单词，count方法会将这个流简化为一个结果。即： 创建一个流 指定将初始化流转化为其他流的中间操作，可能包含多个步骤 应用终止，产生结果 1.2 流的创建 可以用collection接口的stream方法将任何集合转换为一个流。如果你有一个数组，可以使用静态方法Stream.of方法进行流化。除此之外，也可以使用Array.stream(array,from,to)从数组from到to元素创建一个流。 也可以使用stream.empty创建一个不包含任何元素的流。 stream接口有两个用于创建无限流的静态方法。 generate方法会接收一个不包含任何引元的函数。无论何时，只需要一个流类型的值，该函数会被调用产生一个这样的值。我们可以像下面这样获得一个常量值的流： iterate方法会接收一个“种子”值，以及一个函数，并且反复的将函数应用到之前的结果上，例如： java API中有大量方法可以产生流。 1.3filter、map和flatMap方法 流的转换会产生一个新的流，它的元素派生自另一个流的元素。 filter的引元是Predicate，即从T到boolean的函数。 通常我们想要按照某种方法来转换流中的值，此时，可以使用map方法并传递执行该转换的函数。例如，我们可以用下面的方式，将单词转化为小写： 这里，我们使用的是带有方法引用的map，但是，通常我们可以使用lambda表达式来代替： 在使用map时，会有一个函数应用到每个元素上，并且其结果是包含了应用函数后所产生的所有结果的流。 1.4 抽取子流和连接流 stream.limit（n）会返回一个新的流，它在n个元素之后结束（如果原来的流更短，那么就会在流结束时结束）。这个方法对于剪裁无限流的尺寸会显得特别有用。 会产生包含100个随机数的流。 调用strea.skip(n)则相反，会对其前n个元素。 1.5 其他的流转换 distinct方法会返回一个流，他的元素时从原有流产生的，即剔除掉重复元素的流。 对于流的排序，有多种sorted方法的变体可用。其中一种用于操作comparable元素的流，而另一种则支持comparator。 与所有流转换一样，sorted方法会产生一个新的流（按照规则已经排序）。 最后，peek方法会产生另一个流，在每次获取元素时，都会调用一个函数，对于调式很有用。 1.6 简单约简简单约简是一种终结操作，他们会将流约简为可以在程序中使用的非流值。 count就是其中一种（返回流中的元素） 其他的简单约简还有max和min之类。他们会返回最大值和最小值。而这里返回的值是一个类型Optional的值，它要么在其中包装了答案，要么表示没有任何值（流碰巧为空）。所有Optional的引入是为了避免空指针异常这类问题的出现。 如果只想知道是否匹配，那么可以使用angMatch、allMatch、nonMatch方法。 1.7 收集结果当流处理完后，可以使用iterator方法查看结果，也可以使用forEach（在并行流中，调用forEach会以任意顺序遍历，可以使用forEachOrdered方法，当然该方法会丧失并行处理的优势） 如果想将结果收集到数据结构中，可以使用toArray函数。 如果是收集到另一个目标中，可以使用collect方法。 1.8 收集到映射表中Collectors.toMap方法有两个函数引元，他们用来产生映射表的键和值 如果有多个元素具体相同的键，则会存在冲突，收集器将会抛出一个Illeagel-StateException对象。可以通过第三个函数引元来覆盖这种行为。 未完，待续…","categories":[{"name":"Java","slug":"Java","permalink":"http://example.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"流","slug":"流","permalink":"http://example.com/tags/%E6%B5%81/"}],"author":"John Doe"},{"title":"关于Java泛型","slug":"关于Java泛型","date":"2022-04-09T06:32:00.000Z","updated":"2022-04-09T06:50:29.403Z","comments":true,"path":"2022/04/09/关于Java泛型/","link":"","permalink":"http://example.com/2022/04/09/%E5%85%B3%E4%BA%8EJava%E6%B3%9B%E5%9E%8B/","excerpt":"","text":"泛型的理解： 泛型又称参数化类型，于jdk5.0提出的提特性，解决数据类型的安全性问题 在类声明或实例化时只要指定好需要的具体的类型即可 java的泛型可以保证如果程序在编译时没有发出警告，运行时就不会产生ClassCastException异常。同时，代码更加简介、健壮 泛型的作用是在类声明时通过一个标识表示类中某个属性类型，或者是某个方法返回值的类型，或者是参数类型 泛型的好处： 编译时，检查添加元素的类型，提高了安全性 减少了类型转换的次数，提高了效率 不再提示编译警告 自定义泛型类：public class Solution&lt;K,V&gt; &#123; K k; V v; public K getK()&#123; return null; &#125; &#125; 注意： 普通成员可以使用泛型（属性、方法） 使用泛型的数组不能初始化 静态方法中不能使用类的泛型 泛型类的类型是在创建对象时确定的 如果在创建对象时没有指定类型，默认为object 自定义泛型接口interface B&lt;T&gt;&#123; T getInstance(); &#125; 注意： 接口中，静态成员不能使用泛型 泛型接口的类型在继承接口或者实现接口时确定 没有指定类型，默认为Object 泛型的继承和通配符 泛型不具备继承性 :支持任意泛型类型 : 支持A类以及A的子类，规定了泛型的上限 :支持A类以及A的父类，不限于直接父类，规避了泛型的下限","categories":[{"name":"Java","slug":"Java","permalink":"http://example.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"泛型","slug":"泛型","permalink":"http://example.com/tags/%E6%B3%9B%E5%9E%8B/"}],"author":"John Doe"},{"title":"关于next-key lock的加锁规则","slug":"关于next-key-lock的加锁规则","date":"2022-04-05T08:29:00.000Z","updated":"2022-04-05T10:24:43.455Z","comments":true,"path":"2022/04/05/关于next-key-lock的加锁规则/","link":"","permalink":"http://example.com/2022/04/05/%E5%85%B3%E4%BA%8Enext-key-lock%E7%9A%84%E5%8A%A0%E9%94%81%E8%A7%84%E5%88%99/","excerpt":"","text":"next-key lock的加锁规则总结的加锁规则里面，包含了两个 “ “ 原则 ” ” 、两个 “ “ 优化 ” ” 和一个 “bug” 。 原则 1 ：加锁的基本单位是 next-key lock 。 next-key lock 是前开后闭区间。 原则 2 ：查找过程中访问到的对象才会加锁。任何辅助索引上的锁，或者非索引列上的锁，最终都要回溯到主键上，在主键上也要加一把锁。 优化 1 ：索引上的等值查询，给唯一索引加锁的时候， next-key lock 退化为行锁。也就是说如果InnoDB扫描的是一个主键、或是一个唯一索引的话，那InnoDB只会采用行锁方式来加锁 优化 2 ：索引上（不一定是唯一索引）的等值查询，向右遍历时且最后一个值不满足等值条件的时候， next-keylock 退化为间隙锁。 一个 bug ：唯一索引上的范围查询会访问到不满足条件的第一个值为止。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"next-key lock","slug":"next-key-lock","permalink":"http://example.com/tags/next-key-lock/"}],"author":"John Doe"},{"title":"读已提交下，为什么建议binlog使用row格式","slug":"读已提交下，为什么建议binlog使用row格式","date":"2022-04-05T05:16:00.000Z","updated":"2022-04-05T08:29:04.394Z","comments":true,"path":"2022/04/05/读已提交下，为什么建议binlog使用row格式/","link":"","permalink":"http://example.com/2022/04/05/%E8%AF%BB%E5%B7%B2%E6%8F%90%E4%BA%A4%E4%B8%8B%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BB%BA%E8%AE%AEbinlog%E4%BD%BF%E7%94%A8row%E6%A0%BC%E5%BC%8F/","excerpt":"","text":"next-key lock 实际上是由间隙锁加行锁实现的。而间隙锁是在可重复读隔离级别下才会生效的。因此在读已提交这个隔离级别下，为了解决数据和日志的不一致问题，需要将binlog的格式改为row，而不是statement。（statement记录的是逻辑上的SQL，而row记录的是对应行的变化情况） 解释： CREATE TABLE t ( id int(11) NOT NULL, c int(11) DEFAULT NULL, d int(11) DEFAULT NULL, PRIMARY KEY (id), KEY c (c)) ENGINE=InnoDB; insert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25); 建表执行过程如上所述，首先针对于T3和T5时session A查询会出现幻读问题（语义上，我们是要把d=5的所有数据锁住，而后续SessionB和C都能够操作d=5的数据），其次会出现数据和日志不一致的问题（binlog写入是根据commit提交决定的，因此SessionB会先写入，随后Session C，最后才是Session A，这样导致的结果就是，我们数据库的数据是： 经过 T1 时刻，id=5 这一行变成 (5,5,100)，当然这个结果最终是在 T6 时刻正式提交的 ; 经过 T2 时刻，id=0 这一行变成 (0,5,5); 经过 T4 时刻，表里面多了一行 (1,5,5); 其他行跟这个执行序列无关，保持不变。 而binlog日志里面记录的数据却是这样的：update t set d=5 where id=0; /(0,0,5)/ update t set c=5 where id=0; /(0,5,5)/ insert into t values(1,1,5); /(1,1,5)/ update t set c=5 where id=1; /(1,5,5)/ update t set d=100 where d=5;/* 所有 d=5 的行，d 改成 100*/ 这个语句序列，不论是拿到备库去执行，还是以后用 binlog 来克隆一个库，这三行的结果，都变成了 (0,5,100)、(1,5,100) 和 (5,5,100)。也就是说，id=0 和 id=1 这两行，发生了数据不一致。） 因此在读已提交下，使用Statement格式的binlog日志是不可取的。而如果你改用row格式，那么对于上面这种情况，记录的是对应行的修改，而不是逻辑上的SQL语句，能够避免这种问题。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"读已提交","slug":"读已提交","permalink":"http://example.com/tags/%E8%AF%BB%E5%B7%B2%E6%8F%90%E4%BA%A4/"},{"name":"binlog","slug":"binlog","permalink":"http://example.com/tags/binlog/"}],"author":"John Doe"},{"title":"MySQL中的自增长锁需要注意的点","slug":"MySQL中的自增长锁需要注意的点","date":"2022-04-05T02:13:00.000Z","updated":"2022-04-05T02:24:26.726Z","comments":true,"path":"2022/04/05/MySQL中的自增长锁需要注意的点/","link":"","permalink":"http://example.com/2022/04/05/MySQL%E4%B8%AD%E7%9A%84%E8%87%AA%E5%A2%9E%E9%95%BF%E9%94%81%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E7%82%B9/","excerpt":"","text":"在表设计的时候，int类型自增长的主键一直是我们的最爱，但是如果你不了解自增长主键，那么在某些情况下可能会给你带来一些意想不到的错误。 在innoDB存储引擎中，对每个含有自增长值的表都有一个自增长计数器，当对该表插入记录时，这个计数器会+n，插入方式会根据这个计数器的值确定自增id。而这种插入，为了避免在多线程下的冲突问题，采用了表锁来处理。同时为了提高插入性能，该锁并不是在一个事务提交之后才释放，而是完成对自增长值插入SQL语句后立即释放。尽管如此，这种方式的插入性能还是很差。事务必须等前一个插入完成，其次对于大批量的数据插入影响性能。 因此从MySQL5.1.22开始，innoDB提供了一种轻量级互斥遍历的自增长实现机制，通过innoDB_autoinc_lock_mode来控制自增长的模式。如下图所示 此外，还需要特别注意的是innoDB存储引擎的自增长和MyISAM不同，MyISAM是表级锁，自增长无需考虑并发插入。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"自增主键","slug":"自增主键","permalink":"http://example.com/tags/%E8%87%AA%E5%A2%9E%E4%B8%BB%E9%94%AE/"},{"name":"锁","slug":"锁","permalink":"http://example.com/tags/%E9%94%81/"}],"author":"John Doe"},{"title":"Redis的通信协议RESP","slug":"Redis的通信协议RESP","date":"2022-04-03T14:00:00.000Z","updated":"2022-04-03T14:09:04.344Z","comments":true,"path":"2022/04/03/Redis的通信协议RESP/","link":"","permalink":"http://example.com/2022/04/03/Redis%E7%9A%84%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AERESP/","excerpt":"","text":"我们知道，Redis 客户端与服务端是通过命令的方式来完成交互过程的，主要分为两个部分：网络模型和序列化协议。前者讨论的是数据交互的组织方式，后者讨论的是数据如何序列化。 简介Redis 的通信协议是 Redis Serialization Protocol，翻译为 Redis 序列化协议，简称 RESP。它具有如下特征： 在 TCP 层 是二进制安全的 基于请求 - 响应模式 简单、易懂（人都可以看懂） RESP 所描述的是 Redis 客户端 - 服务端的交互方式。 RESP描述Redis 协议将传输的结构数据分为 5 种类型，单元结束时统一加上回车换行符号 \\r\\n。 单行字符串，第一个字节为 + 错误消息，第一个字节为 - 整型数字，第一个字节为 :，后跟整数的字符串 多行字符串，第一个字节为 $，后跟字符串的长度 数组，第一个字节为 *，后跟跟着数组的长度 请求命令Redis 对每一条请求命令都做了统一规范，格式如下： *&lt;number of arguments&gt; CR LF $&lt;number of bytes of argument 1&gt; CR LF &lt;argument data&gt; CR LF ... $&lt;number of bytes of argument N&gt; CR LF &lt;argument data&gt; CR LF 翻译如下： number of arguments ： 参数的数量 CR LF：\\r\\n number of bytes of argument 1：参数 1 的字节数 number of bytes of argument N：参数 N 的字节数以命令 set userName chenssy 为例，如下： *3 $3 SET $8 userName $7 chenssy 解释： *3 数组，表明有三个参数 SET、userName、chenssy $3 多行字符串，第一个参数 SET ，有 3 个字符 $8 多行字符串，第二个参数 userName，有 8 个字符 $7 多行字符串，第三个参数 chenssy，有 7 个字符 上面只是格式化显示的结果，真正传输的结果如下： *3\\r\\n$3\\r\\nSET\\r\\n$8\\r\\nuserName\\r\\n$7\\r\\nchenssy\\r\\n 回复命令Redis 服务端响应要支持多种数据格式，所以回复命令一般都会显得复杂些，但是无论如何它都逃脱不了上面 5 中类型及其组合。 从上面我们可以看出 RESP 协议是非常简单直观的一种协议，我们肉眼都可以看懂，而且数据结构类型也只有少少的 5 中，所以实现起来就变得很简单了。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"}],"tags":[{"name":"RESP","slug":"RESP","permalink":"http://example.com/tags/RESP/"},{"name":"通信协议","slug":"通信协议","permalink":"http://example.com/tags/%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/"},{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"author":"John Doe"},{"title":"Java中的Queue那些事","slug":"Java中的Queue那些事","date":"2022-04-03T08:37:00.000Z","updated":"2022-04-03T11:42:52.074Z","comments":true,"path":"2022/04/03/Java中的Queue那些事/","link":"","permalink":"http://example.com/2022/04/03/Java%E4%B8%AD%E7%9A%84Queue%E9%82%A3%E4%BA%9B%E4%BA%8B/","excerpt":"","text":"PriorityQueue优先级队列，是0个或多个元素的集合，集合中的每个元素都有一个权重值，每次出队都弹出优先级最大或最小的元素。 主要属性（1）默认容量是11； （2）queue，元素存储在数组中，堆一般使用数组来存储； （3）comparator，比较器，在优先级队列中，也有两种方式比较元素，一种是元素的自然顺序，一种是通过比较器来比较； （4）modCount，修改次数，有这个属性表示PriorityQueue也是fast-fail的； 入队（1）入队不允许null元素； （2）如果数组不够用了，先扩容； （3）如果还没有元素，就插入下标0的位置； （4）如果有元素了，就插入到最后一个元素往后的一个位置（实际并没有插入哈）； （5）自下而上堆化，一直往上跟父节点比较； （6）如果比父节点小，就与父节点交换位置，直到出现比父节点大为止； （7）由此可见，PriorityQueue是一个小顶堆。 扩容（1）当数组比较小（小于64）的时候每次扩容容量翻倍； （2）当数组比较大的时候每次扩容只增加一半的容量； 出队（1）将队列首元素弹出； （2）将队列末元素移到队列首； （3）自上而下堆化，一直往下与最小的子节点比较； （4）如果比最小的子节点大，就交换位置，再继续与最小的子节点比较； （5）如果比最小的子节点小，就不用交换位置了，堆化结束； （6）这就是堆中的删除堆顶元素； 结论（1）PriorityQueue是一个小顶堆； （2）PriorityQueue是非线程安全的； （3）PriorityQueue不是有序的，只有堆顶存储着最小的元素； （4）入队就是堆的插入元素的实现； （5）出队就是堆的删除元素的实现； ArrayBlockingQueueArrayBlockingQueue是java并发包下一个以数组实现的阻塞队列，它是线程安全的 主要属性（1）利用数组存储元素； （2）通过放指针和取指针来标记下一次操作的位置； （3）利用重入锁来保证并发安全； 构造方法（1）ArrayBlockingQueue初始化时必须传入容量，也就是数组的大小； （2）可以通过构造方法控制重入锁的类型是公平锁还是非公平锁； 入队（1）add(e)时如果队列满了则抛出异常； （2）offer(e)时如果队列满了则返回false； （3）put(e)时如果队列满了则使用notFull等待； （4）offer(e, timeout, unit)时如果队列满了则等待一段时间后如果队列依然满就返回false； （5）利用放指针循环使用数组来存储元素； 出队（1）remove()时如果队列为空则抛出异常； （2）poll()时如果队列为空则返回null； （3）take()时如果队列为空则阻塞等待在条件notEmpty上； （4）poll(timeout, unit)时如果队列为空则阻塞等待一段时间后如果还为空就返回null； （5）利用取指针循环从数组中取元素； 总结（1）ArrayBlockingQueue不需要扩容，因为是初始化时指定容量，并循环利用数组； （2）ArrayBlockingQueue利用takeIndex和putIndex循环利用数组； （3）入队和出队各定义了四组方法为满足不同的用途； （4）利用重入锁和两个条件保证并发安全； ArrayBlockingQueue有哪些缺点呢？a）队列长度固定且必须在初始化时指定，所以使用之前一定要慎重考虑好容量； b）如果消费速度跟不上入队速度，则会导致提供者线程一直阻塞，且越阻塞越多，非常危险； c）只使用了一个锁来控制入队出队，效率较低，可以借助分段的思想把入队出队分裂成两个锁。 LinkedBlockingQueueLinkedBlockingQueue是java并发包下一个以单链表实现的阻塞队列，它是线程安全的 主要属性（1）capacity，有容量，可以理解为LinkedBlockingQueue是有界队列 （2）head, last，链表头、链表尾指针 （3）takeLock，notEmpty，take锁及其对应的条件 （4）putLock, notFull，put锁及其对应的条件 （5）入队、出队使用两个不同的锁控制，锁分离，提高效率 入队（1）使用putLock加锁； （2）如果队列满了就阻塞在notFull条件上； （3）否则就入队； （4）如果入队后元素数量小于容量，唤醒其它阻塞在notFull条件上的线程； （5）释放锁； （6）如果放元素之前队列长度为0，就唤醒notEmpty条件； 出队（1）使用takeLock加锁； （2）如果队列空了就阻塞在notEmpty条件上； （3）否则就出队； （4）如果出队前元素数量大于1，唤醒其它阻塞在notEmpty条件上的线程； （5）释放锁； （6）如果取元素之前队列长度等于容量，就唤醒notFull条件； 总结（1）LinkedBlockingQueue采用单链表的形式实现； （2）LinkedBlockingQueue采用两把锁的锁分离技术实现入队出队互不阻塞； （3）LinkedBlockingQueue是有界队列，不传入容量时默认为最大int值； LinkedBlockingQueue与ArrayBlockingQueue对比？a）后者入队出队采用一把锁，导致入队出队相互阻塞，效率低下； b）前才入队出队采用两把锁，入队出队互不干扰，效率较高； c）二者都是有界队列，如果长度相等且出队速度跟不上入队速度，都会导致大量线程阻塞； d）前者如果初始化不传入初始容量，则使用最大int值，如果出队速度跟不上入队速度，会导致队列特别长，占用大量内存； SynchronousQueueSynchronousQueue是java并发包下无缓冲阻塞队列，它用来在两个线程之间移交元素 主要属性（1）这个阻塞队列里面是会自旋的； （2）它使用了一个叫做transferer的东西来交换元素； 主要内部类（1）定义了一个抽象类Transferer，里面定义了一个传输元素的方法； （2）有两种传输元素的方法，一种是栈，一种是队列； （3）栈的特点是后进先出，队列的特点是先进行出； （4）栈只需要保存一个头节点就可以了，因为存取元素都是操作头节点； （5）队列需要保存一个头节点一个尾节点，因为存元素操作尾节点，取元素操作头节点； （6）每个节点中保存着存储的元素、等待着的线程，以及下一个节点； 构造方法（1）默认使用非公平模式，也就是栈结构； （2）公平模式使用队列，非公平模式使用栈； 入队调用transferer的transfer()方法，传入元素e，说明是生产者 出队调用transferer的transfer()方法，传入null，说明是消费者。 transferer（1）如果栈中没有元素，或者栈顶元素跟将要入栈的元素模式一样，就入栈； （2）入栈后自旋等待一会看有没有其它线程匹配到它，自旋完了还没匹配到元素就阻塞等待； （3）阻塞等待被唤醒了说明其它线程匹配到了当前的元素，就返回匹配到的元素； （4）如果两者模式不一样，且头节点没有在匹配中，就拿当前节点跟它匹配，匹配成功了就返回匹配到的元素； （5）如果两者模式不一样，且头节点正在匹配中，当前线程就协助去匹配，匹配完成了再让当前节点重新入栈重新匹配； 总结1）SynchronousQueue是java里的无缓冲队列，用于在两个线程之间直接移交元素； （2）SynchronousQueue有两种实现方式，一种是公平（队列）方式，一种是非公平（栈）方式； （3）栈方式中的节点有三种模式：生产者、消费者、正在匹配中； （4）栈方式的大致思路是如果栈顶元素跟自己一样的模式就入栈并等待被匹配，否则就匹配，匹配到了就返回； SynchronousQueue真的是无缓冲的队列吗？通过源码分析，我们可以发现其实SynchronousQueue内部或者使用栈或者使用队列来存储包含线程和元素值的节点，如果同一个模式的节点过多的话，它们都会存储进来，且都会阻塞着，所以，严格上来说，SynchronousQueue并不能算是一个无缓冲队列。 SynchronousQueue有什么缺点呢？试想一下，如果有多个生产者，但只有一个消费者，如果消费者处理不过来，是不是生产者都会阻塞起来？反之亦然。 这是一件很危险的事，所以，SynchronousQueue一般用于生产、消费的速度大致相当的情况，这样才不会导致系统中过多的线程处于阻塞状态。 PriorityBlockingQueuePriorityBlockingQueue是java并发包下的优先级阻塞队列，它是线程安全的 主要属性（1）依然是使用一个数组来使用元素； （2）使用一个锁加一个notEmpty条件来保证并发安全； （3）使用一个变量的CAS操作来控制扩容； 入队入队的整个操作跟PriorityQueue几乎一致： （1）加锁； （2）判断是否需要扩容； （3）添加元素并做自下而上的堆化； （4）元素个数加1并唤醒notEmpty条件，唤醒取元素的线程； （5）解锁； 扩容（1）解锁，解除offer()方法中加的锁； （2）使用allocationSpinLock变量的CAS操作来控制扩容的过程； （3）旧容量小于64则翻倍，旧容量大于64则增加一半； （4）创建新数组； （5）修改allocationSpinLock为0，相当于解锁； （6）其它线程在扩容的过程中要让出CPU； （7）再次加锁； （8）新数组创建成功，把旧数组元素拷贝过来，并返回到offer()方法中继续添加元素操作； 出队（1）加锁； （2）判断是否出队成功，未成功就阻塞在notEmpty条件上； （3）出队时弹出堆顶元素，并把堆尾元素拿到堆顶； （4）再做自上而下的堆化； （5）解锁； 总结（1）PriorityBlockingQueue整个入队出队的过程与PriorityQueue基本是保持一致的； （2）PriorityBlockingQueue使用一个锁+一个notEmpty条件控制并发安全； （3）PriorityBlockingQueue扩容时使用一个单独变量的CAS操作来控制只有一个线程进行扩容； （4）入队使用自下而上的堆化； （5）出队使用自上而下的堆化； 为什么PriorityBlockingQueue不需要notFull条件？ 因为PriorityBlockingQueue在入队的时候如果没有空间了是会自动扩容的，也就不存在队列满了的状态，也就是不需要等待通知队列不满了可以放元素了，所以也就不需要notFull条件了。 LinkedTransferQueueLinkedTransferQueue是LinkedBlockingQueue、SynchronousQueue（公平模式）、ConcurrentLinkedQueue三者的集合体，它综合了这三者的方法，并且提供了更加高效的实现方式。 继承体系LinkedTransferQueue实现了TransferQueue接口，而TransferQueue接口是继承自BlockingQueue的，所以LinkedTransferQueue也是一个阻塞队列。 存储结构LinkedTransferQueue使用了一个叫做dual data structure的数据结构，或者叫做dual queue，译为双重数据结构或者双重队列。 双重队列是什么意思呢？ 放取元素使用同一个队列，队列中的节点具有两种模式，一种是数据节点，一种是非数据节点。 放元素时先跟队列头节点对比，如果头节点是非数据节点，就让他们匹配，如果头节点是数据节点，就生成一个数据节点放在队列尾端（入队）。 取元素时也是先跟队列头节点对比，如果头节点是数据节点，就让他们匹配，如果头节点是非数据节点，就生成一个非数据节点放在队列尾端（入队）。不管是放元素还是取元素，都先跟头节点对比，如果二者模式不一样就匹配它们，如果二者模式一样，就入队。 典型的单链表结构，内部除了存储元素的值和下一个节点的指针外，还包含了是否为数据节点和持有元素的线程。是无界的一个阻塞队列。 总结（1）LinkedTransferQueue可以看作LinkedBlockingQueue、SynchronousQueue（公平模式）、ConcurrentLinkedQueue三者的集合体； （2）LinkedTransferQueue的实现方式是使用一种叫做双重队列的数据结构； （3）不管是取元素还是放元素都会入队； （4）先尝试跟头节点比较，如果二者模式不一样，就匹配它们，组成CP，然后返回对方的值； （5）如果二者模式一样，就入队，并自旋或阻塞等待被唤醒； （6）至于是否入队及阻塞有四种模式，NOW、ASYNC、SYNC、TIMED； （7）LinkedTransferQueue全程都没有使用synchronized、重入锁等比较重的锁，基本是通过 自旋+CAS 实现； （8）对于入队之后，先自旋一定次数后再调用LockSupport.park()或LockSupport.parkNanos阻塞； LinkedTransferQueue与SynchronousQueue（公平模式）有什么异同呢？ （1）在java8中两者的实现方式基本一致，都是使用的双重队列； （2）前者完全实现了后者，但比后者更灵活； （3）后者不管放元素还是取元素，如果没有可匹配的元素，所在的线程都会阻塞； （4）前者可以自己控制放元素是否需要阻塞线程，比如使用四个添加元素的方法就不会阻塞线程，只入队元素，使用transfer()会阻塞线程； （5）取元素两者基本一样，都会阻塞等待有新的元素进入被匹配到； ConcurrentLinkedQueueConcurrentLinkedQueue只实现了Queue接口，并没有实现BlockingQueue接口，所以它不是阻塞队列，也不能用于线程池中，但是它是线程安全的，可用于多线程环境中。 主要属性就这两个主要属性，一个头节点，一个尾节点。这是一个无界的单链表实现的队列。 入队入队整个流程还是比较清晰的，这里有个前提是出队时会把出队的那个节点的next设置为节点本身。 （1）定位到链表尾部，尝试把新节点到后面； （2）如果尾部变化了，则重新获取尾部，再重试； 出队（1）定位到头节点，尝试更新其值为null； （2）如果成功了，就成功出队； （3）如果失败或者头节点变化了，就重新寻找头节点，并重试； （4）整个出队过程没有一点阻塞相关的代码，所以出队的时候不会阻塞线程，没找到元素就返回null； 总结（1）ConcurrentLinkedQueue不是阻塞队列； （2）ConcurrentLinkedQueue不能用在线程池中； （3）ConcurrentLinkedQueue使用（CAS+自旋）更新头尾节点控制出队入队操作； ConcurrentLinkedQueue与LinkedBlockingQueue对比？ （1）两者都是线程安全的队列； （2）两者都可以实现取元素时队列为空直接返回null，后者的poll()方法可以实现此功能； （3）前者全程无锁，后者全部都是使用重入锁控制的； （4）前者效率较高，后者效率较低； （5）前者无法实现如果队列为空等待元素到来的操作； （6）前者是非阻塞队列，后者是阻塞队列； （7）前者无法用在线程池中，后者可以； DelayQueueDelayQueue是java并发包下的延时阻塞队列，常用于实现定时任务。 从继承体系可以看到，DelayQueue实现了BlockingQueue，所以它是一个阻塞队列。 另外，DelayQueue还组合了一个叫做Delayed的接口，DelayQueue中存储的所有元素必须实现Delayed接口。 Delayed是一个继承自Comparable的接口，并且定义了一个getDelay()方法，用于表示还有多少时间到期，到期了应返回小于等于0的数值。 主要属性从属性我们可以知道，延时队列主要使用优先级队列来实现，并辅以重入锁和条件来控制并发安全。 因为优先级队列是无界的，所以这里只需要一个条件就可以了。 入队（1）加锁； （2）添加元素到优先级队列中； （3）如果添加的元素是堆顶元素，就把leader置为空，并唤醒等待在条件available上的线程； （4）解锁； 出队（1）加锁； （2）检查第一个元素，如果为空或者还没到期，就返回null； （3）如果第一个元素到期了就调用poll()弹出第一个元素； （4）解锁。 总结（1）DelayQueue是阻塞队列； （2）DelayQueue内部存储结构使用优先级队列； （3）DelayQueue使用重入锁和条件来控制并发安全； （4）DelayQueue常用于定时任务； java中的线程池实现定时任务是直接用的DelayQueue吗？ 当然不是，ScheduledThreadPoolExecutor中使用的是它自己定义的内部类DelayedWorkQueue，其实里面的实现逻辑基本都是一样的，只不过DelayedWorkQueue里面没有使用现在的PriorityQueue，而是使用数组又实现了一遍优先级队列，本质上没有什么区别。 ArrayDeque双端队列是一种特殊的队列，它的两端都可以进出元素，故而得名双端队列。 ArrayDeque是一种以数组方式实现的双端队列，它是非线程安全的。 通过继承体系可以看，ArrayDeque实现了Deque接口，Deque接口继承自Queue接口，它是对Queue的一种增强。 从属性我们可以看到，ArrayDeque使用数组存储元素，并使用头尾指针标识队列的头和尾，其最小容量是8。 通过构造方法，我们知道默认初始容量是16，最小容量是8。 入队（1）入队有两种方式，从队列头或者从队列尾； （2）如果容量不够了，直接扩大为两倍； （3）通过取模的方式让头尾指针在数组范围内循环； （4）x &amp; (len - 1) = x % len，使用&amp;的方式更快； 出队（1）出队有两种方式，从队列头或者从队列尾； （2）通过取模的方式让头尾指针在数组范围内循环； （3）出队之后没有缩容哈哈^^ 总结（1）ArrayDeque是采用数组方式实现的双端队列； （2）ArrayDeque的出队入队是通过头尾指针循环利用数组实现的； （3）ArrayDeque容量不足时是会扩容的，每次扩容容量增加一倍； （4）ArrayDeque可以直接作为栈使用； LinkedList（1）LinkedList是一个以双链表实现的List； （2）LinkedList还是一个双端队列，具有队列、双端队列、栈的特性； （3）LinkedList在队列首尾添加、删除元素非常高效，时间复杂度为O(1)； （4）LinkedList在中间添加、删除元素比较低效，时间复杂度为O(n)； （5）LinkedList不支持随机访问，所以访问非队列首尾的元素比较低效； （6）LinkedList在功能上等于ArrayList + ArrayDeque；","categories":[{"name":"Java","slug":"Java","permalink":"http://example.com/categories/Java/"},{"name":"集合","slug":"Java/集合","permalink":"http://example.com/categories/Java/%E9%9B%86%E5%90%88/"}],"tags":[{"name":"集合","slug":"集合","permalink":"http://example.com/tags/%E9%9B%86%E5%90%88/"},{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"Queue","slug":"Queue","permalink":"http://example.com/tags/Queue/"}],"author":"John Doe"},{"title":"Java中的Set那些事","slug":"Java中的Set那些事","date":"2022-04-03T07:31:00.000Z","updated":"2022-04-03T08:00:23.733Z","comments":true,"path":"2022/04/03/Java中的Set那些事/","link":"","permalink":"http://example.com/2022/04/03/Java%E4%B8%AD%E7%9A%84Set%E9%82%A3%E4%BA%9B%E4%BA%8B/","excerpt":"","text":"HashSet（1）HashSet内部使用HashMap的key存储元素，以此来保证元素不重复； （2）HashSet是无序的，因为HashMap的key是无序的； （3）HashSet中允许有一个null元素，因为HashMap允许key为null； （4）HashSet是非线程安全的； （5）HashSet是没有get()方法的； public class HashSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable &#123; static final long serialVersionUID = -5024744406713321676L; // 内部元素存储在HashMap中 private transient HashMap&lt;E,Object&gt; map; // 虚拟元素，用来存到map元素的value中的，没有实际意义 private static final Object PRESENT = new Object(); // 空构造方法 public HashSet() &#123; map = new HashMap&lt;&gt;(); &#125; // 把另一个集合的元素全都添加到当前Set中 // 注意，这里初始化map的时候是计算了它的初始容量的 public HashSet(Collection&lt;? extends E&gt; c) &#123; map = new HashMap&lt;&gt;(Math.max((int) (c.size()/.75f) + 1, 16)); addAll(c); &#125; // 指定初始容量和装载因子 public HashSet(int initialCapacity, float loadFactor) &#123; map = new HashMap&lt;&gt;(initialCapacity, loadFactor); &#125; // 只指定初始容量 public HashSet(int initialCapacity) &#123; map = new HashMap&lt;&gt;(initialCapacity); &#125; // LinkedHashSet专用的方法 // dummy是没有实际意义的, 只是为了跟上上面那个操持方法签名不同而已 HashSet(int initialCapacity, float loadFactor, boolean dummy) &#123; map = new LinkedHashMap&lt;&gt;(initialCapacity, loadFactor); &#125; // 迭代器 public Iterator&lt;E&gt; iterator() &#123; return map.keySet().iterator(); &#125; // 元素个数 public int size() &#123; return map.size(); &#125; // 检查是否为空 public boolean isEmpty() &#123; return map.isEmpty(); &#125; // 检查是否包含某个元素 public boolean contains(Object o) &#123; return map.containsKey(o); &#125; // 添加元素 public boolean add(E e) &#123; return map.put(e, PRESENT)==null; &#125; // 删除元素 public boolean remove(Object o) &#123; return map.remove(o)==PRESENT; &#125; // 清空所有元素 public void clear() &#123; map.clear(); &#125; // 克隆方法 @SuppressWarnings(&quot;unchecked&quot;) public Object clone() &#123; try &#123; HashSet&lt;E&gt; newSet = (HashSet&lt;E&gt;) super.clone(); newSet.map = (HashMap&lt;E, Object&gt;) map.clone(); return newSet; &#125; catch (CloneNotSupportedException e) &#123; throw new InternalError(e); &#125; &#125; // 序列化写出方法 private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException &#123; // 写出非static非transient属性 s.defaultWriteObject(); // 写出map的容量和装载因子 s.writeInt(map.capacity()); s.writeFloat(map.loadFactor()); // 写出元素个数 s.writeInt(map.size()); // 遍历写出所有元素 for (E e : map.keySet()) s.writeObject(e); &#125; // 序列化读入方法 private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; // 读入非static非transient属性 s.defaultReadObject(); // 读入容量, 并检查不能小于0 int capacity = s.readInt(); if (capacity &lt; 0) &#123; throw new InvalidObjectException(&quot;Illegal capacity: &quot; + capacity); &#125; // 读入装载因子, 并检查不能小于等于0或者是NaN(Not a Number) // java.lang.Float.NaN = 0.0f / 0.0f; float loadFactor = s.readFloat(); if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) &#123; throw new InvalidObjectException(&quot;Illegal load factor: &quot; + loadFactor); &#125; // 读入元素个数并检查不能小于0 int size = s.readInt(); if (size &lt; 0) &#123; throw new InvalidObjectException(&quot;Illegal size: &quot; + size); &#125; // 根据元素个数重新设置容量 // 这是为了保证map有足够的容量容纳所有元素, 防止无意义的扩容 capacity = (int) Math.min(size * Math.min(1 / loadFactor, 4.0f), HashMap.MAXIMUM_CAPACITY); // 再次检查某些东西, 不重要的代码忽视掉 SharedSecrets.getJavaOISAccess() .checkArray(s, Map.Entry[].class, HashMap.tableSizeFor(capacity)); // 创建map, 检查是不是LinkedHashSet类型 map = (((HashSet&lt;?&gt;)this) instanceof LinkedHashSet ? new LinkedHashMap&lt;E,Object&gt;(capacity, loadFactor) : new HashMap&lt;E,Object&gt;(capacity, loadFactor)); // 读入所有元素, 并放入map中 for (int i=0; i&lt;size; i++) &#123; @SuppressWarnings(&quot;unchecked&quot;) E e = (E) s.readObject(); map.put(e, PRESENT); &#125; &#125; // 可分割的迭代器, 主要用于多线程并行迭代处理时使用 public Spliterator&lt;E&gt; spliterator() &#123; return new HashMap.KeySpliterator&lt;E,Object&gt;(map, 0, -1, 0, 0); &#125; &#125; LinkedHashSet（1）LinkedHashSet的底层使用LinkedHashMap存储元素。 （2）LinkedHashSet是有序的，它是按照插入的顺序排序的。 注意：LinkedHashSet是不支持按访问顺序对元素排序的，只能按插入顺序排序。 package java.util; // LinkedHashSet继承自HashSet public class LinkedHashSet&lt;E&gt; extends HashSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable &#123; private static final long serialVersionUID = -2851667679971038690L; // 传入容量和装载因子 public LinkedHashSet(int initialCapacity, float loadFactor) &#123; super(initialCapacity, loadFactor, true); &#125; // 只传入容量, 装载因子默认为0.75 public LinkedHashSet(int initialCapacity) &#123; super(initialCapacity, .75f, true); &#125; // 使用默认容量16, 默认装载因子0.75 public LinkedHashSet() &#123; super(16, .75f, true); &#125; // 将集合c中的所有元素添加到LinkedHashSet中 // 好奇怪, 这里计算容量的方式又变了 // HashSet中使用的是Math.max((int) (c.size()/.75f) + 1, 16) // 这一点有点不得其解, 是作者偷懒？ public LinkedHashSet(Collection&lt;? extends E&gt; c) &#123; super(Math.max(2*c.size(), 11), .75f, true); addAll(c); &#125; // 可分割的迭代器, 主要用于多线程并行迭代处理时使用 @Override public Spliterator&lt;E&gt; spliterator() &#123; return Spliterators.spliterator(this, Spliterator.DISTINCT | Spliterator.ORDERED); &#125; &#125; TreeSet（1）TreeSet底层使用NavigableMap存储元素； （2）TreeSet是有序的； （3）TreeSet是非线程安全的； （4）TreeSet实现了NavigableSet接口，而NavigableSet继承自SortedSet接口； （5）TreeSet实现了SortedSet接口； package java.util; // TreeSet实现了NavigableSet接口，所以它是有序的 public class TreeSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements NavigableSet&lt;E&gt;, Cloneable, java.io.Serializable &#123; // 元素存储在NavigableMap中 // 注意它不一定就是TreeMap private transient NavigableMap&lt;E,Object&gt; m; // 虚拟元素, 用来作为value存储在map中 private static final Object PRESENT = new Object(); // 直接使用传进来的NavigableMap存储元素 // 这里不是深拷贝,如果外面的map有增删元素也会反映到这里 // 而且, 这个方法不是public的, 说明只能给同包使用 TreeSet(NavigableMap&lt;E,Object&gt; m) &#123; this.m = m; &#125; // 使用TreeMap初始化 public TreeSet() &#123; this(new TreeMap&lt;E,Object&gt;()); &#125; // 使用带comparator的TreeMap初始化 public TreeSet(Comparator&lt;? super E&gt; comparator) &#123; this(new TreeMap&lt;&gt;(comparator)); &#125; // 将集合c中的所有元素添加的TreeSet中 public TreeSet(Collection&lt;? extends E&gt; c) &#123; this(); addAll(c); &#125; // 将SortedSet中的所有元素添加到TreeSet中 public TreeSet(SortedSet&lt;E&gt; s) &#123; this(s.comparator()); addAll(s); &#125; // 迭代器 public Iterator&lt;E&gt; iterator() &#123; return m.navigableKeySet().iterator(); &#125; // 逆序迭代器 public Iterator&lt;E&gt; descendingIterator() &#123; return m.descendingKeySet().iterator(); &#125; // 以逆序返回一个新的TreeSet public NavigableSet&lt;E&gt; descendingSet() &#123; return new TreeSet&lt;&gt;(m.descendingMap()); &#125; // 元素个数 public int size() &#123; return m.size(); &#125; // 判断是否为空 public boolean isEmpty() &#123; return m.isEmpty(); &#125; // 判断是否包含某元素 public boolean contains(Object o) &#123; return m.containsKey(o); &#125; // 添加元素, 调用map的put()方法, value为PRESENT public boolean add(E e) &#123; return m.put(e, PRESENT)==null; &#125; // 删除元素 public boolean remove(Object o) &#123; return m.remove(o)==PRESENT; &#125; // 清空所有元素 public void clear() &#123; m.clear(); &#125; // 添加集合c中的所有元素 public boolean addAll(Collection&lt;? extends E&gt; c) &#123; // 满足一定条件时直接调用TreeMap的addAllForTreeSet()方法添加元素 if (m.size()==0 &amp;&amp; c.size() &gt; 0 &amp;&amp; c instanceof SortedSet &amp;&amp; m instanceof TreeMap) &#123; SortedSet&lt;? extends E&gt; set = (SortedSet&lt;? extends E&gt;) c; TreeMap&lt;E,Object&gt; map = (TreeMap&lt;E, Object&gt;) m; Comparator&lt;?&gt; cc = set.comparator(); Comparator&lt;? super E&gt; mc = map.comparator(); if (cc==mc || (cc != null &amp;&amp; cc.equals(mc))) &#123; map.addAllForTreeSet(set, PRESENT); return true; &#125; &#125; // 不满足上述条件, 调用父类的addAll()通过遍历的方式一个一个地添加元素 return super.addAll(c); &#125; // 子set（NavigableSet中的方法） public NavigableSet&lt;E&gt; subSet(E fromElement, boolean fromInclusive, E toElement, boolean toInclusive) &#123; return new TreeSet&lt;&gt;(m.subMap(fromElement, fromInclusive, toElement, toInclusive)); &#125; // 头set（NavigableSet中的方法） public NavigableSet&lt;E&gt; headSet(E toElement, boolean inclusive) &#123; return new TreeSet&lt;&gt;(m.headMap(toElement, inclusive)); &#125; // 尾set（NavigableSet中的方法） public NavigableSet&lt;E&gt; tailSet(E fromElement, boolean inclusive) &#123; return new TreeSet&lt;&gt;(m.tailMap(fromElement, inclusive)); &#125; // 子set（SortedSet接口中的方法） public SortedSet&lt;E&gt; subSet(E fromElement, E toElement) &#123; return subSet(fromElement, true, toElement, false); &#125; // 头set（SortedSet接口中的方法） public SortedSet&lt;E&gt; headSet(E toElement) &#123; return headSet(toElement, false); &#125; // 尾set（SortedSet接口中的方法） public SortedSet&lt;E&gt; tailSet(E fromElement) &#123; return tailSet(fromElement, true); &#125; // 比较器 public Comparator&lt;? super E&gt; comparator() &#123; return m.comparator(); &#125; // 返回最小的元素 public E first() &#123; return m.firstKey(); &#125; // 返回最大的元素 public E last() &#123; return m.lastKey(); &#125; // 返回小于e的最大的元素 public E lower(E e) &#123; return m.lowerKey(e); &#125; // 返回小于等于e的最大的元素 public E floor(E e) &#123; return m.floorKey(e); &#125; // 返回大于等于e的最小的元素 public E ceiling(E e) &#123; return m.ceilingKey(e); &#125; // 返回大于e的最小的元素 public E higher(E e) &#123; return m.higherKey(e); &#125; // 弹出最小的元素 public E pollFirst() &#123; Map.Entry&lt;E,?&gt; e = m.pollFirstEntry(); return (e == null) ? null : e.getKey(); &#125; public E pollLast() &#123; Map.Entry&lt;E,?&gt; e = m.pollLastEntry(); return (e == null) ? null : e.getKey(); &#125; // 克隆方法 @SuppressWarnings(&quot;unchecked&quot;) public Object clone() &#123; TreeSet&lt;E&gt; clone; try &#123; clone = (TreeSet&lt;E&gt;) super.clone(); &#125; catch (CloneNotSupportedException e) &#123; throw new InternalError(e); &#125; clone.m = new TreeMap&lt;&gt;(m); return clone; &#125; // 序列化写出方法 private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException &#123; // Write out any hidden stuff s.defaultWriteObject(); // Write out Comparator s.writeObject(m.comparator()); // Write out size s.writeInt(m.size()); // Write out all elements in the proper order. for (E e : m.keySet()) s.writeObject(e); &#125; // 序列化写入方法 private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; // Read in any hidden stuff s.defaultReadObject(); // Read in Comparator @SuppressWarnings(&quot;unchecked&quot;) Comparator&lt;? super E&gt; c = (Comparator&lt;? super E&gt;) s.readObject(); // Create backing TreeMap TreeMap&lt;E,Object&gt; tm = new TreeMap&lt;&gt;(c); m = tm; // Read in size int size = s.readInt(); tm.readTreeSet(size, s, PRESENT); &#125; // 可分割的迭代器 public Spliterator&lt;E&gt; spliterator() &#123; return TreeMap.keySpliteratorFor(m); &#125; // 序列化id private static final long serialVersionUID = -2479143000061671589L; &#125; （1）我们知道TreeSet和LinkedHashSet都是有序的，那它们有何不同？ LinkedHashSet并没有实现SortedSet接口，它的有序性主要依赖于LinkedHashMap的有序性，所以它的有序性是指按照插入顺序保证的有序性；而TreeSet实现了SortedSet接口，它的有序性主要依赖于NavigableMap的有序性，而NavigableMap又继承自SortedMap，这个接口的有序性是指按照key的自然排序保证的有序性，而key的自然排序又有两种实现方式，一种是key实现Comparable接口，一种是构造方法传入Comparator比较器。 （2）TreeSet里面真的是使用TreeMap来存储元素的吗？ 我们知道TreeSet里面实际上是使用的NavigableMap来存储元素，虽然大部分时候这个map确实是TreeMap，但不是所有时候都是TreeMap。所以，TreeSet的底层不完全是使用TreeMap来实现的，更准确地说，应该是NavigableMap。 CopyOnWriteArraySet（1）CopyOnWriteArraySet是用Map实现的吗？ CopyOnWriteArraySet底层是使用CopyOnWriteArrayList存储元素的，所以它并不是使用Map来存储元素的。 （2）CopyOnWriteArraySet是有序的吗？ 是有序的 （3）CopyOnWriteArraySet是并发安全的吗？ 因为底层是使用了CopyOnWriteArrayList，因此CopyOnWriteArraySet是并发安全的，而且是读写分离的。 （4）CopyOnWriteArraySet以何种方式保证元素不重复？ CopyOnWriteArrayList底层其实是一个数组，它是允许元素重复的。而CopyOnWriteArraySet通过调用其addIfAbsent来保证元素的不重复 public class CopyOnWriteArraySet&lt;E&gt; extends AbstractSet&lt;E&gt; implements java.io.Serializable &#123; private static final long serialVersionUID = 5457747651344034263L; // 内部使用CopyOnWriteArrayList存储元素 private final CopyOnWriteArrayList&lt;E&gt; al; // 构造方法 public CopyOnWriteArraySet() &#123; al = new CopyOnWriteArrayList&lt;E&gt;(); &#125; // 将集合c中的元素初始化到CopyOnWriteArraySet中 public CopyOnWriteArraySet(Collection&lt;? extends E&gt; c) &#123; if (c.getClass() == CopyOnWriteArraySet.class) &#123; // 如果c是CopyOnWriteArraySet类型，说明没有重复元素， // 直接调用CopyOnWriteArrayList的构造方法初始化 @SuppressWarnings(&quot;unchecked&quot;) CopyOnWriteArraySet&lt;E&gt; cc = (CopyOnWriteArraySet&lt;E&gt;)c; al = new CopyOnWriteArrayList&lt;E&gt;(cc.al); &#125; else &#123; // 如果c不是CopyOnWriteArraySet类型，说明有重复元素 // 调用CopyOnWriteArrayList的addAllAbsent()方法初始化 // 它会把重复元素排除掉 al = new CopyOnWriteArrayList&lt;E&gt;(); al.addAllAbsent(c); &#125; &#125; // 获取元素个数 public int size() &#123; return al.size(); &#125; // 检查集合是否为空 public boolean isEmpty() &#123; return al.isEmpty(); &#125; // 检查是否包含某个元素 public boolean contains(Object o) &#123; return al.contains(o); &#125; // 集合转数组 public Object[] toArray() &#123; return al.toArray(); &#125; // 集合转数组，这里是可能有bug的，详情见ArrayList中分析 public &lt;T&gt; T[] toArray(T[] a) &#123; return al.toArray(a); &#125; // 清空所有元素 public void clear() &#123; al.clear(); &#125; // 删除元素 public boolean remove(Object o) &#123; return al.remove(o); &#125; // 添加元素 // 这里是调用CopyOnWriteArrayList的addIfAbsent()方法 // 它会检测元素不存在的时候才添加 // 还记得这个方法吗？当时有分析过的，建议把CopyOnWriteArrayList拿出来再看看 public boolean add(E e) &#123; return al.addIfAbsent(e); &#125; // 是否包含c中的所有元素 public boolean containsAll(Collection&lt;?&gt; c) &#123; return al.containsAll(c); &#125; // 并集 public boolean addAll(Collection&lt;? extends E&gt; c) &#123; return al.addAllAbsent(c) &gt; 0; &#125; // 单方向差集 public boolean removeAll(Collection&lt;?&gt; c) &#123; return al.removeAll(c); &#125; // 交集 public boolean retainAll(Collection&lt;?&gt; c) &#123; return al.retainAll(c); &#125; // 迭代器 public Iterator&lt;E&gt; iterator() &#123; return al.iterator(); &#125; // equals()方法 public boolean equals(Object o) &#123; // 如果两者是同一个对象，返回true if (o == this) return true; // 如果o不是Set对象，返回false if (!(o instanceof Set)) return false; Set&lt;?&gt; set = (Set&lt;?&gt;)(o); Iterator&lt;?&gt; it = set.iterator(); // 集合元素数组的快照 Object[] elements = al.getArray(); int len = elements.length; // 我觉得这里的设计不太好 // 首先，Set中的元素本来就是不重复的，所以不需要再用个matched[]数组记录有没有出现过 // 其次，两个集合的元素个数如果不相等，那肯定不相等了，这个是不是应该作为第一要素先检查 boolean[] matched = new boolean[len]; int k = 0; // 从o这个集合开始遍历 outer: while (it.hasNext()) &#123; // 如果k&gt;len了，说明o中元素多了 if (++k &gt; len) return false; // 取值 Object x = it.next(); // 遍历检查是否在当前集合中 for (int i = 0; i &lt; len; ++i) &#123; if (!matched[i] &amp;&amp; eq(x, elements[i])) &#123; matched[i] = true; continue outer; &#125; &#125; // 如果不在当前集合中，返回false return false; &#125; return k == len; &#125; // 移除满足过滤条件的元素 public boolean removeIf(Predicate&lt;? super E&gt; filter) &#123; return al.removeIf(filter); &#125; // 遍历元素 public void forEach(Consumer&lt;? super E&gt; action) &#123; al.forEach(action); &#125; // 分割的迭代器 public Spliterator&lt;E&gt; spliterator() &#123; return Spliterators.spliterator (al.getArray(), Spliterator.IMMUTABLE | Spliterator.DISTINCT); &#125; // 比较两个元素是否相等 private static boolean eq(Object o1, Object o2) &#123; return (o1 == null) ? o2 == null : o1.equals(o2); &#125; &#125; ConcurrentSkipListSet（1）ConcurrentSkipListSet的底层是ConcurrentSkipListMap吗？ ConcurrentSkipListSet底层是通过ConcurrentNavigableMap来实现的， （2）ConcurrentSkipListSet是线程安全的吗？ 它是一个有序的线程安全的集合。 （3）ConcurrentSkipListSet是有序的吗？ 有序的 （4）ConcurrentSkipListSet和之前讲的Set有何不同？ ConcurrentSkipListSet基本上都是使用ConcurrentSkipListMap实现的，虽然取子set部分是使用ConcurrentSkipListMap中的内部类，但是这些内部类其实也是和ConcurrentSkipListMap相关的，它们返回ConcurrentSkipListMap的一部分数据。 // 实现了NavigableSet接口，并没有所谓的ConcurrentNavigableSet接口 public class ConcurrentSkipListSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements NavigableSet&lt;E&gt;, Cloneable, java.io.Serializable &#123; private static final long serialVersionUID = -2479143111061671589L; // 存储使用的map private final ConcurrentNavigableMap&lt;E,Object&gt; m; // 初始化 public ConcurrentSkipListSet() &#123; m = new ConcurrentSkipListMap&lt;E,Object&gt;(); &#125; // 传入比较器 public ConcurrentSkipListSet(Comparator&lt;? super E&gt; comparator) &#123; m = new ConcurrentSkipListMap&lt;E,Object&gt;(comparator); &#125; // 使用ConcurrentSkipListMap初始化map // 并将集合c中所有元素放入到map中 public ConcurrentSkipListSet(Collection&lt;? extends E&gt; c) &#123; m = new ConcurrentSkipListMap&lt;E,Object&gt;(); addAll(c); &#125; // 使用ConcurrentSkipListMap初始化map // 并将有序Set中所有元素放入到map中 public ConcurrentSkipListSet(SortedSet&lt;E&gt; s) &#123; m = new ConcurrentSkipListMap&lt;E,Object&gt;(s.comparator()); addAll(s); &#125; // ConcurrentSkipListSet类内部返回子set时使用的 ConcurrentSkipListSet(ConcurrentNavigableMap&lt;E,Object&gt; m) &#123; this.m = m; &#125; // 克隆方法 public ConcurrentSkipListSet&lt;E&gt; clone() &#123; try &#123; @SuppressWarnings(&quot;unchecked&quot;) ConcurrentSkipListSet&lt;E&gt; clone = (ConcurrentSkipListSet&lt;E&gt;) super.clone(); clone.setMap(new ConcurrentSkipListMap&lt;E,Object&gt;(m)); return clone; &#125; catch (CloneNotSupportedException e) &#123; throw new InternalError(); &#125; &#125; /* ---------------- Set operations -------------- */ // 返回元素个数 public int size() &#123; return m.size(); &#125; // 检查是否为空 public boolean isEmpty() &#123; return m.isEmpty(); &#125; // 检查是否包含某个元素 public boolean contains(Object o) &#123; return m.containsKey(o); &#125; // 添加一个元素 // 调用map的putIfAbsent()方法 public boolean add(E e) &#123; return m.putIfAbsent(e, Boolean.TRUE) == null; &#125; // 移除一个元素 public boolean remove(Object o) &#123; return m.remove(o, Boolean.TRUE); &#125; // 清空所有元素 public void clear() &#123; m.clear(); &#125; // 迭代器 public Iterator&lt;E&gt; iterator() &#123; return m.navigableKeySet().iterator(); &#125; // 降序迭代器 public Iterator&lt;E&gt; descendingIterator() &#123; return m.descendingKeySet().iterator(); &#125; /* ---------------- AbstractSet Overrides -------------- */ // 比较相等方法 public boolean equals(Object o) &#123; // Override AbstractSet version to avoid calling size() if (o == this) return true; if (!(o instanceof Set)) return false; Collection&lt;?&gt; c = (Collection&lt;?&gt;) o; try &#123; // 这里是通过两次两层for循环来比较 // 这里是有很大优化空间的，参考上篇文章CopyOnWriteArraySet中的彩蛋 return containsAll(c) &amp;&amp; c.containsAll(this); &#125; catch (ClassCastException unused) &#123; return false; &#125; catch (NullPointerException unused) &#123; return false; &#125; &#125; // 移除集合c中所有元素 public boolean removeAll(Collection&lt;?&gt; c) &#123; // Override AbstractSet version to avoid unnecessary call to size() boolean modified = false; for (Object e : c) if (remove(e)) modified = true; return modified; &#125; /* ---------------- Relational operations -------------- */ // 小于e的最大元素 public E lower(E e) &#123; return m.lowerKey(e); &#125; // 小于等于e的最大元素 public E floor(E e) &#123; return m.floorKey(e); &#125; // 大于等于e的最小元素 public E ceiling(E e) &#123; return m.ceilingKey(e); &#125; // 大于e的最小元素 public E higher(E e) &#123; return m.higherKey(e); &#125; // 弹出最小的元素 public E pollFirst() &#123; Map.Entry&lt;E,Object&gt; e = m.pollFirstEntry(); return (e == null) ? null : e.getKey(); &#125; // 弹出最大的元素 public E pollLast() &#123; Map.Entry&lt;E,Object&gt; e = m.pollLastEntry(); return (e == null) ? null : e.getKey(); &#125; /* ---------------- SortedSet operations -------------- */ // 取比较器 public Comparator&lt;? super E&gt; comparator() &#123; return m.comparator(); &#125; // 最小的元素 public E first() &#123; return m.firstKey(); &#125; // 最大的元素 public E last() &#123; return m.lastKey(); &#125; // 取两个元素之间的子set public NavigableSet&lt;E&gt; subSet(E fromElement, boolean fromInclusive, E toElement, boolean toInclusive) &#123; return new ConcurrentSkipListSet&lt;E&gt; (m.subMap(fromElement, fromInclusive, toElement, toInclusive)); &#125; // 取头子set public NavigableSet&lt;E&gt; headSet(E toElement, boolean inclusive) &#123; return new ConcurrentSkipListSet&lt;E&gt;(m.headMap(toElement, inclusive)); &#125; // 取尾子set public NavigableSet&lt;E&gt; tailSet(E fromElement, boolean inclusive) &#123; return new ConcurrentSkipListSet&lt;E&gt;(m.tailMap(fromElement, inclusive)); &#125; // 取子set，包含from，不包含to public NavigableSet&lt;E&gt; subSet(E fromElement, E toElement) &#123; return subSet(fromElement, true, toElement, false); &#125; // 取头子set，不包含to public NavigableSet&lt;E&gt; headSet(E toElement) &#123; return headSet(toElement, false); &#125; // 取尾子set，包含from public NavigableSet&lt;E&gt; tailSet(E fromElement) &#123; return tailSet(fromElement, true); &#125; // 降序set public NavigableSet&lt;E&gt; descendingSet() &#123; return new ConcurrentSkipListSet&lt;E&gt;(m.descendingMap()); &#125; // 可分割的迭代器 @SuppressWarnings(&quot;unchecked&quot;) public Spliterator&lt;E&gt; spliterator() &#123; if (m instanceof ConcurrentSkipListMap) return ((ConcurrentSkipListMap&lt;E,?&gt;)m).keySpliterator(); else return (Spliterator&lt;E&gt;)((ConcurrentSkipListMap.SubMap&lt;E,?&gt;)m).keyIterator(); &#125; // 原子更新map，给clone方法使用 private void setMap(ConcurrentNavigableMap&lt;E,Object&gt; map) &#123; UNSAFE.putObjectVolatile(this, mapOffset, map); &#125; // 原子操作相关内容 private static final sun.misc.Unsafe UNSAFE; private static final long mapOffset; static &#123; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; k = ConcurrentSkipListSet.class; mapOffset = UNSAFE.objectFieldOffset (k.getDeclaredField(&quot;m&quot;)); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125; &#125; 总结： （1）除了HashSet其它Set都是有序的； （2）实现了NavigableSet或者SortedSet接口的都是自然顺序的； （3）使用并发安全的集合实现的Set也是并发安全的； （4）TreeSet虽然不是全部都是使用的TreeMap实现的，但其实都是跟TreeMap相关的（TreeMap的子Map中组合了TreeMap）； （5）ConcurrentSkipListSet虽然不是全部都是使用的ConcurrentSkipListMap实现的，但其实都是跟ConcurrentSkipListMap相关的（ConcurrentSkipListeMap的子Map中组合了ConcurrentSkipListMap）；","categories":[{"name":"Java","slug":"Java","permalink":"http://example.com/categories/Java/"},{"name":"集合","slug":"Java/集合","permalink":"http://example.com/categories/Java/%E9%9B%86%E5%90%88/"}],"tags":[{"name":"集合","slug":"集合","permalink":"http://example.com/tags/%E9%9B%86%E5%90%88/"},{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"author":"John Doe"},{"title":"关于fail-safe","slug":"关于fail-safe","date":"2022-04-01T14:00:00.000Z","updated":"2022-04-02T00:19:14.601Z","comments":true,"path":"2022/04/01/关于fail-safe/","link":"","permalink":"http://example.com/2022/04/01/%E5%85%B3%E4%BA%8Efail-safe/","excerpt":"","text":"Fail-Safe 迭代的出现，是为了解决fail-fast抛出异常处理不方便的情况。fail-safe是针对线程安全的集合类。 采⽤安全失败机制的集合容器，在遍历时不是直接在集合内容上访问的，⽽是先复制原有集合内容，在拷⻉的集合上进⾏遍历。所以，在遍历过程中对原集合所作的修改并不能被迭代器检测到，故不会抛ConcurrentModificationException 异常。 换句话说，并发容器的iterate方法返回的iterator对象，内部都是保存了该集合对象的一个快照副本，并且没有modCount等数值做检查。这也造成了并发容器的iterator读取的数据是某个时间点的快照版本。你可以并发读取，不会抛出异常，但是不保证你遍历读取的值和当前集合对象的状态是一致的！这就是安全失败的含义。 所以Fail-Safe 迭代的缺点是：首先是iterator不能保证返回集合更新后的数据，因为其工作在集合克隆上，而非集合本身。其次，创建集合拷贝需要相应的开销，包括时间和内存。 在java.util.concurrent 包中集合的迭代器，如 ConcurrentHashMap, CopyOnWriteArrayList等默认为都是Fail-Safe。","categories":[{"name":"集合","slug":"集合","permalink":"http://example.com/categories/%E9%9B%86%E5%90%88/"},{"name":"Java","slug":"集合/Java","permalink":"http://example.com/categories/%E9%9B%86%E5%90%88/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"fail-safe","slug":"fail-safe","permalink":"http://example.com/tags/fail-safe/"}],"author":"John Doe"},{"title":"Arrays.asList()避坑","slug":"Arrays-asList-避坑","date":"2022-04-01T13:53:00.000Z","updated":"2022-04-01T13:59:09.226Z","comments":true,"path":"2022/04/01/Arrays-asList-避坑/","link":"","permalink":"http://example.com/2022/04/01/Arrays-asList-%E9%81%BF%E5%9D%91/","excerpt":"","text":"Arrays.asList() 我们可以使⽤它将⼀个数组转换为⼀个List集合。 jdk对这个方法的说明：返回由指定数组⽀持的固定⼤⼩的列表。此⽅法作为基于数组和基于集合的API之间的桥梁，与 Collection.toArray()结合使⽤。返回的List是可序列化并实现RandomAccess接⼝。 《阿⾥巴巴 Java 开发⼿册》对其的描述：Arrays.asList() 将数组转换为集合后,底层其实还是数组。强制使用add/remove/clear等方法会抛出异常。asList返回的对象是一个Arrays内部类，并没有实现集合的修改方法。Arrays.asList（）体现的是适配器模式，只是接口转换，后台的数据仍是数组。 传递的数组必须是对象数组，⽽不是基本类型。Arrays.asList() 是泛型⽅法，传⼊的对象必须是对象数组。 当传⼊⼀个原⽣数据类型数组时， Arrays.asList() 的真正得到的参数就不是数组中的元素，⽽是数组对象本身！此时 List 的唯⼀元素就是这个数组，这也就解释了上⾯的代码。我们使用包装类可以解决该问题，但调用add/remove/clear等方法仍是会报错。 Arrays.asList() ⽅法返回的并不是 java.util.ArrayList ，⽽是 java.util.Arrays 的⼀个内部类,这个内部类并没有实现集合的修改⽅法或者说并没有重写这些⽅法。","categories":[{"name":"Java","slug":"Java","permalink":"http://example.com/categories/Java/"},{"name":"集合","slug":"Java/集合","permalink":"http://example.com/categories/Java/%E9%9B%86%E5%90%88/"}],"tags":[{"name":"集合","slug":"集合","permalink":"http://example.com/tags/%E9%9B%86%E5%90%88/"},{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"author":"John Doe"},{"title":"内存模型之伪共享(False Sharing)","slug":"内存模型之伪共享-False-Sharing","date":"2022-03-23T13:09:00.000Z","updated":"2022-03-23T13:15:05.799Z","comments":true,"path":"2022/03/23/内存模型之伪共享-False-Sharing/","link":"","permalink":"http://example.com/2022/03/23/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E4%B9%8B%E4%BC%AA%E5%85%B1%E4%BA%AB-False-Sharing/","excerpt":"","text":"在对称多处理器(SMP)系统中，每个处理器均有一个本地高速缓存。内存系统必须保证高速缓存的一致性。当不同处理器上的线程修改驻留在同一高速缓存行中的变量时就会发生假共享，结果导致高速缓存行无效，并强制执行更新，进而影响系统性能。 线程0和线程1会用到不同变量，它们在内存中彼此相邻，并驻留在同一高速缓存行。高速缓存行被加载到CPU0和CPU1的高速缓存中（灰色箭头）。尽管这些线程修改的是不同变量（红色和蓝色箭头），高速缓存行仍会无效，并强制内存更新以维持高速缓存的一致性。 缓存系统中是以缓存行（cacheline）为单位存储的。缓存行是2的整数幂个连续字节，一般为32-256个字节。最常见的缓存行大小是64个字节。一个Java的long类型是8字节，因此在一个缓存行中可以存8个long类型的变量。所以，如果你访问一个long数组，当数组中的一个值被加载到缓存中，它会额外加载另外7个，这会带来一些优势。但是也有伪共享问题，比如两个线程，修改long数组的第一个与第七个，会频发发生缓存失效，影响性能。解决办法就是填充，在JDK8中提供了@sun.misc.Contended注解来避免伪共享，即通过padding填充，让数据占据不同的缓存行。","categories":[{"name":"Java","slug":"Java","permalink":"http://example.com/categories/Java/"},{"name":"JMM","slug":"Java/JMM","permalink":"http://example.com/categories/Java/JMM/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"JMM","slug":"JMM","permalink":"http://example.com/tags/JMM/"}],"author":"John Doe"},{"title":"MySQL其他一下查询优化策略","slug":"MySQL其他一下查询优化策略","date":"2022-03-23T04:01:00.000Z","updated":"2022-03-23T04:09:30.098Z","comments":true,"path":"2022/03/23/MySQL其他一下查询优化策略/","link":"","permalink":"http://example.com/2022/03/23/MySQL%E5%85%B6%E4%BB%96%E4%B8%80%E4%B8%8B%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5/","excerpt":"","text":"1、 in 和 exists的区别: 如果子查询得出的结果集记录较少，主查询中的表较大且又有索引时应该用in, 反之如果外层的主查询记录较少，子查询中的表大，又有索引时使用exists。其实我们区分in和exists主要是造成了驱动顺序的改变(这是性能变化的关键)，如果是exists，那么以外层表为驱动表，先被访问，如果是IN，那么先执行子查询，所以我们会以驱动表的快速返回为目标，那么就会考虑到索引及结果集的关系了 ，另外IN时不对NULL进行处理。 in 是把外表和内表作hash 连接，而exists是对外表作loop循环，每次loop循环再对内表进行查询。一直以来认为exists比in效率高的说法是不准确的。 not in 和not exists：如果查询语句使用了not in 那么内外表都进行全表扫描，没有用到索引；而not extsts 的子查询依然能用到表上的索引。所以无论那个表大，用not exists都比not in要快。 2、COUNT(*)与COUNT(具体字段)效率 在表查询中，建议明确字段，不要使用 * 作为查询的字段列表，推荐使用SELECT &lt;字段列表&gt; 查询。原因：① MySQL 在解析的过程中，会通过 查询数据字典 将”*”按序转换成所有列名，这会大大的耗费资源和时间。② 无法使用 覆盖索引 3、 LIMIT 1 对优化的影响 针对的是会扫描全表的 SQL 语句，如果你可以确定结果集只有一条，那么加上 LIMIT 1 的时候，当找到一条结果的时候就不会继续扫描了，这样会加快查询速度。如果数据表已经对字段建立了唯一索引，那么可以通过索引进行查询，不会全表扫描的话，就不需要加上 LIMIT 1 了。 4、多使用COMMIT 只要有可能，在程序中尽量多使用 COMMIT，这样程序的性能得到提高，需求也会因为 COMMIT 所释放的资源而减少。COMMIT 所释放的资源：1、回滚段上用于恢复数据的信息2、被程序语句获得的锁 3、redo / undo log buffer 中的空间 4、管理上述 3 种资源中的内部花费","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"其它","slug":"其它","permalink":"http://example.com/tags/%E5%85%B6%E5%AE%83/"},{"name":"SQL优化","slug":"SQL优化","permalink":"http://example.com/tags/SQL%E4%BC%98%E5%8C%96/"}],"author":"John Doe"},{"title":"change buffer的使用场景","slug":"change-buffer的使用场景","date":"2022-03-23T03:58:00.000Z","updated":"2022-03-23T03:59:55.566Z","comments":true,"path":"2022/03/23/change-buffer的使用场景/","link":"","permalink":"http://example.com/2022/03/23/change-buffer%E7%9A%84%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/","excerpt":"","text":"普通索引和唯一索引应该怎么选择？其实，这两类索引在查询能力上是没差别的，主要考虑的是对 更新性能 的影响。所以，建议你 尽量选择普通索引 。 2. 在实际使用中会发现， 普通索引 和 change buffer 的配合使用，对于 数据量大 的表的更新优化还是很明显的。 如果所有的更新后面，都马上 伴随着对这个记录的查询 ，那么你应该 关闭change buffer 。而在其他情况下，change buffer都能提升更新性能。 由于唯一索引用不changebuffer的优化机制，因此如果 业务可以接受 ，从性能角度出发建议优先考虑非唯一索引。但是如果”业务可能无法确保”的情况下，怎么处理呢？ 首先， 业务正确性优先 。我们的前提是“业务代码已经保证不会写入重复数据”的情况下，讨论性能问题。如果业务不能保证，或者业务就是要求数据库来做约束，那么没得选，必须创建唯一索引。这种情况下，本节的意义在于，如果碰上了大量插入数据慢、内存命中率低的时候，给你多提供一个排查思路。 然后，在一些“ 归档库 ”的场景，你是可以考虑使用唯一索引的。比如，线上数据只需要保留半年，然后历史数据保存在归档库。这时候，归档数据已经是确保没有唯一键冲突了。要提高归档效率，可以考虑把表里面的唯一索引改成普通索引。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"change buffer","slug":"change-buffer","permalink":"http://example.com/tags/change-buffer/"}],"author":"John Doe"},{"title":"索引下推","slug":"索引下推","date":"2022-03-23T03:50:00.000Z","updated":"2022-03-23T03:55:43.730Z","comments":true,"path":"2022/03/23/索引下推/","link":"","permalink":"http://example.com/2022/03/23/%E7%B4%A2%E5%BC%95%E4%B8%8B%E6%8E%A8/","excerpt":"","text":"Index Condition Pushdown(ICP)是MySQL 5.6中新特性，是一种在存储引擎层使用索引过滤数据的一种优化方式。ICP可以减少存储引擎访问基表的次数以及MySQL服务器访问存储引擎的次数。 在不使用ICP索引扫描的过程： storage层：只将满足index key条件的索引记录对应的整行记录取出，返回给server层 server 层：对返回的数据，使用后面的where条件过滤，直至返回最后一行。 使用ICP扫描的过程： storage层：首先将index key条件满足的索引记录区间确定，然后在索引上使用index filter进行过滤。将满足的index filter条件的索引记录才去回表取出整行记录返回server层。不满足index filter条件的索引记录丢弃，不回表、也不会返回server层。 server 层：对返回的数据，使用table filter条件做最后的过滤。 使用前后的成本差别：使用前，存储层多返回了需要被index filter过滤掉的整行记录使用ICP后，直接就去掉了不满足index filter条件的记录，省去了他们回表和传递到server层的成本。ICP的 加速效果 取决于在存储引擎内通过 ICP筛选 掉的数据的比例。 ICP的使用条件： ① 只能用于二级索引(secondary index) ②explain显示的执行计划中type值（join 类型）为 range 、 ref 、 eq_ref 或者 ref_or_null 。 ③ 并非全部where条件都可以用ICP筛选，如果where条件的字段不在索引列中，还是要读取整表的记录到server端做where过滤。 ④ ICP可以用于MyISAM和InnnoDB存储引擎 ⑤ MySQL 5.6版本的不支持分区表的ICP功能，5.7版本的开始支持。 ⑥ 当SQL使用覆盖索引时，不支持ICP优化方法。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"索引下推","slug":"索引下推","permalink":"http://example.com/tags/%E7%B4%A2%E5%BC%95%E4%B8%8B%E6%8E%A8/"}],"author":"John Doe"},{"title":"优化分页查询","slug":"优化分页查询","date":"2022-03-23T02:56:00.000Z","updated":"2022-03-23T02:58:13.556Z","comments":true,"path":"2022/03/23/优化分页查询/","link":"","permalink":"http://example.com/2022/03/23/%E4%BC%98%E5%8C%96%E5%88%86%E9%A1%B5%E6%9F%A5%E8%AF%A2/","excerpt":"","text":"在索引上完成排序分页操作，最后根据主键关联回原表查询所需要的其他列内容。 该方案适用于主键自增的表，可以把Limit 查询转换成某个位置的查询 。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"分页查询","slug":"分页查询","permalink":"http://example.com/tags/%E5%88%86%E9%A1%B5%E6%9F%A5%E8%AF%A2/"}],"author":"John Doe"},{"title":"GROUP BY优化","slug":"GROUP-BY优化","date":"2022-03-23T02:55:00.000Z","updated":"2022-03-23T02:56:43.112Z","comments":true,"path":"2022/03/23/GROUP-BY优化/","link":"","permalink":"http://example.com/2022/03/23/GROUP-BY%E4%BC%98%E5%8C%96/","excerpt":"","text":"group by 使用索引的原则几乎跟order by一致 ，group by 即使没有过滤条件用到索引，也可以直接使用索引。 group by 先排序再分组，遵照索引建的最佳左前缀法则 当无法使用索引列，增大 max_length_for_sort_data 和 sort_buffer_size 参数的设置 where效率高于having，能写在where限定的条件就不要写在having中了 减少使用order by，和业务沟通能不排序就不排序，或将排序放到程序端去做。Order by、group by、distinct这些语句较为耗费CPU，数据库的CPU资源是极其宝贵的。 包含了order by、group by、distinct这些查询的语句，where条件过滤出来的结果集请保持在1000行以内，否则SQL会很慢。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"GROUP BY优化","slug":"GROUP-BY优化","permalink":"http://example.com/tags/GROUP-BY%E4%BC%98%E5%8C%96/"},{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"}],"author":"John Doe"},{"title":"排序优化","slug":"排序优化","date":"2022-03-23T02:47:00.000Z","updated":"2022-03-23T02:54:53.268Z","comments":true,"path":"2022/03/23/排序优化/","link":"","permalink":"http://example.com/2022/03/23/%E6%8E%92%E5%BA%8F%E4%BC%98%E5%8C%96/","excerpt":"","text":"在 WHERE 条件字段上加索引，但是为什么在 ORDER BY 字段上还要加索引呢？ 优化建议： SQL 中，可以在 WHERE 子句和 ORDER BY 子句中使用索引，目的是在 WHERE 子句中 避免全表扫 描 ，在 ORDER BY 子句 避免使用 FileSort 排序 。当然，某些情况下全表扫描，或者 FileSort 排序不一定比索引慢。但总的来说，我们还是要避免，以提高查询效率。 尽量使用 Index 完成 ORDER BY 排序。如果 WHERE 和 ORDER BY 后面是相同的列就使用单索引列；如果不同就使用联合索引。 无法使用 Index 时，需要对 FileSort 方式进行调优。 两个索引同时存在，mysql自动选择最优的方案。（对于这个例子，mysql选idx_age_stuno_name）。但是， 随着数据量的变化，选择的索引也会随之变化的 。 当【范围条件】和【group by 或者 order by】的字段出现二选一时，优先观察条件字段的过滤数量，如果过滤的数据足够多，而需要排序的数据并不多时，优先把索引放在范围字段上。反之，亦然。 filesort：双路排序和单路排序 双路排序 （慢）MySQL 4.1之前是使用双路排序 ，字面意思就是两次扫描磁盘，最终得到数据， 读取行指针和order by列 ，对他们进行排序，然后扫描已经排序好的列表，按照列表中的值重新从列表中读取对应的数据输出从磁盘取排序字段，在buffer进行排序，再从 磁盘取其他字段 。取一批数据，要对磁盘进行两次扫描，众所周知，IO是很耗时的，所以在mysql4.1之后，出现了第二种改进的算法，就是单路排序。 单路排序 （快）从磁盘读取查询需要的 所有列 ，按照order by列在buffer对它们进行排序，然后扫描排序后的列表进行输出， 它的效率更快一些，避免了第二次读取数据。并且把随机IO变成了顺序IO，但是它会使用更多的空间， 因为它把每一行都保存在内存中了。 优化策略 尝试提高 sort_buffer_size 2. 尝试提高 max_length_for_sort_data Order by 时select * 是一个大忌。最好只Query需要的字段。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"排序优化","slug":"排序优化","permalink":"http://example.com/tags/%E6%8E%92%E5%BA%8F%E4%BC%98%E5%8C%96/"}],"author":"John Doe"},{"title":"子查询优化","slug":"子查询优化","date":"2022-03-23T02:43:00.000Z","updated":"2022-03-23T02:47:07.393Z","comments":true,"path":"2022/03/23/子查询优化/","link":"","permalink":"http://example.com/2022/03/23/%E5%AD%90%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96/","excerpt":"","text":"使用子查询可以进行SELECT语句的嵌套查询，即一个SELECT查询的结果作为另一个SELECT语句的条件。 子查询可以一次性完成很多逻辑上需要多个步骤才能完成的SQL操作。 但值得注意的是：子查询虽然可以帮助我们通过一个 SQL 语句实现比较复杂的查询。但是，子查询的执行效率不高。 原因：① 执行子查询时，MySQL需要为内层查询语句的查询结果 建立一个临时表 ，然后外层查询语句从临时表中查询记录。查询完毕后，再撤销这些临时表 。这样会消耗过多的CPU和IO资源，产生大量的慢查询。② 子查询的结果集存储的临时表，不论是内存临时表还是磁盘临时表都 不会存在索引 ，所以查询性能会受到一定的影响。③ 对于返回结果集比较大的子查询，其对查询性能的影响也就越大。 在MySQL中，可以使用连接（JOIN）查询来替代子查询。连接查询 不需要建立临时表 ，其速度比子查询要快 ，如果查询中使用索引的话，性能会更好。 因此：尽量不要使用NOT IN 或者 NOT EXISTS，用LEFT JOIN xxx ON xx WHERE xx IS NULL替代","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"子查询优化","slug":"子查询优化","permalink":"http://example.com/tags/%E5%AD%90%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96/"}],"author":"John Doe"},{"title":"关联查询优化","slug":"关联查询优化","date":"2022-03-23T02:24:00.000Z","updated":"2022-03-23T02:41:22.043Z","comments":true,"path":"2022/03/23/关联查询优化/","link":"","permalink":"http://example.com/2022/03/23/%E5%85%B3%E8%81%94%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96/","excerpt":"","text":"采用左外连接 被驱动的表简历索引，可以避免全表扫描（LEFT JOIN条件用于确定如何从右表搜索行，左边一定都有，所以 右边是我们的关键点,一定需要建立索引 。） 采用内连接 MySQL自动选择驱动表（小结果集），保证被驱动的表的JOIN字段已经建立了索引。 小结： 保证被驱动表的JOIN字段已经创建了索引 需要JOIN 的字段，数据类型保持绝对一致。 LEFT JOIN 时，选择小表作为驱动表， 大表作为被驱动表 。减少外层循环的次数。 INNER JOIN 时，MySQL会自动将 小结果集的表选为驱动表 。选择相信MySQL优化策略。 能够直接多表关联的尽量直接关联，不用子查询。(减少查询的趟数) 不建议使用子查询，建议将子查询SQL拆开结合程序多次查询，或使用 JOIN 来代替子查询。 衍生表建不了索引 补充： 什么叫作“小表”？在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与join的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表。 在这个流程里： 对驱动表t1做了全表扫描，这个过程需要扫描100行； 而对于每一行R，根据a字段去表t2查找，走的是树搜索过程。由于我们构造的数据都是一一对应的，因此每次的搜索过程都只扫描一行，也是总共扫描100行； 所以，整个执行流程，总扫描行数是200。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"关联查询","slug":"关联查询","permalink":"http://example.com/tags/%E5%85%B3%E8%81%94%E6%9F%A5%E8%AF%A2/"}],"author":"John Doe"},{"title":"索引失效案例","slug":"索引失效案例","date":"2022-03-23T02:15:00.000Z","updated":"2022-03-23T02:22:42.930Z","comments":true,"path":"2022/03/23/索引失效案例/","link":"","permalink":"http://example.com/2022/03/23/%E7%B4%A2%E5%BC%95%E5%A4%B1%E6%95%88%E6%A1%88%E4%BE%8B/","excerpt":"","text":"没有遵守最左前缀法则（联合索引中，左边的值未确认，无法使用此索引） SQL语句中使用计算、函数、类型转换等 SQL语句中索引条件在范围查询右边 使用！=或者&lt;&gt;也会令索引失效 is null 可以使用索引，而is not null无法使用索引 like以%开头，索引会失效（页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。） OR 前后存在非索引的列，索引失效 数据库和表的字符集统一使用utf8mb4（统一使用utf8mb4( 5.5.3版本以上支持)兼容性更好，统一字符集可以避免由于字符集转换产生的乱码。不同的 字符集 进行比较前需要进行 转换 会造成索引失效。）","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"索引失效","slug":"索引失效","permalink":"http://example.com/tags/%E7%B4%A2%E5%BC%95%E5%A4%B1%E6%95%88/"}],"author":"John Doe"},{"title":"哪些情况下不适合建立索引？","slug":"哪些情况下不适合建立索引？","date":"2022-03-23T01:29:00.000Z","updated":"2022-03-23T01:30:24.048Z","comments":true,"path":"2022/03/23/哪些情况下不适合建立索引？/","link":"","permalink":"http://example.com/2022/03/23/%E5%93%AA%E4%BA%9B%E6%83%85%E5%86%B5%E4%B8%8B%E4%B8%8D%E9%80%82%E5%90%88%E5%BB%BA%E7%AB%8B%E7%B4%A2%E5%BC%95%EF%BC%9F/","excerpt":"","text":"在where中使用不到的字段，不要设置索引 数据量小的表最好不要使用索引 有大量重复数据的列上不要建立索引 避免对经常更新的表创建过多的索引 不建议用无序的值作为索引 删除不再使用或者很少使用的索引 不要定义冗余或重复的索引","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://example.com/tags/%E7%B4%A2%E5%BC%95/"},{"name":"设计原则","slug":"设计原则","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/"}],"author":"John Doe"},{"title":"哪些情况下适合建立索引？","slug":"哪些情况下适合建立索引？","date":"2022-03-23T01:18:00.000Z","updated":"2022-03-23T01:28:05.806Z","comments":true,"path":"2022/03/23/哪些情况下适合建立索引？/","link":"","permalink":"http://example.com/2022/03/23/%E5%93%AA%E4%BA%9B%E6%83%85%E5%86%B5%E4%B8%8B%E9%80%82%E5%90%88%E5%BB%BA%E7%AB%8B%E7%B4%A2%E5%BC%95%EF%BC%9F/","excerpt":"","text":"字段的数值有唯一性约束（业务上具有唯一特性的字段，即使是组合字段，也必须建成唯一索引。（来源：Alibaba）） 频繁作为where查询条件的字段（某个字段在SELECT语句的 WHERE 条件中经常被使用到，那么就需要给这个字段创建索引了。尤其是在数据量大的情况下，创建普通索引就可以大幅提升数据查询的效率。） 经常 GROUP BY 和 ORDER BY 的列（索引就是让数据按照某种顺序进行存储或检索，因此当我们使用 GROUP BY 对数据进行分组查询，或者使用 ORDER BY 对数据进行排序的时候，就需要 对分组或者排序的字段进行索引 。如果待排序的列有多个，那么可以在这些列上建立 组合索引 。） UPDATE、DELETE 的 WHERE 条件列（对数据按照某个条件进行查询后再进行 UPDATE 或 DELETE 的操作，如果对 WHERE 字段创建了索引，就能大幅提升效率。原理是因为我们需要先根据 WHERE 条件列检索出来这条记录，然后再对它进行更新或删除。如果进行更新的时候，更新的字段是非索引字段，提升的效率会更明显，这是因为非索引字段更新不需要对索引进行维护。） DISTINCT 字段需要创建索引（有时候我们需要对某个字段进行去重，使用 DISTINCT，那么对这个字段创建索引，也会提升查询效率。） 多表 JOIN 连接操作时，创建索引注意事项（首先， 连接表的数量尽量不要超过 3 张 ，因为每增加一张表就相当于增加了一次嵌套的循环，数量级增长会非常快，严重影响查询的效率。其次， 对 WHERE 条件创建索引 ，因为 WHERE 才是对数据条件的过滤。如果在数据量非常大的情况下，没有 WHERE 条件过滤是非常可怕的。最后， 对用于连接的字段创建索引 ，并且该字段在多张表中的 类型必须一致 。比如 course_id 在 student_info 表和 course 表中都为 int(11) 类型，而不能一个为 int 另一个为 varchar 类型。） 使用列的类型小的创建索引 使用字符串前缀创建索引（例如创建一张商户表，因为地址字段比较长，可以在地址字段上建立前缀索引） 注意： 【 强制 】在 varchar 字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度。 说明：索引的长度与区分度是一对矛盾体，一般对字符串类型数据，长度为 20 的索引，区分度会 高达 90% 以上 ，可以使用 count(distinct left(列名, 索引长度))/count(*)的区分度来确定。 区分度高(散列性高)的列适合作为索引 使用最频繁的列放到联合索引的左侧 在多个字段都要创建索引的情况下，联合索引优于单值索引 限制索引的数目","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"索引","slug":"索引","permalink":"http://example.com/tags/%E7%B4%A2%E5%BC%95/"},{"name":"原则","slug":"原则","permalink":"http://example.com/tags/%E5%8E%9F%E5%88%99/"}],"author":"John Doe"},{"title":"MySQL8.0索引新特性","slug":"MySQL8-0索引新特性","date":"2022-03-23T01:12:00.000Z","updated":"2022-03-23T01:18:15.471Z","comments":true,"path":"2022/03/23/MySQL8-0索引新特性/","link":"","permalink":"http://example.com/2022/03/23/MySQL8-0%E7%B4%A2%E5%BC%95%E6%96%B0%E7%89%B9%E6%80%A7/","excerpt":"","text":"支持降序索引分别在MySQL 5.7版本和MySQL 8.0版本中创建数据表ts1，结果如下： CREATE TABLE ts1(a int,b int,index idx_a_b(a,b desc)); 5.7 8.0 降序索引在特性降序查询效率比升序自然要好，具体根据情况而进行设定。 隐藏索引 在MySQL 5.7版本及之前，只能通过显式的方式删除索引。此时，如果发现删除索引后出现错误，又只能通过显式创建索引的方式将删除的索引创建回来。如果数据表中的数据量非常大，或者数据表本身比较大，这种操作就会消耗系统过多的资源，操作成本非常高。 从MySQL 8.x开始支持 隐藏索引（invisible indexes） ，只需要将待删除的索引设置为隐藏索引，使查询优化器不再使用这个索引（即使使用force index（强制使用索引），优化器也不会使用该索引），确认将索引设置为隐藏索引后系统不受任何响应，就可以彻底删除索引。 这种通过先将索引设置为隐藏索引，再删除索引的方式就是软删除 。 注意：注意 当索引被隐藏时，它的内容仍然是和正常索引一样实时更新的。如果一个索引需要长期被隐藏，那么可以将其删除，因为索引的存在会影响插入、更新和删除的性能。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"新特性","slug":"新特性","permalink":"http://example.com/tags/%E6%96%B0%E7%89%B9%E6%80%A7/"},{"name":"MySQL8.0","slug":"MySQL8-0","permalink":"http://example.com/tags/MySQL8-0/"}],"author":"John Doe"},{"title":"MySQL中大多数情况查询缓存就是个鸡肋，为什么呢？","slug":"MySQL中大多数情况查询缓存就是个鸡肋，为什么呢？","date":"2022-03-22T12:34:00.000Z","updated":"2022-03-22T12:37:08.225Z","comments":true,"path":"2022/03/22/MySQL中大多数情况查询缓存就是个鸡肋，为什么呢？/","link":"","permalink":"http://example.com/2022/03/22/MySQL%E4%B8%AD%E5%A4%A7%E5%A4%9A%E6%95%B0%E6%83%85%E5%86%B5%E6%9F%A5%E8%AF%A2%E7%BC%93%E5%AD%98%E5%B0%B1%E6%98%AF%E4%B8%AA%E9%B8%A1%E8%82%8B%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E5%91%A2%EF%BC%9F/","excerpt":"","text":"查询缓存是提前把查询结果缓存起来，这样下次不需要执行就可以直接拿到结果。需要说明的是，在MySQL 中的查询缓存，不是缓存查询计划，而是查询对应的结果。这就意味着查询匹配的鲁棒性大大降低 ，只有相同的查询操作才会命中查询缓存。两个查询请求在任何字符上的不同（例如：空格、注释、大小写），都会导致缓存不会命中。因此 MySQL的查询缓存命中率不高 。同时，如果查询请求中包含某些系统函数、用户自定义变量和函数、一些系统表，如 mysql 、 information_schema、 performance_schema 数据库中的表，那这个请求就不会被缓存。以某些系统函数举例，可能同样的函数的两次调用会产生不一样的结果，比如函数NOW ，每次调用都会产生最新的当前时间，如果在一个查询请求中调用了这个函数，那即使查询请求的文本信息都一样，那不同时间的两次查询也应该得到不同的结果，如果在第一次查询时就缓存了，那第二次查询的时候直接使用第一次查询的结果就是错误的！ 此外，既然是缓存，那就有它缓存失效的时候 。MySQL的缓存系统会监测涉及到的每张表，只要该表的结构或者数据被修改，如对该表使用了INSERT 、 UPDATE 、 DELETE 、 TRUNCATE TABLE 、 ALTER TABLE 、 DROP TABLE 或 DROP DATABASE 语句，那使用该表的所有高速缓存查询都将变为无效并从高速缓存中删除！对于更新压力大的数据库 来说，查询缓存的命中率会非常低。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"缓存","slug":"缓存","permalink":"http://example.com/tags/%E7%BC%93%E5%AD%98/"}],"author":"John Doe"},{"title":"MySQL两阶段提交","slug":"MySQL两阶段提交","date":"2022-03-21T08:12:00.000Z","updated":"2022-03-21T08:19:53.324Z","comments":true,"path":"2022/03/21/MySQL两阶段提交/","link":"","permalink":"http://example.com/2022/03/21/MySQL%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4/","excerpt":"","text":"MySQL中经常说的WAL技术，WAL的全称是Write Ahead Logging，它的关键点就是先写日志，再写磁盘。即当有一条记录需要更新时，InnoDB引擎就会先把记录写到redo log里，并更新内存，这个时候更新就完成了。因为如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程IO成本、查找成本都很高。 在执行一条update语句时候，通过连接器、分析器、优化器之后，调用操作引擎，将新行写入内存，写入redo log，状态为prepare-&gt;写binlog-&gt;redo log状态修改为commit。写入redo的过程分为了prepare和commit称为二阶段提交。 采用二阶段提交的原因： 先写redolog再写binlog：如果在一条语句redolog之后崩溃了，binlog则没有记录这条语句。系统在crash recovery时重新执行了一遍binlog便会少了这一次的修改。恢复的数据库少了这条更新。 先写binlog再写redolog：如果在一条语句binlog之后崩溃了，redolog则没有记录这条语句（数据库物理层面并没有执行这条语句）。系统在crash recovery时重新执行了一遍binlog便会多了这一次的修改。恢复的数据库便多了这条更新。 Crash recovery在做Crash recovery时，分为以下3种情况： binlog有记录，redolog状态commit：正常完成的事务，不需要恢复； binlog有记录，redolog状态prepare：在binlog写完提交事务之前的crash，恢复操作：提交事务。（因为之前没有提交） binlog无记录，redolog状态prepare：在binlog写完之前的crash，恢复操作：回滚事务（因为crash时并没有成功写入数据库）","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"两阶段提交","slug":"两阶段提交","permalink":"http://example.com/tags/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4/"},{"name":"Binlog","slug":"Binlog","permalink":"http://example.com/tags/Binlog/"}],"author":"John Doe"},{"title":"如何判断一个数据库是否出现了问题？","slug":"如何判断一个数据库是否出现了问题？","date":"2022-03-21T07:30:00.000Z","updated":"2022-03-21T07:50:00.480Z","comments":true,"path":"2022/03/21/如何判断一个数据库是否出现了问题？/","link":"","permalink":"http://example.com/2022/03/21/%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E4%B8%80%E4%B8%AA%E6%95%B0%E6%8D%AE%E5%BA%93%E6%98%AF%E5%90%A6%E5%87%BA%E7%8E%B0%E4%BA%86%E9%97%AE%E9%A2%98%EF%BC%9F/","excerpt":"","text":"在一主一备的双 M 架构里，主备切换只需要把客户端流量切到备库；而在一主多从架构里，主备切换除了要把客户端流量切到备库外，还需要把从库接到新主库上。 主备切换有两种场景，一种是主动切换，一种是被动切换。而其中被动切换，往往是因为主库出问题了，由 HA 系统发起的。 而怎么判断主库出现了问题则是一个重点。 select 1判断select 1 成功返回，只能说明这个库的进程还在，并不能说明主库没问题。 我们设置 innodb_thread_concurrency 参数为3控制InnoDB 的并发线程上限。也就是说，一旦并发线程数达到这个值，InnoDB 在接收到新请求的时候，就会进入等待状态，直到有线程退出。 此时执行三个select sleep(100) from t，然后执行 select 1会返回成功，但执行select * from t会阻塞。这select 1 成功返回，只能说明这个库的进程还在，并不能说明主库没问题。 在 InnoDB 中，innodb_thread_concurrency 这个参数的默认值是 0，表示不限制并发线程数量。但是，不限制并发线程数肯定是不行的。因为，一个机器的 CPU 核数有限，线程全冲进来，上下文切换的成本就会太高。 所以，通常情况下，我们建议把 innodb_thread_concurrency 设置为 64~128 之间的值。这时，你一定会有疑问，并发线程上限数设置为 128 够干啥，线上的并发连接数动不动就上千了。 并发连接和并发查询，并不是同一个概念。你在 show processlist 的结果里，看到的几千个连接，指的就是并发连接。而“当前正在执行”的语句，才是我们所说的并发查询。 并发连接数达到几千个影响并不大，就是多占一些内存而已。我们应该关注的是并发查询，因为并发查询太高才是 CPU 杀手。这也是为什么我们需要设置innodb_thread_concurrency 参数的原因。 查表判断为了能够检测 InnoDB 并发线程数过多导致的系统不可用情况，我们需要找一个访问InnoDB 的场景。一般的做法是，在系统库（mysql 库）里创建一个表，比如命名为health_check，里面只放一行数据，然后定期执行。 使用这个方法，我们可以检测出由于并发线程过多导致的数据库不可用的情况。 但是，我们马上还会碰到下一个问题，即：空间满了以后，这种方法又会变得不好使。 我们知道，更新事务要写 binlog，而一旦 binlog 所在磁盘的空间占用率达到 100%，那么所有的更新语句和事务提交的 commit 语句就都会被堵住。但是，系统这时候还是可以正常读数据的。 更新判断既然要更新，就要放个有意义的字段，常见做法是放一个 timestamp 字段，用来表示最后一次执行检测的时间。 节点可用性的检测都应该包含主库和备库。如果用更新来检测主库的话，那么备库也要进行更新检测。 但，备库的检测也是要写 binlog 的。由于我们一般会把数据库 A 和 B 的主备关系设计为双 M 结构，所以在备库 B 上执行的检测命令，也要发回给主库 A。 但是，如果主库 A 和备库 B 都用相同的更新命令，就可能出现行冲突，也就是可能会导致主备同步停止。所以，现在看来 mysql.health_check 这个表就不能只有一行数据了。 为了让主备之间的更新不产生冲突，我们可以在 mysql.health_check 表上存入多行数据，并用 A、B 的 server_id 做主键。 由于 MySQL 规定了主库和备库的 server_id 必须不同（否则创建主备关系的时候就会报错），这样就可以保证主、备库各自的检测命令不会发生冲突。 更新判断是一个相对比较常用的方案了，不过依然存在一些问题。其中，“判定慢”一直是让 DBA 头疼的问题。 其实，这里涉及到的是服务器 IO 资源分配的问题。 首先，所有的检测逻辑都需要一个超时时间 N。执行一条 update 语句，超过 N 秒后还不返回，就认为系统不可用。 你可以设想一个日志盘的 IO 利用率已经是 100% 的场景。这时候，整个系统响应非常慢，已经需要做主备切换了。 但是你要知道，IO 利用率 100% 表示系统的 IO 是在工作的，每个请求都有机会获得 IO资源，执行自己的任务。而我们的检测使用的 update 命令，需要的资源很少，所以可能在 拿到 IO 资源的时候就可以提交成功，并且在超时时间 N 秒未到达之前就返回给了检测系统。 检测系统一看，update 命令没有超时，于是就得到了“系统正常”的结论。 也就是说，这时候在业务系统上正常的 SQL 语句已经执行得很慢了，但是 DBA 上去一看，HA 系统还在正常工作，并且认为主库现在处于可用状态。 之所以会出现这个现象，根本原因是我们上面说的所有方法，都是基于外部检测的。外部检测天然有一个问题，就是随机性。 因为，外部检测都需要定时轮询，所以系统可能已经出问题了，但是却需要等到下一个检测发起执行语句的时候，我们才有可能发现问题。而且，如果你的运气不够好的话，可能第一次轮询还不能发现，这就会导致切换慢的问题。 内部统计针对磁盘利用率这个问题，如果 MySQL 可以告诉我们，内部每一次 IO 请求的时间，那我们判断数据库是否出问题的方法就可靠得多了。 其实，MySQL 5.6 版本以后提供的 performance_schema 库，就在file_summary_by_event_name 表里统计了每次 IO 请求的时间。 因为我们每一次操作数据库，performance_schema 都需要额外地统计这些信息，所以我们打开这个统计功能是有性能损耗的。 假设，现在你已经开启了 redo log 和 binlog 这两个统计信息，那要怎么把这个信息用在实例状态诊断上呢？ 很简单，你可以通过 MAX_TIMER 的值来判断数据库是否出问题了。比如，你可以设定阈值，单次 IO 请求时间超过 200 毫秒属于异常，然后使用类似下面这条语句作为检测逻辑。 mysql&gt; select event_name,MAX_TIMER_WAIT FROM performance_schema.file_summary_by_event_n 发现异常后，取到你需要的信息，再通过下面这条语句： mysql&gt; truncate table performance_schema.file_summary_by_event_name; 把之前的统计信息清空。这样如果后面的监控中，再次出现这个异常，就可以加入监控累积值了。 转载：https://www.jianshu.com/p/a95064c25e45","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"},{"name":"主从复制","slug":"MySQL/主从复制","permalink":"http://example.com/categories/MySQL/%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"主从复制","slug":"主从复制","permalink":"http://example.com/tags/%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/"},{"name":"故障","slug":"故障","permalink":"http://example.com/tags/%E6%95%85%E9%9A%9C/"}],"author":"John Doe"},{"title":"Java中的Optional类","slug":"Java中的Optional类","date":"2022-03-20T01:17:00.000Z","updated":"2022-03-20T01:25:45.463Z","comments":true,"path":"2022/03/20/Java中的Optional类/","link":"","permalink":"http://example.com/2022/03/20/Java%E4%B8%AD%E7%9A%84Optional%E7%B1%BB/","excerpt":"","text":"Optional 类是一个可以为null的容器对象。如果值存在则isPresent()方法会返回true，调用get()方法会返回该对象。 Optional 是个容器：它可以保存类型T的值，或者仅仅保存null。Optional提供很多有用的方法，这样我们就不用显式进行空值检测。 Optional 类的引入很好的解决空指针异常。","categories":[{"name":"Java","slug":"Java","permalink":"http://example.com/categories/Java/"}],"tags":[{"name":"Optional","slug":"Optional","permalink":"http://example.com/tags/Optional/"},{"name":"空指针","slug":"空指针","permalink":"http://example.com/tags/%E7%A9%BA%E6%8C%87%E9%92%88/"}],"author":"John Doe"},{"title":"G1中的String去重操作","slug":"G1中的String去重操作","date":"2022-03-19T12:04:00.000Z","updated":"2022-03-19T12:12:59.659Z","comments":true,"path":"2022/03/19/G1中的String去重操作/","link":"","permalink":"http://example.com/2022/03/19/G1%E4%B8%AD%E7%9A%84String%E5%8E%BB%E9%87%8D%E6%93%8D%E4%BD%9C/","excerpt":"","text":"堆中存活数据String占了很大一部分，而里面很多可能都是重复的字符串对象。在G1垃圾回收器中，会实现自动持续对重复的string对象进行去重，避免内存浪费。 实现： 当垃圾回收器工作时，会访问堆上存活的对象。对每一个对象的访问都会检查是否是候选的要去重的string对象 如果是，把这个对象的一个引用插入到队列中等待后续处理。一个去重的线程在后台运行，处理这个队列。处理一个元素意味着从队列删除这个元素，然后尝试去重它引用的string对象。 使用一个hashtable来记录所有被string对象使用的不重复的char数组，当去重时，会查这个hashtable，来看是否存在一个一样的char数组。 如果存在，string对象会被调整引用那个对象，释放对原数组的引用，原数组被垃圾回收。如果查找失败，则放入hashtable，就可以用于共享。 开启去重，默认未开启usestringDeduplication(bool)","categories":[{"name":"String","slug":"String","permalink":"http://example.com/categories/String/"}],"tags":[{"name":"String","slug":"String","permalink":"http://example.com/tags/String/"},{"name":"G1","slug":"G1","permalink":"http://example.com/tags/G1/"}],"author":"John Doe"},{"title":"关于String的intern方法","slug":"关于String的intern方法","date":"2022-03-19T11:44:00.000Z","updated":"2022-03-19T12:04:44.977Z","comments":true,"path":"2022/03/19/关于String的intern方法/","link":"","permalink":"http://example.com/2022/03/19/%E5%85%B3%E4%BA%8EString%E7%9A%84intern%E6%96%B9%E6%B3%95/","excerpt":"","text":"首先需要说明的是 常量与常量拼接结果是放在常量池（编译期优化） 常量池不会存放相同字符串（hashtable） 只要其中一个是变量，拼接的时候，结果就是在堆（使用了stringbuiler） 拼接结果调用intern，则主动将常量池还没有的字符串对象放入，并返回对象地址。 intern是一个native方法，调用的是底层c的方法。 jdk1.6，会将这个对象尝试放入串池，如果串池有，则直接返回串池中该对象的地址。如果没有，则将该对象复杂一份，放入串池，并返回串池对象的地址 jdk1.7起，变动在于，当串池没有该对象，会将对象的引用复制一份放入串池，返回串池的引用地址","categories":[{"name":"String","slug":"String","permalink":"http://example.com/categories/String/"}],"tags":[{"name":"String","slug":"String","permalink":"http://example.com/tags/String/"},{"name":"intern","slug":"intern","permalink":"http://example.com/tags/intern/"}],"author":"John Doe"},{"title":"为什么JDK9时String从char换为了byte？","slug":"为什么JDK9时String从char换为了byte？","date":"2022-03-19T11:32:00.000Z","updated":"2022-03-19T11:44:40.964Z","comments":true,"path":"2022/03/19/为什么JDK9时String从char换为了byte？/","link":"","permalink":"http://example.com/2022/03/19/%E4%B8%BA%E4%BB%80%E4%B9%88JDK9%E6%97%B6String%E4%BB%8Echar%E6%8D%A2%E4%B8%BA%E4%BA%86byte%EF%BC%9F/","excerpt":"","text":"jdk1.8及以前String的底层是用char数组构成，但在1.9变为了byte数组，为什么呢？ 首先我们知道char字符占两个字节（16位），其次字符串是堆使用的重要部分，而且大多数字符串对象只包含拉丁字符（这些字符只需一个字节的存储空间），因此对于这些字符串对象的内部char数组可能会有半数以上的空间未使用，造成空间浪费。 因此将char转化为byte来应对这种情况。新的String类将根据字符串的内存存储编码为ISO或UTF的字符。 注意：同String一样的Stringbuffer和Stringbuilder也同样做了修改","categories":[{"name":"String","slug":"String","permalink":"http://example.com/categories/String/"},{"name":"字符串","slug":"String/字符串","permalink":"http://example.com/categories/String/%E5%AD%97%E7%AC%A6%E4%B8%B2/"}],"tags":[{"name":"String","slug":"String","permalink":"http://example.com/tags/String/"},{"name":"新特性","slug":"新特性","permalink":"http://example.com/tags/%E6%96%B0%E7%89%B9%E6%80%A7/"}],"author":"John Doe"},{"title":"HotSpot中JIT的分类","slug":"HotSpot中JIT的分类","date":"2022-03-19T11:11:00.000Z","updated":"2022-03-19T11:23:39.386Z","comments":true,"path":"2022/03/19/HotSpot中JIT的分类/","link":"","permalink":"http://example.com/2022/03/19/HotSpot%E4%B8%ADJIT%E7%9A%84%E5%88%86%E7%B1%BB/","excerpt":"","text":"JIT的编译器分了两种：C1和C2，在HotSpot下对应Client和Server两类。（-client和-servcer指定） C1和C2不同的优化策略C1：方法内联、去虚拟化、冗余消除 方法内联：将引用的函数代码编译到引用点，尖山栈帧的生成，减少参数传递和跳转 去虚拟化：对唯一实现进行内联 冗余消除： 在运行期间把一些不会执行的代码折叠掉 C2：逃逸分析 标量替换：用标量值代替聚合对象的属性值 栈上分配：用于对未逃逸的对象分配对象在栈上，而不是堆 同步消除： 清楚同步操作，通常是指synchronized 总结： JIT编译出来的机器码比解释器执行效率高，但启动速度要慢一点 C2比C1启动慢，但稳定下来后，C2速度远快于C1 补充：AOT编译器（静态提前编译器），它可以将java类文件直接转化为机器码。 好处：java虚拟机加载已经预编译好的二进制库，可以直接执行，不必等待及时编译器的预热 缺点：破坏了java一次编译到处运行。降低了java链接过程的动态性。还需要继续优化。","categories":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/categories/JVM/"}],"tags":[{"name":"JIT","slug":"JIT","permalink":"http://example.com/tags/JIT/"},{"name":"C1","slug":"C1","permalink":"http://example.com/tags/C1/"},{"name":"C2","slug":"C2","permalink":"http://example.com/tags/C2/"}],"author":"John Doe"},{"title":"热点探测技术","slug":"热点探测技术","date":"2022-03-19T10:59:00.000Z","updated":"2022-03-19T11:11:09.698Z","comments":true,"path":"2022/03/19/热点探测技术/","link":"","permalink":"http://example.com/2022/03/19/%E7%83%AD%E7%82%B9%E6%8E%A2%E6%B5%8B%E6%8A%80%E6%9C%AF/","excerpt":"","text":"一个方法被调用多次，或者是一个方法内部多虚循环执行都可以被称作热点代码，因此都可以通过JIT编译器编译为本地机器指令。其过程发生在方法的执行过程中，因此被称为栈上替换，或简称OSR。其主要实现通过热点探测功能。HostSpot采用的热点探测方法是基于计数器的热点探测。 计数器的热点探测：HotSpot会为每一个方法建立2个不同类型的计数器，分别为方法调用计数器和回边计数器。 方法调用计数器统计方法调用次数，默认在Client下是1500，Server下是10000，超过则会触发即时编译。（-XX：complieThreshold）。如果不做任何设置，方法调用计数器统计的并不是方法被调用的绝对次数，而是一个相对的执行频率，即一段时间内方法的调用次数。当超过一定时间调用次数不足让他提交给即使编译器编译，那么这个方法调用的计数器就减少一般（热点衰减：其发生在垃圾回收时顺便进行的） 回边计数器统计循环体执行循环次数","categories":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/categories/JVM/"}],"tags":[{"name":"热点探测","slug":"热点探测","permalink":"http://example.com/tags/%E7%83%AD%E7%82%B9%E6%8E%A2%E6%B5%8B/"},{"name":"即时编译","slug":"即时编译","permalink":"http://example.com/tags/%E5%8D%B3%E6%97%B6%E7%BC%96%E8%AF%91/"},{"name":"方法计数器","slug":"方法计数器","permalink":"http://example.com/tags/%E6%96%B9%E6%B3%95%E8%AE%A1%E6%95%B0%E5%99%A8/"},{"name":"回边计数器","slug":"回边计数器","permalink":"http://example.com/tags/%E5%9B%9E%E8%BE%B9%E8%AE%A1%E6%95%B0%E5%99%A8/"}],"author":"John Doe"},{"title":"Hot Spot JVM执行方式","slug":"Hot-Spot-JVM执行方式","date":"2022-03-19T10:52:00.000Z","updated":"2022-03-19T10:57:38.235Z","comments":true,"path":"2022/03/19/Hot-Spot-JVM执行方式/","link":"","permalink":"http://example.com/2022/03/19/Hot-Spot-JVM%E6%89%A7%E8%A1%8C%E6%96%B9%E5%BC%8F/","excerpt":"","text":"当虚拟机启动的时候，解释器可以先发挥作用，而不必等待及时编译器全部编译完在执行，可以节约不必要的编译时间，而在随着程序运行时间的推移。及时编译器会逐步发挥作用，根据热点探测技术，将有价值的字节码编译成本地的机器指令，换取更高效率的程序运行。 机器在热机状态的负载要大于冷机状态。如果以热机状态进行流量切割，可能会使得处于冷机状态的服务器因无法承载流量而假死","categories":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/categories/JVM/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}],"author":"John Doe"},{"title":"解释器的分类","slug":"解释器的分类","date":"2022-03-19T10:38:00.000Z","updated":"2022-03-19T10:42:48.741Z","comments":true,"path":"2022/03/19/解释器的分类/","link":"","permalink":"http://example.com/2022/03/19/%E8%A7%A3%E9%87%8A%E5%99%A8%E7%9A%84%E5%88%86%E7%B1%BB/","excerpt":"","text":"在java的发展历史上，一共有两套解释执行器，即古老的字节码解释器和现在普遍使用的模板解释器。 字节码解释器在执行时通过纯软件代码模拟字节码的执行，效率低下 模板解释器将每一条字节码和一个模板函数相关联，模板函数中直接产生字节码执行时的机器码，从而提升解释器的性能 在Hotspot JVM中，解释器主要由Interpreter模块和Code模块构成 Interpreter：实现了解释器的核心功能 Code模块： 用于管理HostSpot JVM在运行时生成的本地机器指令","categories":[{"name":"解释器","slug":"解释器","permalink":"http://example.com/categories/%E8%A7%A3%E9%87%8A%E5%99%A8/"},{"name":"JVM","slug":"解释器/JVM","permalink":"http://example.com/categories/%E8%A7%A3%E9%87%8A%E5%99%A8/JVM/"}],"tags":[{"name":"解释器分类","slug":"解释器分类","permalink":"http://example.com/tags/%E8%A7%A3%E9%87%8A%E5%99%A8%E5%88%86%E7%B1%BB/"}],"author":"John Doe"},{"title":"直接内存Direct Memory","slug":"直接内存Direct-Memory","date":"2022-03-19T10:23:00.000Z","updated":"2022-03-19T10:28:48.265Z","comments":true,"path":"2022/03/19/直接内存Direct-Memory/","link":"","permalink":"http://example.com/2022/03/19/%E7%9B%B4%E6%8E%A5%E5%86%85%E5%AD%98Direct-Memory/","excerpt":"","text":"直接内存Direct Memory不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域。直接内存时Java堆外的，直接向系统申请的内存区间。 直接内存的访问速度优于java堆。读写性能更高 存在的问题：不受指定堆大学的限定，而是受系统内存的限制。而且不受jvm内存管理回收，因此回收成本较高。 MaxDirectMemorySize可以知道其大小，默认与xmx参数值一样","categories":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/categories/JVM/"}],"tags":[{"name":"直接内存","slug":"直接内存","permalink":"http://example.com/tags/%E7%9B%B4%E6%8E%A5%E5%86%85%E5%AD%98/"}],"author":"John Doe"},{"title":"缓存和数据库的数据一致性问题","slug":"缓存和数据库的数据一致性问题","date":"2022-03-19T01:21:00.000Z","updated":"2022-03-19T01:49:36.849Z","comments":true,"path":"2022/03/19/缓存和数据库的数据一致性问题/","link":"","permalink":"http://example.com/2022/03/19/%E7%BC%93%E5%AD%98%E5%92%8C%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/","excerpt":"","text":"缓存和数据库的数据一致性问题是一个老生常谈的问题了。在这里我记录几种方案及其可能出现的问题。 先更新缓存，再更新数据库：此时线程A去先去更新缓存，然后更新数据库，但数据库更新时出现回滚，更新失败，数据库和缓存数据不一致。 先更新数据库，在更新缓存：线程A去更新数据库-&gt;线程B也去更新数据库-&gt;A更新数据库完毕-&gt;A去更新缓存-&gt;B更新数据库完毕-&gt;B更新缓存完毕-&gt;A更新缓存完毕，此时导致数据不一致 先删除缓存，在更新数据库：线程A删除缓存-&gt;线程B查询会去更新缓存-&gt;线程A更新数据库，数据不一致。（延迟双删，即在更新数据库后，延迟在去删除缓存，保证在此期间，其他线程读，导致数据不一致。注意：延迟双删不能绝对解决一致性问题，在更新完毕，延时删除之间，来了读线程，但因为网络等原因，更新缓存操作被延迟到了，延迟删除之后，也会导致数据不一致） 先更新数据库，在删除缓存：线程A更新数据库-&gt;线程A删除缓存-&gt;线程B查询-&gt;线程C更新数据库-&gt;线程C删除缓存-&gt;线程B更新缓存。数据不一致。 请求串行化：将访问操作串行化，先删缓存，将更新数据库的操作放进有序队列中从缓存查不到的查询操作，都进入有序队列。需要解决的问题：读请求积压，大量超时，导致数据库的压力：限流、熔断。如何避免大量请求积压：将队列水平拆分，提高并行度。保证相同请求路由正确。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"},{"name":"数据一致性","slug":"数据一致性","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7/"}],"author":"John Doe"},{"title":"对ThreadLocal的理解","slug":"对ThreadLocal的理解","date":"2022-03-18T14:22:00.000Z","updated":"2022-03-18T15:40:02.496Z","comments":true,"path":"2022/03/18/对ThreadLocal的理解/","link":"","permalink":"http://example.com/2022/03/18/%E5%AF%B9ThreadLocal%E7%9A%84%E7%90%86%E8%A7%A3/","excerpt":"","text":"理解ThreadLocal是Java语言提供用于支持线程局部变量的标准实现类。 在我们编写的代码层面来讲，我们所编写的代码实际上是在管理系统中各个对象的相关状态。如果不能够对各个对象状态的访问进行合理的管理，对象的状态将被破坏，进而导致系统的不正常运行。特别是多线程环境下，多个线程可能同时对系统中的单一或多个对象状态进行访问，如果不能保证在此期间的线程安全，整个系统将会走向崩溃的方向。 当然，很容易想到的就是你可以使用synchronization的方式上锁来解决多线程下的线程安全问题，但是这种方式无疑是一个重量级的方式，上了synchronization锁，性能消耗很大。 那怎么解决呢？synchronization方式是避免了同一时刻多个线程对共享对象的访问，将之变为同步访问。而此时，其实也可以通过ThreadLocal避免对象共享，来达到线程安全。共享对象在个线程内都有一个副本，线程对副本进行操作，结束后再修改共享变量，达到线程安全。 因此从上面来看，其实synchronization和ThreadLocal在保证线程安全方式上，前者像是从纵向上的一种方式，而后者则是横向上的一种方式。 实现虽然是通过ThreadLocal来设定于各个线程的数据资源，但ThreadLocal自身不会保存这些特定的资源数据的。因为数据资源位于对应的线程，由线程管理，而每个线程有一个ThreadLocal.ThreadLocalMap类型的名为ThreadLocal的实例变量，它就是保持那些通过ThreadL哦按差设置给这个资源的地方。当通过ThreadLocal的set方法设置数据的时候，Threadloacl会获取当前这个线程的引用，然后通过该引用获取当前线程的threadLocals，然后将当前线程当做key，将要设置的数据设置到当前线程 事实上，ThreadLocal就好像是一个窗口，通过这个窗口，我们可以将特定于线程的线程的数据资源绑定到当前线程，也可以通过这个窗口获取绑定的资源。在整个线程的生命周期，我们都可以通过ThreadLoacl这个窗口与当前线程打交道。 类比城市的各个公交线，就好似系统中的个个线程，在各个公交线上，会有对应的公交车（ThreadLocal），用于运送特定线路的乘客（数据资源），乘客可以下车或上车（数据的装载和清除），在整条路线上都有各个乘车点（处理数据），最终达到终点（线程消亡）。 ThreadLocal的理解 管理应用程序实现中的线程安全。对于某些有状态的或者非状态的线程安全对象，可以在多线程为某个线程分配对应副本，而不是让多个线程共享该类型的某个对象，从而避免了需要多线程对这些对象进行访问的危险工作。（比如使用JDBC进行数据库访问的过程中，connection对象就属于那种有状态并且非线程安全的类，我们通过ThreadLocal为每个线程分配一个特有的connection保证数据访问安全） 我们也可以通过ThreadLocal来跟踪保存在线程内的日志序列，在程序执行的任何必要点将系统跟踪信息追加到ThreadLocal，然后在合适的时点取出分析。 我们还可以通过ThreadLocal来保存某个全局变量，在合适的时点取出做某些逻辑。采用ThreadLocal在当前执行的线程类执行数据流转，可以表面耦合性很强的参数传递。（这种处理方式有一种让数据随波逐流的意思，一旦处理不当，甚至会令系统出现差异） 某些情况下的性能优化（即以空间换取时间的方式） per-thread Singleton，当某项资源的初始化代价很大，并且在执行中会多次创建，可以存入threadloca避免多次创建。","categories":[{"name":"多线程","slug":"多线程","permalink":"http://example.com/categories/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"ThreadLocal","slug":"多线程/ThreadLocal","permalink":"http://example.com/categories/%E5%A4%9A%E7%BA%BF%E7%A8%8B/ThreadLocal/"}],"tags":[{"name":"ThreadLocal","slug":"ThreadLocal","permalink":"http://example.com/tags/ThreadLocal/"},{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"author":"John Doe"},{"title":"JVM针对不同年龄段的 对象的分配策略","slug":"JVM针对不同年龄段的-对象的分配策略","date":"2022-03-15T03:25:00.000Z","updated":"2022-03-15T03:35:09.059Z","comments":true,"path":"2022/03/15/JVM针对不同年龄段的-对象的分配策略/","link":"","permalink":"http://example.com/2022/03/15/JVM%E9%92%88%E5%AF%B9%E4%B8%8D%E5%90%8C%E5%B9%B4%E9%BE%84%E6%AE%B5%E7%9A%84-%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/","excerpt":"","text":"优先分配到eden区 对于大对象会直接分配到老年代（eden区young gc后还是无法分配足够内存给大对象，大对象会直接跃升到老年代或者在young gc后，幸存者区放不下跃升到幸存者区的大对象，也会直接跃升到老年代，如果老年代也不够分配，则进行major gc，如果仍然不够，则oom） 长期存活的对象分配到老年代（根据分代年龄晋升，默认15，每经历一次yonug gc存活下来，分代年龄+1） 动态对象年龄判断：如果幸存者区中相同年龄的所有对象大小的总和大于幸存者区空间的一半，年龄大于或等于该年龄的对象可以直接进入老年代，无需达到分代年龄要求","categories":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/categories/JVM/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"},{"name":"堆","slug":"堆","permalink":"http://example.com/tags/%E5%A0%86/"},{"name":"对象内存分配","slug":"对象内存分配","permalink":"http://example.com/tags/%E5%AF%B9%E8%B1%A1%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/"}],"author":"John Doe"},{"title":"Elasticsearch分片原理","slug":"Elasticsearch分片原理","date":"2022-03-14T07:09:00.000Z","updated":"2022-03-14T08:17:41.076Z","comments":true,"path":"2022/03/14/Elasticsearch分片原理/","link":"","permalink":"http://example.com/2022/03/14/Elasticsearch%E5%88%86%E7%89%87%E5%8E%9F%E7%90%86/","excerpt":"","text":"倒排索引分片是 Elasticsearch 最小的工作单元。一个分片是一个Lucene索引。 而索引采用的是一种称为倒排索引的结构，它适用于快速的全文搜索。 见其名，知其意，有倒排索引，肯定会对应有正向索引。正向索引（forward index），反向索引（inverted index）更熟悉的名字是倒排索引。 所谓的正向索引，就是搜索引擎会将待搜索的文件都对应一个文件 ID，搜索时将这个ID 和搜索关键字进行对应，形成 K-V 对，然后对关键字进行统计计数。 但是互联网上收录在搜索引擎中的文档的数目是个天文数字，这样的索引结构根本无法满足实时返回排名结果的要求。所以，搜索引擎会将正向索引重新构建为倒排索引，即把文件ID对应到关键词的映射转换为关键词到文件ID的映射，每个关键词都对应着一系列的文件，这些文件中都出现这个关键词。 一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。 例如，假设我们有两个文档，每个文档的 content 域包含如下内容：  The quick brown fox jumped over the lazy dog  Quick brown foxes leap over lazy dogs in summer 为了创建倒排索引，我们首先将每个文档的 content 域拆分成单独的词（我们称它为词条或 tokens ），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。 现在，如果我们想搜索 quick brown ，我们只需要查找包含每个词条的文档： 两个文档都匹配，但是第一个文档比第二个匹配度更高。如果我们使用仅计算匹配词条数量的简单相似性算法，那么我们可以说，对于我们查询的相关性来讲，第一个文档比第二个文档更佳。 但是，我们目前的倒排索引有一些问题：  Quick 和 quick 以独立的词条出现，然而用户可能认为它们是相同的词。  fox 和 foxes 非常相似, 就像 dog 和 dogs ；他们有相同的词根。  jumped 和 leap, 尽管没有相同的词根，但他们的意思很相近。他们是同义词。 使用前面的索引搜索 +Quick +fox 不会得到任何匹配文档。（记住，+ 前缀表明这个词必须存在。）只有同时出现 Quick 和 fox 的文档才满足这个查询条件，但是第一个文档包含quick fox ，第二个文档包含 Quick foxes 。 我们的用户可以合理的期望两个文档与查询匹配。我们可以做的更好。如果我们将词条规范为标准模式，那么我们可以找到与用户搜索的词条不完全一致，但具有足够相关性的文档。 例如：  Quick 可以小写化为 quick 。  foxes 可以 词干提取 –变为词根的格式– 为 fox 。类似的， dogs 可以为提取为 dog 。  jumped 和 leap 是同义词，可以索引为相同的单词 jump 。 这还远远不够。我们搜索 +Quick +fox 仍然 会失败，因为在我们的索引中，已经没有 Quick 了。但是，如果我们对搜索的字符串使用与 content 域相同的标准化规则，会变成查询+quick +fox，这样两个文档都会匹配！分词和标准化的过程称为分析. 这非常重要。你只能搜索在索引中出现的词条，所以索引文本和查询字符串必须标准化为相同的格式。 文档搜索早期的全文检索会为整个文档集合建立一个很大的倒排索引并将其写入到磁盘。 一旦新的索引就绪，旧的就会被其替换，这样最近的变化便可以被检到。 倒排索引被写入磁盘后是 不可改变 的:它永远不会修改。 不变性有重要的价值： 不需要锁。如果你从来不更新索引，你就不需要担心多进程同时修改数据的问题。 一旦索引被读入内核的文件系统缓存，便会留在哪里，由于其不变性。只要文件系统缓存中还有足够的空间，那么大部分读请求会直接请求内存，而不会命中磁盘。这提供了很大的性能提升。 其它缓存(像 filter 缓存)，在索引的生命周期内始终有效。它们不需要在每次数据改变时被重建，因为数据不会变化。 写入单个大的倒排索引允许数据被压缩，减少磁盘 I/O 和 需要被缓存到内存的索引的使用量。 当然，一个不变的索引也有不好的地方。主要事实是它是不可变的! 你不能修改它。如果你需要让一个新的文档 可被搜索，你需要重建整个索引。这要么对一个索引所能包含的数据量造成了很大的限制，要么对索引可被更新的频率造成了很大的限制。 动态更新索引如何在保留不变性的前提下实现倒排索引的更新？ 答案是: 用更多的索引。通过增加新的补充索引来反映新近的修改，而不是直接重写整个倒排索引。每一个倒排索引都会被轮流查询到，从最早的开始查询完后再对结果进行合并。 Elasticsearch 基于 Lucene, 这个 java 库引入了按段搜索的概念。 每一段本身都是一个倒排索引， 但索引在 Lucene 中除表示所有段的集合外，还增加了提交点的概念 — 一个列出了所有已知段的文件。 按段搜索会以如下流程执行： 新文档被收集到内存索引缓存 不时地, 缓存被提交 (1) 一个新的段—一个追加的倒排索引—被写入磁盘。 (2) 一个新的包含新段名字的 提交点 被写入磁盘 (3) 磁盘进行 同步 — 所有在文件系统缓存中等待的写入都刷新到磁盘，以确保它们被写入物理文件 新的段被开启，让它包含的文档可见以被搜索 内存缓存被清空，等待接收新的文档 当一个查询被触发，所有已知的段按顺序被查询。词项统计会对所有段的结果进行聚合，以保证每个词和每个文档的关联都被准确计算。 这种方式可以用相对较低的成本将新文档添加到索引。 段是不可改变的，所以既不能从把文档从旧的段中移除，也不能修改旧的段来进行反映文档的更新。 取而代之的是，每个提交点会包含一个 .del 文件，文件中会列出这些被删除文档的段信息。 当一个文档被 “删除” 时，它实际上只是在 .del 文件中被 标记 删除。一个被标记删除的文档仍然可以被查询匹配到， 但它会在最终结果被返回前从结果集中移除。文档更新也是类似的操作方式：当一个文档被更新时，旧版本文档被标记删除，文档的新版本被索引到一个新的段中。 可能两个版本的文档都会被一个查询匹配到，但被删除的那个旧版本文档在结果集返回前就已经被移除。 近实时搜索随着按段（per-segment）搜索的发展，一个新的文档从索引到可被搜索的延迟显著降低了。新文档在几分钟之内即可被检索，但这样还是不够快。磁盘在这里成为了瓶颈。提（Commiting）一个新的段到磁盘需要一个 fsync 来确保段被物理性地写入磁盘，这样在断电的时候就不会丢失数据。 但是 fsync 操作代价很大; 如果每次索引一个文档都去执行一次的话会造成很大的性能问题。 我们需要的是一个更轻量的方式来使一个文档可被搜索，这意味着 fsync 要从整个过程中被移除。在 Elasticsearch 和磁盘之间是文件系统缓存。 像之前描述的一样， 在内存索引缓冲区中的文档会被写入到一个新的段中。 但是这里新段会被先写入到文件系统缓存—这一步代价会比较低，稍后再被刷新到磁盘—这一步代价比较高。不过只要文件已经在缓存中，就可以像其它文件一样被打开和读取了。 Lucene 允许新段被写入和打开—使其包含的文档在未进行一次完整提交时便对搜索可见。这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。 在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 refresh 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是 近 实时搜索: 文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。 这些行为可能会对新用户造成困惑: 他们索引了一个文档然后尝试搜索它，但却没有搜到。这个问题的解决办法是用 refresh API 执行一次手动刷新: /users/_refresh 尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候， 手动刷新很有用，但是不要在生产环境下每次索引一个文档都去手动刷新。 相反，你的应用需要意识到 Elasticsearch 的近实时的性质，并接受它的不足。 并不是所有的情况都需要每秒刷新。可能你正在使用 Elasticsearch 索引大量的日志文件，你可能想优化索引速度而不是近实时搜索， 可以通过设置 refresh_interval ， 降低每个索引的刷新频率 refresh_interval 可以在既存索引上进行动态更新。 在生产环境中，当你正在建立一个大的新索引时，可以先关闭自动刷新，待开始使用该索引时，再把它们调回来 持久变更如果没有用 fsync 把数据从文件系统缓存刷（flush）到硬盘，我们不能保证数据在断电甚至是程序正常退出之后依然存在。为了保证 Elasticsearch 的可靠性，需要确保数据变化被持久化到磁盘。在 动态更新索引，我们说一次完整的提交会将段刷到磁盘，并写入一个包含所有段列表的提交点。Elasticsearch 在启动或重新打开一个索引的过程中使用这个提交点来判断哪些段隶属于当前分片。 即使通过每秒刷新（refresh）实现了近实时搜索，我们仍然需要经常进行完整提交来确保能从失败中恢复。但在两次提交之间发生变化的文档怎么办？我们也不希望丢失掉这些数据。Elasticsearch 增加了一个 translog ，或者叫事务日志，在每一次对 Elasticsearch 进行操作时均进行了日志记录 一个文档被索引之后，就会被添加到内存缓冲区，并且追加到了 translog 刷新（refresh）使分片每秒被刷新（refresh）一次： 这些在内存缓冲区的文档被写入到一个新的段中，且没有进行 fsync 操作。 这个段被打开，使其可被搜索 内存缓冲区被清空 这个进程继续工作，更多的文档被添加到内存缓冲区和追加到事务日志 每隔一段时间—例如 translog 变得越来越大—索引被刷新（flush）；一个新的 translog 被创建，并且一个全量提交被执行 所有在内存缓冲区的文档都被写入一个新的段。 缓冲区被清空。 一个提交点被写入硬盘。 文件系统缓存通过 fsync 被刷新（flush）。 老的 translog 被删除。 translog 提供所有还没有被刷到磁盘的操作的一个持久化纪录。当 Elasticsearch 启动的时候， 它会从磁盘中使用最后一个提交点去恢复已知的段，并且会重放 translog 中所有在最后一次提交后发生的变更操作。 translog 也被用来提供实时 CRUD 。当你试着通过 ID 查询、更新、删除一个文档，它会在尝试从相应的段中检索之前， 首先检查 translog 任何最近的变更。这意味着它总是能够实时地获取到文档的最新版本。 执行一个提交并且截断 translog 的行为在 Elasticsearch 被称作一次 flush分片每 30 分钟被自动刷新（flush），或者在 translog 太大的时候也会刷新 你很少需要自己手动执行 flush 操作；通常情况下，自动刷新就足够了。这就是说，在重启节点或关闭索引之前执行 flush 有益于你的索引。当 Elasticsearch 尝试恢复或重新打开一个索引， 它需要重放 translog 中所有的操作，所以如果日志越短，恢复越快。 translog 的目的是保证操作不会丢失，在文件被 fsync 到磁盘前，被写入的文件在重启之后就会丢失。默认 translog 是每 5 秒被 fsync 刷新到硬盘， 或者在每次写请求完成之后执行(e.g. index, delete, update, bulk)。这个过程在主分片和复制分片都会发生。最终， 基本上，这意味着在整个请求被 fsync 到主分片和复制分片的 translog 之前，你的客户端不会得到一个 200 OK 响应。 在每次请求后都执行一个 fsync 会带来一些性能损失，尽管实践表明这种损失相对较小（特别是 bulk 导入，它在一次请求中平摊了大量文档的开销）。但是对于一些大容量的偶尔丢失几秒数据问题也并不严重的集群，使用异步的 fsync 还是比较有益的。比如，写入的数据被缓存到内存中，再每 5 秒执行一次 fsync 。如果你决定使用异步 translog 的话，你需要 保证 在发生 crash 时，丢失掉 sync_interval 时间段的数据也无所谓。请在决定前知晓这个特性。如果你不确定这个行为的后果，最好是使用默认的参数（ “index.translog.durability”: “request” ）来避免数据丢失。 段合并由于自动刷新流程每秒会创建一个新的段 ，这样会导致短时间内的段数量暴增。而段数目太多会带来较大的麻烦。 每一个段都会消耗文件句柄、内存和 cpu 运行周期。更重要的是，每个搜索请求都必须轮流检查每个段；所以段越多，搜索也就越慢。Elasticsearch 通过在后台进行段合并来解决这个问题。小的段被合并到大的段，然后这些大的段再被合并到更大的段。段合并的时候会将那些旧的已删除文档从文件系统中清除。被删除的文档（或被更新文档的旧版本）不会被拷贝到新的大段中。启动段合并不需要你做任何事。进行索引和搜索时会自动进行。 当索引的时候，刷新（refresh）操作会创建新的段并将段打开以供搜索使用。 合并进程选择一小部分大小相似的段，并且在后台将它们合并到更大的段中。这并不会中断索引和搜索。 一旦合并结束，老的段被删除 新的段被刷新（flush）到了磁盘。 ** 写入一个包含新段且排除旧的和较小的段的新提交点。 新的段被打开用来搜索。 老的段被删除。 合并大的段需要消耗大量的 I/O 和 CPU 资源，如果任其发展会影响搜索性能。Elasticsearch在默认情况下会对合并流程进行资源限制，所以搜索仍然 有足够的资源很好地执行。","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"}],"tags":[{"name":"分片","slug":"分片","permalink":"http://example.com/tags/%E5%88%86%E7%89%87/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/tags/Elasticsearch/"}],"author":"John Doe"},{"title":"Elasticsearch分片控制","slug":"Elasticsearch分片控制","date":"2022-03-14T06:55:00.000Z","updated":"2022-03-14T07:09:39.594Z","comments":true,"path":"2022/03/14/Elasticsearch分片控制/","link":"","permalink":"http://example.com/2022/03/14/Elasticsearch%E5%88%86%E7%89%87%E6%8E%A7%E5%88%B6/","excerpt":"","text":"每个节点都有能力处理任意请求。 每个节点都知道集群中任一文档位置，所以可以直接将请求转发到需要的节点上。 写流程： 新建、索引和删除 请求都是 写 操作， 必须在主分片上面完成之后才能被复制到相关的副本分片 新建，索引和删除文档所需要的步骤顺序： 客户端向 Node 1 发送新建、索引或者删除请求。 节点使用文档的 _id 确定文档属于分片 0 。请求会被转发到 Node 3，因为分片 0 的主分片目前被分配在 Node 3 上。 Node 3 在主分片上面执行请求。如果成功了，它将请求并行转发到 Node 1 和 Node 2 的副本分片上。一旦所有的副本分片都报告成功, Node 3 将向协调节点报告成功，协调节点向客户端报告成功。 在客户端收到成功响应时，文档变更已经在主分片和所有副本分片执行完成，变更是安全的。有一些可选的请求参数允许您影响这个过程，可能以数据安全为代价提升性能。 读流程：我们可以从主分片或者从其它任意副本分片检索文档。 从主分片或者副本分片检索文档的步骤顺序： 客户端向 Node 1 发送获取请求。 节点使用文档的 _id 来确定文档属于分片 0 。分片 0 的副本分片存在于所有的三个节点上。 在这种情况下，它将请求转发到 Node 2 。 Node 2 将文档返回给 Node 1 ，然后将文档返回给客户端。在处理读取请求时，协调结点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡。在文档被检索时，已经被索引的文档可能已经存在于主分片上但是还没有复制到副本分片。 在这种情况下，副本分片可能会报告文档不存在，但是主分片可能成功返回文档。 一旦索引请求成功返回给用户，文档在主分片和副本分片都是可用的。 更新流程： 客户端向 Node 1 发送更新请求。 它将请求转发到主分片所在的 Node 3 。 Node 3 从主分片检索文档，修改 _source 字段中的 JSON ，并且尝试重新索引主分片的文档。如果文档已经被另一个进程修改，它会重试步骤 3 ，超过 retry_on_conflict 次后放弃。 如果 Node 3 成功地更新文档，它将新版本的文档并行转发到 Node 1 和 Node 2 上的副本分片，重新建立索引。一旦所有副本分片都返回成功， Node 3 向协调节点也返回成功，协调节点向客户端返回成功。 注意：当主分片把更改转发到副本分片时， 它不会转发更新请求。 相反，它转发完整文档的新版本。请记住，这些更改将会异步转发到副本分片，并且不能保证它们以发送它们相同的顺序到达。 如果 Elasticsearch 仅转发更改请求，则可能以错误的顺序应用改，导致得到损坏的文档。 多文档操作流程：mget 和 bulk API 的模式类似于单文档模式。区别在于协调节点知道每个文档存在于哪个分片中。它将整个多文档请求分解成每个分片的多文档请求，并且将这些请求并行转发到每个参与节点。协调节点一旦收到来自每个节点的应答，就将每个节点的响应收集整理成单个响应，返回给客户端。 用单个 mget 请求取回多个文档所需的步骤顺序: 客户端向 Node 1 发送 mget 请求。 Node 1 为每个分片构建多文档获取请求，然后并行转发这些请求到托管在每个所需的主分片或者副本分片的节点上。一旦收到所有答复， Node 1 构建响应并将其返回给客户端 bulk API 按如下步骤顺序执行： 客户端向 Node 1 发送 bulk 请求。 Node 1 为每个节点创建一个批量请求，并将这些请求并行转发到每个包含主分片的节点主机。 主分片一个接一个按顺序执行每个操作。当每个操作成功时，主分片并行转发新文档（或删除）到副本分片，然后执行下一个操作。 一旦所有的副本分片报告所有操作成功，该节点将向协调节点报告成功，协调节点将这些响应收集整理并返回给客户端。","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"}],"tags":[{"name":"分片","slug":"分片","permalink":"http://example.com/tags/%E5%88%86%E7%89%87/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/tags/Elasticsearch/"}],"author":"John Doe"},{"title":"Elasticsearch路由计算","slug":"Elasticsearch路由计算","date":"2022-03-14T06:53:00.000Z","updated":"2022-03-14T06:55:12.233Z","comments":true,"path":"2022/03/14/Elasticsearch路由计算/","link":"","permalink":"http://example.com/2022/03/14/Elasticsearch%E8%B7%AF%E7%94%B1%E8%AE%A1%E7%AE%97/","excerpt":"","text":"当索引一个文档的时候，文档会被存储到一个主分片中。 Elasticsearch 如何知道一个文档应该存放到哪个分片中呢？当我们创建文档时，它如何决定这个文档应当被存储在分片1 还是分片 2 中呢？首先这肯定不会是随机的，否则将来要获取文档的时候我们就不知道从何处寻找了。实际上，这个过程是根据下面这个公式决定的： routing 是一个可变值，默认是文档的 _id ，也可以设置成一个自定义的值。 routing 通过hash 函数生成一个数字，然后这个数字再除以 number_of_primary_shards （主分片的数量）后得到余数 。这个分布在 0 到number_of_primary_shards-1 之间的余数，就是我们所寻求的文档所在分片的位置。 这就解释了为什么我们要在创建索引的时候就确定好主分片的数量 并且永远不会改变这个数量：因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。所有的文档 API（ get 、 index 、 delete 、 bulk 、 update 以及 mget ）都接受一个叫做 routing 的路由参数 ，通过这个参数我们可以自定义文档到分片的映射。一个自定义的路由参数可以用来确保所有相关的文档——例如所有属于同一个用户的文档——都被存储到同一个分片中。","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"}],"tags":[{"name":"routing","slug":"routing","permalink":"http://example.com/tags/routing/"}],"author":"John Doe"},{"title":"Elasticsearch的基本概念","slug":"Elasticsearch的基本概念","date":"2022-03-14T06:09:00.000Z","updated":"2022-03-14T06:23:48.299Z","comments":true,"path":"2022/03/14/Elasticsearch的基本概念/","link":"","permalink":"http://example.com/2022/03/14/Elasticsearch%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","excerpt":"","text":"Elasticsearch 索引的精髓：一切设计都是为了提高搜索的性能。 Elasticsearch的索引就是一个拥有相似特征的文档的集合。一个索引由一个名字来标识（必须全是小写字母）。当我们要对这个索引中的文档进行索引、搜索、更新和删除的时候，都要使用到这个名字。在一个集群中，可以定义任意多的索引。 在一个索引中，你可以定义一种或多种类型。这个类型是索引的一个逻辑分区/分类。但在7.x已经默认不再支持自定义索引类型。 文档（Document）：一个文档是一个可被索引的基础信息单元，也就是一条数据。 字段（Field）：相当于数据表的字段，对文档数据根据不同属性进行的分类标识。 映射（Mapping）：mapping 是处理数据的方式和规则方面做一些限制，如：某个字段的数据类型、默认值、分析器、是否被索引等等。这些都是映射里面可以设置的，其它就是处理 ES 里面数据的一些使用规则设置也叫做映射，按着最优规则处理数据对性能提高很大，因此才需要建立映射，并且需要思考如何建立映射才能对性能更好。 分片（Shards）：一个索引可以存储超出单个节点硬件限制的大量数据。比如，一个具有 10 亿文档数据的索引占据 1TB 的磁盘空间，而任一节点都可能没有这样大的磁盘空间。或者单个节点处理搜索请求，响应太慢。为了解决这个问题，Elasticsearch 提供了将索引划分成多份的能力，每一份就称之为分片。当你创建一个索引的时候，你可以指定你想要的分片的数量。每个分片本身也是一个功能完善并且独立的“索引”，这个“索引”可以被放置到集群中的任何节点上。 分片很重要，主要有两方面的原因： 1）允许你水平分割 / 扩展你的内容容量。 2）允许你在分片之上进行分布式的、并行的操作，进而提高性能/吞吐量。 至于一个分片怎样分布，它的文档怎样聚合和搜索请求，是完全由 Elasticsearch 管理的，对于作为用户的你来说，这些都是透明的，无需过分关心。 被混淆的概念是，一个 Lucene 索引 我们在 Elasticsearch 称作 分片 。 一个Elasticsearch 索引 是分片的集合。 当 Elasticsearch 在索引中搜索的时候， 他发送查询到每一个属于索引的分片(Lucene 索引)，然后合并每个分片的结果到一个全局的结果集。（即Elasticsearch 索引 是分片的集合，Lucene 索引在Elasticsearch 称作 分片） 副本（Replicas）：在一个网络 / 云的环境里，失败随时都可能发生，在某个分片/节点不知怎么的就处于离线状态，或者由于任何原因消失了，这种情况下，有一个故障转移机制是非常有用并且是强烈推荐的。为此目的，Elasticsearch 允许你创建分片的一份或多份拷贝，这些拷贝叫做复制分片(副本)。 复制分片之所以重要，有两个主要原因：  在分片/节点失败的情况下，提供了高可用性。因为这个原因，注意到复制分片从不与原/主要（original/primary）分片置于同一节点上是非常重要的。  扩展你的搜索量/吞吐量，因为搜索可以在所有的副本上并行运行。 总之，每个索引可以被分成多个分片。一个索引也可以被复制 0 次（意思是没有复制）或多次。一旦复制了，每个索引就有了主分片（作为复制源的原来的分片）和复制分片（主分片的拷贝）之别。分片和复制的数量可以在索引创建的时候指定。在索引创建之后，你可以在任何时候动态地改变复制的数量，但是你事后不能改变分片的数量。默认情况下，Elasticsearch 中的每个索引被分片 1 个主分片和 1 个复制，这意味着，如果你的集群中至少有两个节点，你的索引将会有 1 个主分片和另外 1 个复制分片（1 个完全拷贝），这样的话每个索引总共就有 2 个分片，我们需要根据索引需要确定分片个数。 分配（Allocation）：将分片分配给某个节点的过程，包括分配主分片或者副本。如果是副本，还包含从主分片复制数据的过程。这个过程是由 master 节点完成的。","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/tags/Elasticsearch/"},{"name":"基础概念","slug":"基础概念","permalink":"http://example.com/tags/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"}],"author":"John Doe"},{"title":"时间轮算法","slug":"时间轮算法","date":"2022-03-13T05:08:00.000Z","updated":"2022-03-13T05:18:08.769Z","comments":true,"path":"2022/03/13/时间轮算法/","link":"","permalink":"http://example.com/2022/03/13/%E6%97%B6%E9%97%B4%E8%BD%AE%E7%AE%97%E6%B3%95/","excerpt":"","text":"时间轮就是和手表时钟很相似的存在。时间轮用环形数组实现，数组的每个元素可以称为槽，和 HashMap一样称呼。 槽的内部用双向链表存着待执行的任务，添加和删除的链表操作时间复杂度都是 O(1)，槽位本身也指代时间精度，比如一秒扫一个槽，那么这个时间轮的最高精度就是 1 秒。 也就是说延迟 1.2 秒的任务和 1.5 秒的任务会被加入到同一个槽中，然后在 1 秒的时候遍历这个槽中的链表执行任务。 从图中可以看到此时指针指向的是第一个槽，一共有八个槽0~7，假设槽的时间单位为 1 秒，现在要加入一个延时 5 秒的任务，计算方式就是 5 % 8 + 1 = 6，即放在槽位为 6，下标为 5 的那个槽中。 更具体的就是拼到槽的双向链表的尾部。然后每秒指针顺时针移动一格，这样就扫到了下一格，遍历这格中的双向链表执行任务。然后再循环继续。 可以看到插入任务从计算槽位到插入链表，时间复杂度都是O(1)。那假设现在要加入一个50秒后执行的任务怎么办？这槽好像不够啊？难道要加槽嘛？和HashMap一样扩容？ 不是的，常见有两种方式，一种是通过增加轮次的概念。50 % 8 + 1 = 3，即应该放在槽位是 3，下标是2 的位置。然后 (50 - 1) / 8 = 6，即轮数记为 6。也就是说当循环 6 轮之后扫到下标的 2 的这个槽位会触发这个任务。Netty中HashedWheelTimer 使用的就是这种方式。 还有一种是通过多层次的时间轮，这个和我们的手表就更像了，像我们秒针走一圈，分针走一格，分针走一圈，时针走一格。 多层次时间轮就是这样实现的。假设上图就是第一层，那么第一层走了一圈，第二层就走一格，可以得知第二层的一格就是8秒，假设第二层也是 8 个槽，那么第二层走一圈，第三层走一格，可以得知第三层一格就是 64 秒。那么一格三层，每层8个槽，一共 24 个槽时间轮就可以处理最多延迟 512 秒的任务。 而多层次时间轮还会有降级的操作，假设一个任务延迟 500 秒执行，那么刚开始加进来肯定是放在第三层的，当时间过了 436 秒后，此时还需要 64 秒就会触发任务的执行，而此时相对而言它就是个延迟 64秒后的任务，因此它会被降低放在第二层中，第一层还放不下它。再过个 56 秒，相对而言它就是个延迟 8 秒后执行的任务，因此它会再被降级放在第一层中，等待执行。 降级是为了保证时间精度一致性Kafka内部用的就是多层次的时间轮算法。 Kafka 就利用了空间换时间的思想，通过 DelayQueue，来保存每个槽，通过每个槽的过期时间排序。这样拥有最早需要执行任务的槽会有优先获取。如果时候未到，那么 delayQueue.poll 就会阻塞着，这样就不会有空推进的情况发送。 总的来说Kafka用了多层次时间轮来实现，并且是按需创建时间轮，采用任务的绝对时间来判断延期，并且对于每个槽(槽内存放的也是任务的双向链表)都会维护一个过期时间，利用 DelayQueue 来对每个槽的过期时间排序，来进行时间的推进，防止空推进的存在。 每次推进都会更新 currentTime 为当前时间戳，当然做了点微调使得 currentTime 是 tickMs 的整数倍。并且每次推进都会把能降级的任务重新插入降级。 可以看到这里的 DelayQueue 的元素是每个槽，而不是任务，因此数量就少很多了，这应该是权衡了对于槽操作的延时队列的时间复杂度与空推进的影响。 总结： Timer、DelayQueue 和 ScheduledThreadPool，它们都是基于优先队列实现的，O(logn)的时间复杂度在任务数多的情况下频繁的入队出队对性能来说有损耗。因此适合于任务数不多的情况。 Timer 是单线程的会有阻塞的风险，并且对异常没有做处理，一个任务出错 Timer 就挂了。而ScheduledThreadPool 相比于 Timer 首先可以多线程来执行任务，并且线程池对异常做了处理，使得任务之间不会有影响。 并且 Timer和ScheduledThreadPool 可以周期性执行任务。 而 DelayQueue 就是个具有优先级的阻塞队列。 对比而言时间轮更适合任务数很大的延时场景，它的任务插入和删除时间复杂度都为O(1)。对于延迟超过时间轮所能表示的范围有两种处理方式，一是通过增加一个字段-轮数，Netty 就是这样实现的。二是多层次时间轮，Kakfa 是这样实现的。 相比而言 Netty 的实现会有空推进的问题，而 Kafka 采用 DelayQueue 以槽为单位，利用空间换时间的思想解决了空推进的问题。 可以看出延迟任务的实现都不是很精确的，并且或多或少都会有阻塞的情况，即使你异步执行，线程不够的情况下还是会阻塞。","categories":[{"name":"算法","slug":"算法","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/"},{"name":"Kafka","slug":"算法/Kafka","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/Kafka/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"时间轮","slug":"时间轮","permalink":"http://example.com/tags/%E6%97%B6%E9%97%B4%E8%BD%AE/"}],"author":"John Doe"},{"title":"延迟队列 DelayQueue","slug":"延迟队列-DelayQueue","date":"2022-03-13T05:05:00.000Z","updated":"2022-03-13T05:07:25.947Z","comments":true,"path":"2022/03/13/延迟队列-DelayQueue/","link":"","permalink":"http://example.com/2022/03/13/%E5%BB%B6%E8%BF%9F%E9%98%9F%E5%88%97-DelayQueue/","excerpt":"","text":"Java 中还有个延迟队列 DelayQueue，加入延迟队列的元素都必须实现 Delayed 接口。延迟队列内部是利用 PriorityQueue 实现的，所以还是利用优先队列！Delayed 接口继承了Comparable 因此优先队列是通过 delay 来排序的。 延迟队列是利用优先队列实现的，元素通过实现 Delayed 接口来返回延迟的时间。不过延迟队列就是个容器，需要其他线程来获取和执行任务。 对于 Timer 、ScheduledThreadPool 和 DelayQueue，总结的说下它们都是通过优先队列来获取最早需要执行的任务，因此插入和删除任务的时间复杂度都为O(logn)，并且 Timer 、ScheduledThreadPool 的周期性任务是通过重置任务的下一次执行时间来完成的。 问题就出在时间复杂度上，插入删除时间复杂度是O(logn)，那么假设频繁插入删除次数为 m，总的时间复杂度就是O(mlogn)，这种时间复杂度满足不了 Kafka 这类中间件对性能的要求，而时间轮算法的插入删除时间复杂度是O(1)。","categories":[{"name":"Java","slug":"Java","permalink":"http://example.com/categories/Java/"}],"tags":[{"name":"DelayQueue","slug":"DelayQueue","permalink":"http://example.com/tags/DelayQueue/"},{"name":"队列","slug":"队列","permalink":"http://example.com/tags/%E9%98%9F%E5%88%97/"}],"author":"John Doe"},{"title":"ScheduledThreadPoolExecutor，更多功能的Timer","slug":"ScheduledThreadPoolExecutor，更多功能的Timer","date":"2022-03-13T05:02:00.000Z","updated":"2022-03-13T05:05:25.870Z","comments":true,"path":"2022/03/13/ScheduledThreadPoolExecutor，更多功能的Timer/","link":"","permalink":"http://example.com/2022/03/13/ScheduledThreadPoolExecutor%EF%BC%8C%E6%9B%B4%E5%A4%9A%E5%8A%9F%E8%83%BD%E7%9A%84Timer/","excerpt":"","text":"jdk1.5 引入了 ScheduledThreadPoolExecutor，它是一个具有更多功能的 Timer 的替代品，允许多个服务线程。如果设置一个服务线程和 Timer 没啥差别。 从注释看出相对于 Timer ，可能就是单线程跑任务和多线程跑任务的区别。但ScheduledThreadPoolExecutor继承了 ThreadPoolExecutor，实现了 ScheduledExecutorService。可以定性操作就是正常线程池差不多了。 区别就在于两点，一个是 ScheduledFutureTask ，一个是 DelayedWorkQueue。其实 DelayedWorkQueue 就是优先队列，也是利用数组实现的小顶堆。而 ScheduledFutureTask 继承自 FutureTask 重写了 run 方法，实现了周期性任务的需求。 ScheduledThreadPoolExecutor 大致的流程和 Timer 差不多，也是维护一个优先队列，然后通过重写task 的 run 方法来实现周期性任务，主要差别在于能多线程运行任务，不会单线程阻塞。并且 Java 线程池的设定是 task 出错会把错误吃了，无声无息的。因此一个任务出错也不会影响之后的任务。","categories":[{"name":"Java","slug":"Java","permalink":"http://example.com/categories/Java/"}],"tags":[{"name":"ScheduledThreadPoolExecutor","slug":"ScheduledThreadPoolExecutor","permalink":"http://example.com/tags/ScheduledThreadPoolExecutor/"}],"author":"John Doe"},{"title":"JDK中的Timer","slug":"JDK中的Timer","date":"2022-03-13T04:57:00.000Z","updated":"2022-03-13T05:01:58.460Z","comments":true,"path":"2022/03/13/JDK中的Timer/","link":"","permalink":"http://example.com/2022/03/13/JDK%E4%B8%AD%E7%9A%84Timer/","excerpt":"","text":"java提供了延时操作的timer，里面由一个小根堆数组和执行线程构成。小根堆数组堆顶是当前最先需要执行的任务。执行线程通过不断轮询询问该任务（同系统当前时间做比对）是否需要执行。当需要执行时，看是否是周期性任务，是则将任务执行时间改到下一个周期，然后执行，不是则删除，执行任务。 可以看出 Timer 实际就是根据任务的执行时间维护了一个优先队列，并且起了一个线程不断地拉取任务执行。 有什么弊端呢？ 首先优先队列的插入和删除的时间复杂度是O(logn)，当数据量大的时候，频繁的入堆出堆性能有待考虑。 并且是单线程执行，那么如果一个任务执行的时间过久则会影响下一个任务的执行时间(当然你任务的run要是异步执行也行)。并且从代码可以看到对异常没有做什么处理，那么一个任务出错的时候会导致之后的任务都无法执行。","categories":[{"name":"Java","slug":"Java","permalink":"http://example.com/categories/Java/"}],"tags":[{"name":"Timer","slug":"Timer","permalink":"http://example.com/tags/Timer/"},{"name":"JDK","slug":"JDK","permalink":"http://example.com/tags/JDK/"}],"author":"John Doe"},{"title":"I/O 多路复⽤：select/poll/epoll","slug":"I-O-多路复⽤：select-poll-epoll","date":"2022-03-12T11:22:00.000Z","updated":"2022-03-12T11:44:57.876Z","comments":true,"path":"2022/03/12/I-O-多路复⽤：select-poll-epoll/","link":"","permalink":"http://example.com/2022/03/12/I-O-%E5%A4%9A%E8%B7%AF%E5%A4%8D%E2%BD%A4%EF%BC%9Aselect-poll-epoll/","excerpt":"","text":"最基本的 Socket 模型：要想客户端和服务器能在⽹络中通信，那必须得使⽤ Socket 编程，它是进程间通信⾥⽐较特别的⽅式，特别之处在于它是可以跨主机间通信。创建 Socket 的时候，可以指定⽹络层使⽤的是 IPv4 还是 IPv6，传输层使⽤的是 TCP 还是 UDP。 服务端⾸先调⽤ socket() 函数，创建⽹络协议为 IPv4，以及传输协议为 TCP 的 Socket ，接着调⽤bind() 函数，给这个 Socket 绑定⼀个 IP 地址和端⼝。 绑定端⼝的⽬的：当内核收到 TCP 报⽂，通过 TCP 头⾥⾯的端⼝号，来找到我们的应⽤程序，然后把数据传递给我们。 绑定 IP 地址的⽬的：⼀台机器是可以有多个⽹卡的，每个⽹卡都有对应的 IP 地址，当绑定⼀个⽹卡时，内核在收到该⽹卡上的包，才会发给我们； 绑定完 IP 地址和端⼝后，就可以调⽤ listen() 函数进⾏监听，此时对应 TCP 状态图中的 listen ，如果我们要判定服务器中⼀个⽹络程序有没有启动，可以通过 netstat 命令查看对应的端⼝号是否有被监听。 服务端进⼊了监听状态后，通过调⽤ accept() 函数，来从内核获取客户端的连接，如果没有客户端连接，则会阻塞等待客户端连接的到来。 客户端在创建好 Socket 后，调⽤ connect() 函数发起连接，该函数的参数要指明服务端的 IP 地址和端⼝号，然后万众期待的 TCP 三次握⼿就开始了。 在 TCP 连接的过程中，服务器的内核实际上为每个 Socket 维护了两个队列： ⼀个是还没完全建⽴连接的队列，称为 TCP 半连接队列，这个队列都是没有完成三次握⼿的连接， 此时服务端处于 syn_rcvd 的状态；⼀个是⼀件建⽴连接的队列，称为 TCP 全连接队列，这个队列都是完成了三次握⼿的连接，此时服务端处于 established 状态； 当 TCP 全连接队列不为空后，服务端的 accept() 函数，就会从内核中的 TCP 全连接队列⾥拿出⼀个已经完成连接的 Socket 返回应⽤程序，后续数据传输都⽤这个 Socket。 （注意，监听的 Socket 和真正⽤来传数据的 Socket 是两个：⼀个叫作监听 Socket；⼀个叫作已连接 Socket；） 连接建⽴后，客户端和服务端就开始相互传输数据了，双⽅都可以通过 read() 和 write() 函数来读写数据。 基于 Linux ⼀切皆⽂件的理念，在内核中 Socket 也是以「⽂件」的形式存在的，也是有对应的⽂件描述符。 上面提到的TCP Socket 调⽤流程是最简单、最基本的，它基本只能⼀对⼀通信，因为使⽤的是同步阻塞的⽅式，当服务端在还没处理完⼀个客户端的⽹络 I/O 时，或者 读写操作发⽣阻塞时，其他客户端是⽆法与服务端连接的。可如果我们服务器只能服务⼀个客户，那这样就太浪费资源了，于是我们要改进这个⽹络 I/O 模型，以⽀持更多的客户端。 服务器作为服务⽅，通常会在本地固定监听⼀个端⼝，等待客户端的连接。因此服务器的本地 IP 和端⼝是固定的，于是对于服务端 TCP 连接的四元组只有对端 IP 和端⼝是会变化的，所以最⼤ TCP 连接数 = 客户端 IP 数×客户端端⼝数。 对于 IPv4，客户端的 IP 数最多为 2 的 32 次⽅，客户端的端⼝数最多为 2 的 16 次⽅，也就是服务端单机最⼤ TCP 连接数约为 2 的 48 次⽅。 这个理论值相当“丰满”，但是服务器肯定承载不了那么⼤的连接数，主要会受两个⽅⾯的限制： ⽂件描述符，Socket 实际上是⼀个⽂件，也就会对应⼀个⽂件描述符。在 Linux 下，单个进程打开的⽂件描述符数是有限制的，没有经过修改的值⼀般都是 1024，不过我们可以通过 ulimit 增⼤⽂件描述符的数⽬； 系统内存，每个 TCP 连接在内核中都有对应的数据结构，意味着每个连接都是会占⽤⼀定内存的； 基于最原始的阻塞⽹络 I/O， 如果服务器要⽀持多个客户端，其中⽐较传统的⽅式，就是使⽤多进程模型，也就是为每个客户端分配⼀个进程来处理请求。 服务器的主进程负责监听客户的连接，⼀旦与客户端连接完成，accept() 函数就会返回⼀个「已连接Socket」，这时就通过 fork() 函数创建⼀个⼦进程，实际上就把⽗进程所有相关的东⻄都复制⼀份，包括⽂件描述符、内存地址空间、程序计数器、执⾏的代码等。 这两个进程刚复制完的时候，⼏乎⼀摸⼀样。不过，会根据返回值来区分是⽗进程还是⼦进程，如果返回值是 0，则是⼦进程；如果返回值是其他的整数，就是⽗进程。 正因为⼦进程会复制⽗进程的⽂件描述符，于是就可以直接使⽤「已连接 Socket 」和客户端通信了，可以发现，⼦进程不需要关⼼「监听 Socket」，只需要关⼼「已连接 Socket」；⽗进程则相反，将客户服务交给⼦进程来处理，因此⽗进程不需要关⼼「已连接 Socket」，只需要关⼼「监听 Socket」。 另外，当「⼦进程」退出时，实际上内核⾥还会保留该进程的⼀些信息，也是会占⽤内存的，如果不做好“回收”⼯作，就会变成僵⼫进程，随着僵⼫进程越多，会慢慢耗尽我们的系统资源。 因此，⽗进程要“善后”好⾃⼰的孩⼦，怎么善后呢？那么有两种⽅式可以在⼦进程退出后回收资源，分别是调⽤ wait() 和 waitpid() 函数。 这种⽤多个进程来应付多个客户端的⽅式，在应对 100 个客户端还是可⾏的，但是当客户端数量⾼达⼀万时，肯定扛不住的，因为每产⽣⼀个进程，必会占据⼀定的系统资源，⽽且进程间上下⽂切换的“包袱”是很重的，性能会⼤打折扣。 进程的上下⽂切换不仅包含了虚拟内存、栈、全局变量等⽤户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。 既然进程间上下⽂切换的“包袱”很重，那我们就搞个⽐较轻量级的模型来应对多⽤户的请求 —— 多线程模型。 线程是运⾏在进程中的⼀个“逻辑流”，单进程中可以运⾏多个线程，同进程⾥的线程可以共享进程的部分资源的，⽐如⽂件描述符列表、进程空间、代码、全局数据、堆、共享库等，这些共享些资源在上下⽂切换时是不需要切换，⽽只需要切换线程的私有数据、寄存器等不共享的数据，因此同⼀个进程下的线程上下⽂切换的开销要⽐进程⼩得多。 当服务器与客户端 TCP 完成连接后，通过 pthread_create() 函数创建线程，然后将「已连接 Socket」的⽂件描述符传递给线程函数，接着在线程⾥和客户端进⾏通信，从⽽达到并发处理的⽬的。 如果每来⼀个连接就创建⼀个线程，线程运⾏完后，还得操作系统还得销毁线程，虽说线程切换的上写⽂开销不⼤，但是如果频繁创建和销毁线程，系统开销也是不⼩的。 那么，我们可以使⽤线程池的⽅式来避免线程的频繁创建和销毁，所谓的线程池，就是提前创建若⼲个线程，这样当由新连接建⽴时，将这个已连接的 Socket 放⼊到⼀个队列⾥，然后线程池⾥的线程负责从队列中取出已连接 Socket 进程处理。 需要注意的是，这个队列是全局的，每个线程都会操作，为了避免多线程竞争，线程在操作这个队列前要加锁。 上⾯基于进程或者线程模型的，其实还是有问题的。新到来⼀个 TCP 连接，就需要分配⼀个进程或者线程，那么如果要达到 C10K，意味着要⼀台机器维护 1 万个连接，相当于要维护 1 万个进程/线程，操作系统就算死扛也是扛不住的。 既然为每个请求分配⼀个进程/线程的⽅式不合适，那有没有可能只使⽤⼀个进程来维护多个 Socket 呢？答案是有的，那就是 I/O 多路复⽤技术。 ⼀个进程虽然任⼀时刻只能处理⼀个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1秒内就可以处理上千个请求，把时间拉⻓来看，多个请求复⽤了⼀个进程，这就是多路复⽤，这种思想很类似⼀个 CPU 并发多个进程，所以也叫做时分多路复⽤。 我们熟悉的 select/poll/epoll 内核提供给⽤户态的多路复⽤系统调⽤，进程可以通过⼀个系统调⽤函数从内核中获取多个事件。 select/poll/epoll 是如何获取⽹络事件的呢？在获取事件时，先把所有连接（⽂件描述符）传给内核，再由内核返回产⽣了事件的连接，然后在⽤户态中再处理这些连接对应的请求即可。 所以，对于 select 这种⽅式，需要进⾏ 2 次「遍历」⽂件描述符集合，⼀次是在内核态⾥，⼀个次是在⽤户态⾥ ，⽽且还会发⽣ 2 次「拷⻉」⽂件描述符集合，先从⽤户空间传⼊内核空间，由内核修改后，再传出到⽤户空间中。 select 使⽤固定⻓度的 BitsMap，表示⽂件描述符集合，⽽且所⽀持的⽂件描述符的个数是有限制的，在Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最⼤值为 1024 ，只能监听 0~1023 的⽂件描述符。 poll 不再⽤ BitsMap 来存储所关注的⽂件描述符，取⽽代之⽤动态数组，以链表形式来组织，突破了select 的⽂件描述符个数限制，当然还会受到系统⽂件描述符限制。 但是 poll 和 select 并没有太⼤的本质区别，都是使⽤「线性结构」存储进程关注的 Socket 集合，因此都需要遍历⽂件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，⽽且也需要在⽤户态与内核态之间拷⻉⽂件描述符集合，这种⽅式随着并发数上来，性能的损耗会呈指数级增⻓。 epoll 通过两个⽅⾯，很好解决了 select/poll 的问题。 第⼀点，epoll 在内核⾥使⽤红⿊树来跟踪进程所有待检测的⽂件描述字，把需要监控的 socket 通过epoll_ctl() 函数加⼊内核中的红⿊树⾥，红⿊树是个⾼效的数据结构，增删查⼀般时间复杂度是O(logn) ，通过对这棵⿊红树进⾏操作，这样就不需要像 select/poll 每次操作时都传⼊整个 socket 集合，只需要传⼊⼀个待检测的 socket，减少了内核和⽤户空间⼤量的数据拷⻉和内存分配。 第⼆点， epoll 使⽤事件驱动的机制，内核⾥维护了⼀个链表来记录就绪事件，当某个 socket 有事件发⽣时，通过回调函数内核会将其加⼊到这个就绪事件列表中，当⽤户调⽤ epoll_wait() 函数时，只会返回有事件发⽣的⽂件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，⼤⼤提⾼了检测的效率。 epoll 的⽅式即使监听的 Socket 数量越多的时候，效率不会⼤幅度降低，能够同时监听的 Socket 的数⽬也⾮常的多了，上限就为系统定义的进程打开的最⼤⽂件描述符个数。因⽽，epoll 被称为解决 C10K 问题的利器。 （注意：epoll_wait 返回时，对于就绪的事件，epoll使⽤的是共享内存的⽅式，即⽤户态和内核态都指向了就绪链表，所以就避免了内存拷⻉消耗。这是错的！看过 epoll 内核源码的都知道，压根就没有使⽤共享内存这个玩意。你可以从下⾯这份代码看到， epoll_wait 实现的内核代码中调⽤了 __put_user 函数，这个函数就是将数据从内核拷⻉到⽤户空间。） epoll ⽀持两种事件触发模式，分别是边缘触发（edge-triggered，ET）和⽔平触发（level-triggered，LT）。 使⽤边缘触发模式时，当被监控的 Socket 描述符上有可读事件发⽣时，服务器端只会从 epoll_wait中苏醒⼀次，即使进程没有调⽤ read 函数从内核读取数据，也依然只苏醒⼀次，因此我们程序要保证⼀次性将内核缓冲区的数据读取完； 使⽤⽔平触发模式时，当被监控的 Socket 上有可读事件发⽣时，服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束，⽬的是告诉我们有数据需要读取； 举个例⼦，你的快递被放到了⼀个快递箱⾥，如果快递箱只会通过短信通知你⼀次，即使你⼀直没有去取，它也不会再发送第⼆条短信提醒你，这个⽅式就是边缘触发；如果快递箱发现你的快递没有被取出，它就会不停地发短信通知你，直到你取出了快递，它才消停，这个就是⽔平触发的⽅式。 这就是两者的区别，⽔平触发的意思是只要满⾜事件的条件，⽐如内核中有数据需要读，就⼀直不断地把这个事件传递给⽤户；⽽边缘触发的意思是只有第⼀次满⾜条件的时候才触发，之后就不会再传递同样的事件了。 如果使⽤⽔平触发模式，当内核通知⽂件描述符可读写时，接下来还可以继续去检测它的状态，看它是否依然可读或可写。所以在收到通知后，没必要⼀次执⾏尽可能多的读写操作。 如果使⽤边缘触发模式，I/O 事件发⽣时只会通知⼀次，⽽且我们不知道到底能读写多少数据，所以在收到通知后应尽可能地读写数据，以免错失读写的机会。因此，我们会循环从⽂件描述符读写数据，那么如果⽂件描述符是阻塞的，没有数据可读写时，进程会阻塞在读写函数那⾥，程序就没办法继续往下执⾏。所以，边缘触发模式⼀般和⾮阻塞 I/O 搭配使⽤，程序会⼀直执⾏ I/O 操作，直到系统调⽤（如 read 和write ）返回错误，错误类型为 EAGAIN 或 EWOULDBLOCK 。 ⼀般来说，边缘触发的效率⽐⽔平触发的效率要⾼，因为边缘触发可以减少 epoll_wait 的系统调⽤次数，系统调⽤也是有⼀定的开销的的，毕竟也存在上下⽂的切换。 select/poll 只有⽔平触发模式，epoll 默认的触发模式是⽔平触发，但是可以根据应⽤场景设置为边缘触发模式。 另外，使⽤ I/O 多路复⽤时，最好搭配⾮阻塞 I/O ⼀起使⽤，简单点理解，就是多路复⽤ API 返回的事件并不⼀定可读写的，如果使⽤阻塞 I/O， 那么在调⽤read/write 时则会发⽣程序阻塞，因此最好搭配⾮阻塞 I/O，以便应对极少数的特殊情况。 最基础的 TCP 的 Socket 编程，它是阻塞 I/O 模型，基本上只能⼀对⼀通信，那为了服务更多的客户端，我们需要改进⽹络 I/O 模型。 ⽐较传统的⽅式是使⽤多进程/线程模型，每来⼀个客户端连接，就分配⼀个进程/线程，然后后续的读写都在对应的进程/线程，这种⽅式处理 100 个客户端没问题，但是当客户端增⼤到 10000 个时，10000 个进程/线程的调度、上下⽂切换以及它们占⽤的内存，都会成为瓶颈。 为了解决上⾯这个问题，就出现了 I/O 的多路复⽤，可以只在⼀个进程⾥处理多个⽂件的 I/O，Linux 下有三种提供 I/O 多路复⽤的 API，分别是： select、poll、epoll。 select 和 poll 并没有本质区别，它们内部都是使⽤「线性结构」来存储进程关注的 Socket 集合。在使⽤的时候，⾸先需要把关注的 Socket 集合通过 select/poll 系统调⽤从⽤户态拷⻉到内核态，然后由内核检测事件，当有⽹络事件产⽣时，内核需要遍历进程关注 Socket 集合，找到对应的 Socket，并设置其状态为可读/可写，然后把整个 Socket 集合从内核态拷⻉到⽤户态，⽤户态还要继续遍历整个 Socket 集合找到可读/可写的 Socket，然后对其处理。 很明显发现，select 和 poll 的缺陷在于，当客户端越多，也就是 Socket 集合越⼤，Socket 集合的遍历和拷⻉会带来很⼤的开销，因此也很难应对 C10K。 epoll 是解决 C10K 问题的利器，通过两个⽅⾯解决了 select/poll 的问题。 epoll 在内核⾥使⽤「红⿊树」来关注进程所有待检测的 Socket，红⿊树是个⾼效的数据结构，增删查⼀般时间复杂度是 O(logn)，通过对这棵⿊红树的管理，不需要像 select/poll 在每次操作时都传⼊整个 Socket 集合，减少了内核和⽤户空间⼤量的数据拷⻉和内存分配。 epoll 使⽤事件驱动的机制，内核⾥维护了⼀个「链表」来记录就绪事件，只将有事件发⽣的 Socket集合传递给应⽤程序，不需要像 select/poll 那样轮询扫描整个集合（包含有和⽆事件的 Socket ），⼤⼤提⾼了检测的效率。 ⽽且，epoll ⽀持边缘触发和⽔平触发的⽅式，⽽ select/poll 只⽀持⽔平触发，⼀般⽽⾔，边缘触发的⽅式会⽐⽔平触发的效率⾼。","categories":[{"name":"I/O多路复用","slug":"I-O多路复用","permalink":"http://example.com/categories/I-O%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"},{"name":"I/O多路复用","slug":"I-O多路复用","permalink":"http://example.com/tags/I-O%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/"},{"name":"select/epoll/poll","slug":"select-epoll-poll","permalink":"http://example.com/tags/select-epoll-poll/"}],"author":"John Doe"},{"title":"Kafka-Kraft 模式","slug":"Kafka-Kraft-模式","date":"2022-03-12T05:30:00.000Z","updated":"2022-03-12T05:32:23.547Z","comments":true,"path":"2022/03/12/Kafka-Kraft-模式/","link":"","permalink":"http://example.com/2022/03/12/Kafka-Kraft-%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"左图为 Kafka 现有架构，元数据在 zookeeper 中，运行时动态选举 controller，由controller 进行 Kafka 集群管理。右图为 kraft 模式架构（实验性），不再依赖 zookeeper 集群，而是用三台 controller 节点代替 zookeeper，元数据保存在 controller 中，由 controller 直接进行 Kafka 集群管理。 这样做的好处有以下几个： ⚫ Kafka 不再依赖外部框架，而是能够独立运行； ⚫ controller 管理集群时，不再需要从 zookeeper 中先读取数据，集群性能上升； ⚫ 由于不依赖 zookeeper，集群扩展时不再受到 zookeeper 读写能力限制； ⚫ controller 不再动态选举，而是由配置文件规定。这样我们可以有针对性的加强controller 节点的配置，而不是像以前一样对随机 controller 节点的高负载束手无策。","categories":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/categories/Kafka/"}],"tags":[{"name":"Kraft模式","slug":"Kraft模式","permalink":"http://example.com/tags/Kraft%E6%A8%A1%E5%BC%8F/"}],"author":"John Doe"},{"title":"Kafka数据积压（消费者如何提高吞吐量）","slug":"Kafka数据积压（消费者如何提高吞吐量）","date":"2022-03-12T05:28:00.000Z","updated":"2022-03-12T05:29:37.815Z","comments":true,"path":"2022/03/12/Kafka数据积压（消费者如何提高吞吐量）/","link":"","permalink":"http://example.com/2022/03/12/Kafka%E6%95%B0%E6%8D%AE%E7%A7%AF%E5%8E%8B%EF%BC%88%E6%B6%88%E8%B4%B9%E8%80%85%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98%E5%90%9E%E5%90%90%E9%87%8F%EF%BC%89/","excerpt":"","text":"1）如果是Kafka消费能力不足，则可以考虑增 加Topic的分区数，并且同时提升消费组的消费者数量，消费者数 = 分区数。（两者缺一不可） 2）如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取数据/处理时间 &lt; 生产速度），使处理的数据小于生产的数据，也会造成数据积压。","categories":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/categories/Kafka/"}],"tags":[{"name":"数据积压","slug":"数据积压","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E7%A7%AF%E5%8E%8B/"},{"name":"消费者","slug":"消费者","permalink":"http://example.com/tags/%E6%B6%88%E8%B4%B9%E8%80%85/"}],"author":"John Doe"},{"title":"Kafka消费者漏消费和重复消费问题","slug":"Kafka消费者漏消费和重复消费问题","date":"2022-03-12T05:12:00.000Z","updated":"2022-03-12T05:27:39.970Z","comments":true,"path":"2022/03/12/Kafka消费者漏消费和重复消费问题/","link":"","permalink":"http://example.com/2022/03/12/Kafka%E6%B6%88%E8%B4%B9%E8%80%85%E6%BC%8F%E6%B6%88%E8%B4%B9%E5%92%8C%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9%E9%97%AE%E9%A2%98/","excerpt":"","text":"重复消费：已经消费了数据，但是 offset 没提交，下次还会消费到当前数据。 漏消费：先提交 offset 后消费，有可能会造成数据的漏消费。 如果想完成Consumer端的精准一次性消费（既不漏消费也不重复消费），那么需要Kafka消费端将消费过程和提交offset过程做原子绑定。此时我们需要将Kafka的offset保存到支持事务的自定义介质（比 如MySQL）。 参考：https://blog.csdn.net/qingqing7/article/details/80054281?spm=1001.2101.3001.6650.14&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-14.pc_relevant_default&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-14.pc_relevant_default&amp;utm_relevant_index=25","categories":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/categories/Kafka/"}],"tags":[{"name":"消费者","slug":"消费者","permalink":"http://example.com/tags/%E6%B6%88%E8%B4%B9%E8%80%85/"},{"name":"漏消费","slug":"漏消费","permalink":"http://example.com/tags/%E6%BC%8F%E6%B6%88%E8%B4%B9/"},{"name":"重复消费","slug":"重复消费","permalink":"http://example.com/tags/%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9/"}],"author":"John Doe"},{"title":"Kafka消费者的offset提交","slug":"Kafka消费者的offset提交","date":"2022-03-12T05:00:00.000Z","updated":"2022-03-12T05:11:59.292Z","comments":true,"path":"2022/03/12/Kafka消费者的offset提交/","link":"","permalink":"http://example.com/2022/03/12/Kafka%E6%B6%88%E8%B4%B9%E8%80%85%E7%9A%84offset%E6%8F%90%E4%BA%A4/","excerpt":"","text":"offset偏移量表明了该消费者当前消费的数据到哪一步，其存储在系统主题_consumer_offset中（0.9版本之前是存在Zookeeper中），以key,value形式，每隔一段时间kafka都会对其Compact（即保留当前最新的数据）。 1、自动提交offset：为了能让我们专注于业务处理，Kafka提供了自动提交offset功能，通过参数 ⚫ enable.auto.commit：是否开启自动提交offset功能，默认是true ⚫ auto.commit.interval.ms：自动提交offset的时间间隔，默认是5s 2、手动提交：自动提交固然遍历，但基于时间的提交，我们很难把握那个度，因此更多时候，我们可以选择手动提交。 1）同步提交：同步提交会阻塞当前线程，一直到成功为止，并且失败会自动重试 2）异步提交：异步提交则不会阻塞当前线程，且没有重试机制，可能提交失败。 两者都会将本次提交的一批数据最高偏移量提交。 指定offset消费：auto.offset.reset = earliest | latest | none 默认是 latest。 当kafka中没有初始偏移量（消费者组第一次消费）或服务器上不存在当前偏移量时（数据被删除）需要指定offset消费。 1）earliest：自动将偏移量重置为最早的偏移量，–from-beginning。 2）latest（默认值）：自动将偏移量重置为最新偏移量。 （3）none：如果未找到消费者组的先前偏移量，则向消费者抛出异常。 （4）任意指定 offset 位移开始消费 指定时间消费：在生产环境中，会遇到最近消费的几个小时数据异常，想重新按照时间消费。例如要求按照时间消费前一天的数据","categories":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/categories/Kafka/"}],"tags":[{"name":"消费者","slug":"消费者","permalink":"http://example.com/tags/%E6%B6%88%E8%B4%B9%E8%80%85/"},{"name":"offset","slug":"offset","permalink":"http://example.com/tags/offset/"}],"author":"John Doe"},{"title":"Kafka消费者分区的分配以及再平衡","slug":"Kafka消费者分区的分配以及再平衡","date":"2022-03-12T04:47:00.000Z","updated":"2022-03-12T04:53:38.116Z","comments":true,"path":"2022/03/12/Kafka消费者分区的分配以及再平衡/","link":"","permalink":"http://example.com/2022/03/12/Kafka%E6%B6%88%E8%B4%B9%E8%80%85%E5%88%86%E5%8C%BA%E7%9A%84%E5%88%86%E9%85%8D%E4%BB%A5%E5%8F%8A%E5%86%8D%E5%B9%B3%E8%A1%A1/","excerpt":"","text":"一个consumer group中有多个consumer组成，一个 topic有多个partition组成，现在的问题是，到底由哪个consumer来消费哪个partition的数据。 2、Kafka有四种主流的分区分配策略： Range、RoundRobin、Sticky、CooperativeSticky。可以通过配置参数partition.assignment.strategy，修改分区的分配策略。默认策略是Range + CooperativeSticky。Kafka可以同时使用多个分区分配策略。 1）Range 是对每个 topic 而言的。 首先对同一个 topic 里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。 假如现在有 7 个分区，3 个消费者，排序后的分区将会是0,1,2,3,4,5,6；消费者排序完之后将会是C0,C1,C2。例如，7/3 = 2 余 1 ，除不尽，那么 消费者 C0 便会多消费 1 个分区。 8/3=2余2，除不尽，那么C0和C1分别多消费一个。 通过 partitions数/consumer数 来决定每个消费者应该消费几个分区。如果除不尽，那么前面几个消费者将会多消费 1 个分区。 注意：如果只是针对 1 个 topic 而言，C0消费者多消费1个分区影响不是很大。但是如果有 N 多个 topic，那么针对个 topic，消费者 C0都将多消费 1 个分区，topic越多，C0消 费的分区会比其他消费者明显多消费 N 个分区。容易产生数据倾斜！ （注意：说明：某个消费者挂掉后，消费者组需要按照超时时间 45s 来判断它是否退出，所以需要等待，时间到了 45s 后，判断它真的退出就会把任务分配给其他 broker 执行。） 2）RoundRobin 分区策略原理： RoundRobin 针对集群中所有Topic而言。RoundRobin 轮询分区策略，是把所有的 partition 和所有的consumer 都列出来，然后按照 hashcode 进行排序，最后通过轮询算法来分配 partition 给到各个消费者。 3） Sticky 以及再平衡： 粘性分区定义：可以理解为分配的结果带有“粘性的”。即在执行一次新的分配之前，考虑上一次分配的结果，尽量少的调整分配的变动，可以节省大量的开销。粘性分区是 Kafka 从 0.11.x 版本开始引入这种分配策略，首先会尽量均衡的放置分区到消费者上面，在出现同一消费者组内消费者出现问题的时候，会尽量保持原有分配的分区不变化。","categories":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/categories/Kafka/"}],"tags":[{"name":"消费者","slug":"消费者","permalink":"http://example.com/tags/%E6%B6%88%E8%B4%B9%E8%80%85/"},{"name":"分区分配策略","slug":"分区分配策略","permalink":"http://example.com/tags/%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/"}],"author":"John Doe"},{"title":"Kafka高效读写数据","slug":"Kafka高效读写数据","date":"2022-03-12T03:23:00.000Z","updated":"2022-03-12T03:24:21.616Z","comments":true,"path":"2022/03/12/Kafka高效读写数据/","link":"","permalink":"http://example.com/2022/03/12/Kafka%E9%AB%98%E6%95%88%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE/","excerpt":"","text":"1）Kafka 本身是分布式集群，可以采用分区技术，并行度高 2）读数据采用稀疏索引，可以快速定位要消费的数据 3）顺序写磁盘（Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到 600M/s，而随机写只有 100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。） 4）页缓存 + 零拷贝技术","categories":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/categories/Kafka/"}],"tags":[{"name":"高效读写","slug":"高效读写","permalink":"http://example.com/tags/%E9%AB%98%E6%95%88%E8%AF%BB%E5%86%99/"}],"author":"John Doe"},{"title":"Kafka文件清理策略","slug":"Kafka文件清理策略","date":"2022-03-12T03:19:00.000Z","updated":"2022-03-12T03:23:07.944Z","comments":true,"path":"2022/03/12/Kafka文件清理策略/","link":"","permalink":"http://example.com/2022/03/12/Kafka%E6%96%87%E4%BB%B6%E6%B8%85%E7%90%86%E7%AD%96%E7%95%A5/","excerpt":"","text":"Kafka 中默认的日志保存时间为 7 天，可以通过调整如下参数修改保存时间。⚫ log.retention.hours，最低优先级小时，默认 7 天。 ⚫ log.retention.minutes，分钟。 ⚫ log.retention.ms，最高优先级毫秒。 ⚫log.retention.check.interval.ms，负责设置检查周期，默认 5 分钟。 对于超过设置事件的数据，有两种清楚策略，delete和Compact 1）delete 日志删除：将过期数据删除 ⚫ log.cleanup.policy = delete 所有数据启用删除策略 （1）基于时间：默认打开。以 segment 中所有记录中的最大时间戳作为该文件时间戳。 （2）基于大小：默认关闭。超过设置的所有日志总大小，删除最早segment。log.retention.bytes，默认等于-1，表示无穷大。 2）compact 日志压缩 compact日志压缩：对于相同key的不同value值，只保留最后一个版本。 ⚫ log.cleanup.policy = compact 所有数据启用压缩策略 压缩后的offset可能是不连续的，比如上图中没有6，当从这些offset消费消息时，将会拿到比这个offset大 的offset对应的消息，实际上会拿到offset为7的消息，并从这个位置开始消费。 这种策略只适合特殊场景，比如消息的key是用户ID，value是用户的资料，通过这种压缩策略，整个消息集里就保存了所有用户最新的资料。","categories":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/categories/Kafka/"}],"tags":[{"name":"清楚策略","slug":"清楚策略","permalink":"http://example.com/tags/%E6%B8%85%E6%A5%9A%E7%AD%96%E7%95%A5/"}],"author":"John Doe"},{"title":"Kafka文件存储机制","slug":"Kafka文件存储机制","date":"2022-03-12T03:16:00.000Z","updated":"2022-03-12T03:19:20.616Z","comments":true,"path":"2022/03/12/Kafka文件存储机制/","link":"","permalink":"http://example.com/2022/03/12/Kafka%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6/","excerpt":"","text":"Topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是Producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引机制， 将每个partition分为多个segment。每个segment包括：“.index”文件、“.log”文件和.timeindex等文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号，例如：first-0。 Log文件和Index文件详解：","categories":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/categories/Kafka/"}],"tags":[{"name":"文件存储","slug":"文件存储","permalink":"http://example.com/tags/%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"}],"author":"John Doe"},{"title":"Leader Partition 负载平衡","slug":"Leader-Partition-负载平衡","date":"2022-03-12T03:11:00.000Z","updated":"2022-03-12T03:15:02.258Z","comments":true,"path":"2022/03/12/Leader-Partition-负载平衡/","link":"","permalink":"http://example.com/2022/03/12/Leader-Partition-%E8%B4%9F%E8%BD%BD%E5%B9%B3%E8%A1%A1/","excerpt":"","text":"正常情况下，Kafka本身会自动把Leader Partition均匀分散在各个机器上，来保证每台机器的读写吞吐量都是均匀的。但是如果某些broker宕机，会导致Leader Partition过于集中在其他少部分几台broker上，这会导致少数几台broker的读写请求压力过高，其他宕机的broker重启之后都是follower partition，读写请求很低，造成集群负载不均衡。 策略： 1、auto.leader.rebalance.enable，默认是true。（自动Leader Partition 平衡） 2、leader.imbalance.per.broker.percentage，默认是10%。每个broker允许的不平衡的leader的比率。如果每个broker超过了这个值，控制器会触发leader的平衡。 3、leader.imbalance.check.interval.seconds，默认值300秒。检查leader负载是否平衡的间隔时间。 例如：针对broker0节点，分区2的AR优先副本是0节点，但是0节点却不是Leader节点，所以不平衡数加1，AR副本总数是4，所以broker0节点不平衡率为1/4&gt;10%，需要再平衡。","categories":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/categories/Kafka/"}],"tags":[{"name":"Leader","slug":"Leader","permalink":"http://example.com/tags/Leader/"},{"name":"Partition","slug":"Partition","permalink":"http://example.com/tags/Partition/"}],"author":"John Doe"},{"title":"Leader 和 Follower 故障处理细节","slug":"Leader-和-Follower-故障处理细节","date":"2022-03-12T03:06:00.000Z","updated":"2022-03-12T03:10:07.966Z","comments":true,"path":"2022/03/12/Leader-和-Follower-故障处理细节/","link":"","permalink":"http://example.com/2022/03/12/Leader-%E5%92%8C-Follower-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E7%BB%86%E8%8A%82/","excerpt":"","text":"LEO（Log End Offset）：每个副本的最后一个offset，LEO其实就是最新的offset + 1。 HW（High Watermark）：所有副本中最小的LEO 。 1）Follower故障： （1） Follower发生故障后会被临时踢出ISR （2） 这个期间Leader和Follower继续接收数据 （3）待该Follower恢复后，Follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向Leader进行同步。 （4）等该Follower的LEO大于等于该Partition的HW，即Follower追上Leader之后，就可以重新加入ISR了。 2）Leader故障： （1） Leader发生故障之后，会从ISR中选出一个新的Leader （2）为保证多个副本之间的数据一致性，其余的Follower会先将各自的log文件高于HW的部分截掉，然后从新的Leader同步数据。 注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。","categories":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/categories/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"},{"name":"Leader和Follower故障","slug":"Leader和Follower故障","permalink":"http://example.com/tags/Leader%E5%92%8CFollower%E6%95%85%E9%9A%9C/"}],"author":"John Doe"},{"title":"Kafka Broker总体工作流程","slug":"Kafka-Broker总体工作流程","date":"2022-03-12T03:00:00.000Z","updated":"2022-03-12T03:00:21.303Z","comments":true,"path":"2022/03/12/Kafka-Broker总体工作流程/","link":"","permalink":"http://example.com/2022/03/12/Kafka-Broker%E6%80%BB%E4%BD%93%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/","excerpt":"","text":"","categories":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/categories/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"},{"name":"工作流程","slug":"工作流程","permalink":"http://example.com/tags/%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/"}],"author":"John Doe"},{"title":"Zookeeper中存储的Kafka 信息","slug":"Zookeeper中存储的Kafka-信息","date":"2022-03-12T02:52:00.000Z","updated":"2022-03-12T02:53:19.813Z","comments":true,"path":"2022/03/12/Zookeeper中存储的Kafka-信息/","link":"","permalink":"http://example.com/2022/03/12/Zookeeper%E4%B8%AD%E5%AD%98%E5%82%A8%E7%9A%84Kafka-%E4%BF%A1%E6%81%AF/","excerpt":"","text":"在zookeeper的服务端存储的Kafka相关信息： 1）/kafka/brokers/ids [0,1,2] 记录有哪些服务器 2）/kafka/brokers/topics/first/partitions/0/state{“leader”:1 ,”isr”:[1,0,2] } 记录谁是Leader，有哪些服务器可用 3）/kafka/controller{“brokerid”:0}辅助选举Leader","categories":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/categories/Kafka/"},{"name":"Zookeeper","slug":"Kafka/Zookeeper","permalink":"http://example.com/categories/Kafka/Zookeeper/"}],"tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"http://example.com/tags/Zookeeper/"}],"author":"John Doe"},{"title":"Kafka数据乱序","slug":"Kafka数据乱序","date":"2022-03-12T02:47:00.000Z","updated":"2022-03-12T02:50:26.550Z","comments":true,"path":"2022/03/12/Kafka数据乱序/","link":"","permalink":"http://example.com/2022/03/12/Kafka%E6%95%B0%E6%8D%AE%E4%B9%B1%E5%BA%8F/","excerpt":"","text":"1）kafka在1.x版本之前保证数据单分区有序，条件如下：max.in.flight.requests.per.connection=1（不需要考虑是否开启幂等性）。 2）kafka在1.x及以后版本保证数据单分区有序，条件如下： （1）未开启幂等性max.in.flight.requests.per.connection需要设置为1。 （2）开启幂等性max.in.flight.requests.per.connection需要设置小于等于5。 原因说明：因为在kafka1.x以后，启用幂等后，kafka服务端会缓存producer发来的最近5个request的元数据，故无论如何，都可以保证最近5个request的数据都是有序的。","categories":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/categories/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"},{"name":"数据有序","slug":"数据有序","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E6%9C%89%E5%BA%8F/"}],"author":"John Doe"},{"title":"Kafka的生产者事务原理","slug":"Kafka的生产者事务原理","date":"2022-03-12T02:29:00.000Z","updated":"2022-03-12T02:33:52.261Z","comments":true,"path":"2022/03/12/Kafka的生产者事务原理/","link":"","permalink":"http://example.com/2022/03/12/Kafka%E7%9A%84%E7%94%9F%E4%BA%A7%E8%80%85%E4%BA%8B%E5%8A%A1%E5%8E%9F%E7%90%86/","excerpt":"","text":"注意：开启事务，必须要开启幂等性。另外Procuder在使用事务功能前，必须先自定义一个唯一的transaction.id。有了transaction.id，即使客户端挂掉了，它重启后也能继续处理未完成的事务。","categories":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/categories/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"},{"name":"事务","slug":"事务","permalink":"http://example.com/tags/%E4%BA%8B%E5%8A%A1/"}],"author":"John Doe"},{"title":"Kafka保证生产者生产的数据不重复：幂等性+至少一次","slug":"Kafka保证生产者生产的数据不重复：幂等性-至少一次","date":"2022-03-12T02:25:00.000Z","updated":"2022-03-12T02:34:13.739Z","comments":true,"path":"2022/03/12/Kafka保证生产者生产的数据不重复：幂等性-至少一次/","link":"","permalink":"http://example.com/2022/03/12/Kafka%E4%BF%9D%E8%AF%81%E7%94%9F%E4%BA%A7%E8%80%85%E7%94%9F%E4%BA%A7%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%8D%E9%87%8D%E5%A4%8D%EF%BC%9A%E5%B9%82%E7%AD%89%E6%80%A7-%E8%87%B3%E5%B0%91%E4%B8%80%E6%AC%A1/","excerpt":"","text":"至少一次：ack级别设置为-1+分区副本大于等于2+ISR里面的应答最小副本大于等于2（保证数据不会丢失） 幂等性：指Producer不论向Broker发送多少次重复数据，Broker端都只会持久化一条，保证了不重复。（重复数据的判断标准：具有&lt;PID, Partition, SeqNumber&gt;相同主键的消息提交时，Broker只会持久化一条。其 中PID是Kafka每次重启都会分配一个新的；Partition 表示分区号；Sequence Number是单调自增的。） 因此幂等性只能保证的是在单分区单会话内不重复。 如何使用幂等性：开启参数 enable.idempotence 默认为 true，false 关闭。","categories":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/categories/Kafka/"},{"name":"消息队列","slug":"Kafka/消息队列","permalink":"http://example.com/categories/Kafka/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"},{"name":"数据","slug":"数据","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE/"}],"author":"John Doe"},{"title":"Spring Security认证过程","slug":"Spring-Security认证过程","date":"2022-03-11T13:54:00.000Z","updated":"2022-03-11T14:09:54.174Z","comments":true,"path":"2022/03/11/Spring-Security认证过程/","link":"","permalink":"http://example.com/2022/03/11/Spring-Security%E8%AE%A4%E8%AF%81%E8%BF%87%E7%A8%8B/","excerpt":"","text":"我们知道Spring Security的核心就是认证和授权，但是具体它是如何进行认证和授权的呢？下面让我们来聊聊Spring Security的认证过程，具体步骤如下图所示： 在开始之前，我们需要了解一下如下类： AuthenticationManager核心验证器，该对象提供了认证方法的入口，接收一个Authentiation对象作为参数。 public interface AuthenticationManager &#123; Authentication authenticate(Authentication authentication) throws AuthenticationException; &#125; ProviderManager：它是 AuthenticationManager 的一个实现类，提供了基本的认证逻辑和方法；它包含了一个 List 对象，通过 AuthenticationProvider 接口来扩展出不同的认证提供者(当Spring Security默认提供的实现类不能满足需求的时候可以扩展AuthenticationProvider 覆盖supports(Class&lt;?&gt; authentication)方法)； 具体验证逻辑： AuthenticationManager 接收 Authentication 对象作为参数，并通过 authenticate(Authentication) 方法对其进行验证；AuthenticationProvider实现类用来支撑对 Authentication 对象的验证动作；UsernamePasswordAuthenticationToken实现了 Authentication主要是将用户输入的用户名和密码进行封装，并供给 AuthenticationManager 进行验证；验证完成以后将返回一个认证成功的 Authentication 对象；","categories":[{"name":"Spring Security","slug":"Spring-Security","permalink":"http://example.com/categories/Spring-Security/"}],"tags":[{"name":"认证","slug":"认证","permalink":"http://example.com/tags/%E8%AE%A4%E8%AF%81/"}],"author":"John Doe"},{"title":"Spring的AOP是在哪个阶段创建的动态代理？","slug":"Spring的AOP是在哪个阶段创建的动态代理？","date":"2022-03-10T00:44:00.000Z","updated":"2022-03-10T00:47:08.519Z","comments":true,"path":"2022/03/10/Spring的AOP是在哪个阶段创建的动态代理？/","link":"","permalink":"http://example.com/2022/03/10/Spring%E7%9A%84AOP%E6%98%AF%E5%9C%A8%E5%93%AA%E4%B8%AA%E9%98%B6%E6%AE%B5%E5%88%9B%E5%BB%BA%E7%9A%84%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%EF%BC%9F/","excerpt":"","text":"1、正常情况下会在bean的生命周期“初始化”后，通过BeanPostProcessor.postProcessAfterInitialization创建AOP的动态代理 2、特殊情况下，即存在循环依赖的时候，Bean会在生命周期的“属性注入”时，通过MergedBeanDefinitionPostProcessor.postProcessMergedBeanDefinition创建aop动态代理","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"AOP","slug":"AOP","permalink":"http://example.com/tags/AOP/"},{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"author":"John Doe"},{"title":"什么情况下AOP会失效,怎么解决？","slug":"什么情况下AOP会失效-怎么解决？","date":"2022-03-10T00:37:00.000Z","updated":"2022-03-10T00:42:55.549Z","comments":true,"path":"2022/03/10/什么情况下AOP会失效-怎么解决？/","link":"","permalink":"http://example.com/2022/03/10/%E4%BB%80%E4%B9%88%E6%83%85%E5%86%B5%E4%B8%8BAOP%E4%BC%9A%E5%A4%B1%E6%95%88-%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%EF%BC%9F/","excerpt":"","text":"1、方法是private 2、目标类没有配置为Bean 3、切点表达式没有写正确 4、jdk动态代理下内部调用不会触发AOP（ 原因： 内部进行自调用，是走的实例对象，而不是代理对象。 解决： 1、在本类中自动注入当前的bean 2、@EnableAspectJAutoProxy(exposProxy = true) 设置暴露当前代理对象到本地线程，可以通过AopContent.currentProxy()拿到当前的动态代理对象。）","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"AOP","slug":"AOP","permalink":"http://example.com/tags/AOP/"},{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"author":"John Doe"},{"title":"AOP有几种实现方式 ","slug":"AOP有几种实现方式","date":"2022-03-10T00:34:00.000Z","updated":"2022-03-10T00:36:27.821Z","comments":true,"path":"2022/03/10/AOP有几种实现方式/","link":"","permalink":"http://example.com/2022/03/10/AOP%E6%9C%89%E5%87%A0%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F/","excerpt":"","text":"1、Spring 1.2 基于接口的配置：最早的 Spring AOP 是完全基于几个接口的，想看源码的同学可以从这里起步。 2、Spring 2.0 schema-based 配置：Spring 2.0 以后使用 XML 的方式来配置，使用 命名空间 3、Spring 2.0 @AspectJ 配置：使用注解的方式来配置，这种方式感觉是最方便的，还有，这里虽然叫做 @AspectJ，但是这个和 AspectJ 其实没啥关系。 4、AspectJ 方式，这种方式其实和Spring没有关系，采用AspectJ 进行动态织入的方式实现AOP，需要用AspectJ 单独编译。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"AOP","slug":"AOP","permalink":"http://example.com/tags/AOP/"}],"author":"John Doe"},{"title":"Spring的AOP通知执行顺序","slug":"Spring的AOP通知执行顺序","date":"2022-03-09T13:25:00.000Z","updated":"2022-03-09T13:28:17.294Z","comments":true,"path":"2022/03/09/Spring的AOP通知执行顺序/","link":"","permalink":"http://example.com/2022/03/09/Spring%E7%9A%84AOP%E9%80%9A%E7%9F%A5%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/","excerpt":"","text":"执行顺序： 5.2.7之前： 1、正常执行：@Before­­­&gt;方法­­­­&gt;@After­­­&gt;@AfterReturning 2、异常执行：@Before­­­&gt;方法­­­­&gt;@After­­­&gt;@AfterThrowing 5.2.7之后： 1、正常执行：@Before­­­&gt;方法­­­­&gt;@AfterReturning­­­&gt;@After 2、异常执行：@Before­­­&gt;方法­­­­&gt;@AfterThrowing­­­&gt;@After","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"AOP","slug":"AOP","permalink":"http://example.com/tags/AOP/"},{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"author":"John Doe"},{"title":"说说@Import可以有几种用法？","slug":"说说-Import可以有几种用法？","date":"2022-03-09T10:41:00.000Z","updated":"2022-03-09T10:43:33.973Z","comments":true,"path":"2022/03/09/说说-Import可以有几种用法？/","link":"","permalink":"http://example.com/2022/03/09/%E8%AF%B4%E8%AF%B4-Import%E5%8F%AF%E4%BB%A5%E6%9C%89%E5%87%A0%E7%A7%8D%E7%94%A8%E6%B3%95%EF%BC%9F/","excerpt":"","text":"1、 直接指定类 （如果配置类会按配置类正常解析、 如果是个普通类就会解析成Bean) 2、 通过ImportSelector 可以一次性注册多个，返回一个string[] 每一个值就是类的完整类路径 3、 通过DeferredImportSelector可以一次性注册多个，返回一个string[] 每一个值就是类的完整类路径 区别：DeferredImportSelector 顺序靠后 4、 通过ImportBeanDefinitionRegistrar 可以一次性注册多个，通过BeanDefinitionRegistry来动态注册BeanDefintion","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"author":"John Doe"},{"title":"如何让自动注入找到多个依赖Bean时不报错","slug":"如何让自动注入找到多个依赖Bean时不报错","date":"2022-03-09T10:36:00.000Z","updated":"2022-03-09T10:39:06.070Z","comments":true,"path":"2022/03/09/如何让自动注入找到多个依赖Bean时不报错/","link":"","permalink":"http://example.com/2022/03/09/%E5%A6%82%E4%BD%95%E8%AE%A9%E8%87%AA%E5%8A%A8%E6%B3%A8%E5%85%A5%E6%89%BE%E5%88%B0%E5%A4%9A%E4%B8%AA%E4%BE%9D%E8%B5%96Bean%E6%97%B6%E4%B8%8D%E6%8A%A5%E9%94%99/","excerpt":"","text":"自动注入找到多个依赖Bean时，@primary可以指定注入哪一个。 @Primary：自动装配时当出现多个Bean候选者时，被注解为@Primary的Bean将作为首选者，否则将抛出异常 @Autowired 默认按类型装配，如果我们想使用按名称装配，可以结合@Qualifier注解一起使用 @Autowired @Qualifier(“personDaoBean”) 存在多个实例配合使用","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"author":"John Doe"},{"title":"说一说@Autowired和@Resource之间的区别","slug":"说一说-Autowired和-Resource之间的区别","date":"2022-03-09T10:31:00.000Z","updated":"2022-03-09T10:35:41.053Z","comments":true,"path":"2022/03/09/说一说-Autowired和-Resource之间的区别/","link":"","permalink":"http://example.com/2022/03/09/%E8%AF%B4%E4%B8%80%E8%AF%B4-Autowired%E5%92%8C-Resource%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"@Autowired可用于：构造函数、成员变量、Setter方法 @Autowired和@Resource之间的区别 @Autowired默认是按照类型装配注入的（按照名称匹配需要@Qualifier），默认情况下它要求依赖对象必须存在（可以设置它required属性为false）。 @Resource默认是按照名称来装配注入的，只有当找不到与名称匹配的bean才会按照类型来装配注入。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"author":"John Doe"},{"title":"使用@Autowired注解自动装配的过程是怎样的？","slug":"使用-Autowired注解自动装配的过程是怎样的？","date":"2022-03-09T10:11:00.000Z","updated":"2022-03-09T10:20:50.684Z","comments":true,"path":"2022/03/09/使用-Autowired注解自动装配的过程是怎样的？/","link":"","permalink":"http://example.com/2022/03/09/%E4%BD%BF%E7%94%A8-Autowired%E6%B3%A8%E8%A7%A3%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D%E7%9A%84%E8%BF%87%E7%A8%8B%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84%EF%BC%9F/","excerpt":"","text":"记住：@Autowired 通过Bean的后置处理器进行解析的 1、 在创建一个Spring上下文的时候再构造函数中进行注册AutowiredAnnotationBeanPostProcessor 2、 在Bean的创建过程中进行解析 2.1、 在实例化后预解析（解析@Autowired标注的属性、方法 比如：把属性的类型、名称、属性所在的类..... 元数据缓存起） 2.2、 在属性注入真正的解析（拿到上一步缓存的元数据 去ioc容器帮进行查找，并且返回注入） a. 首先根据预解析的元数据拿到 类型去容器中进行查找 （如果查询结果刚好为一个，就将该bean装配给@Autowired指定的数据；如果查询的结果不止一个，那么@Autowired会根据名称来查找；如果上述查找的结果为空，那么会抛出异常。解决方法时，使用required=false。）","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"author":"John Doe"},{"title":"配置类@Configuration的作用解析原理","slug":"配置类-Configuration的作用解析原理","date":"2022-03-09T09:00:00.000Z","updated":"2022-03-09T10:09:42.110Z","comments":true,"path":"2022/03/09/配置类-Configuration的作用解析原理/","link":"","permalink":"http://example.com/2022/03/09/%E9%85%8D%E7%BD%AE%E7%B1%BB-Configuration%E7%9A%84%E4%BD%9C%E7%94%A8%E8%A7%A3%E6%9E%90%E5%8E%9F%E7%90%86/","excerpt":"","text":"1、@Configuration用来代替xml配置方式spring.xml配置文件 2、没有@Configuration也是可以配置@Bean 3、 加了@Configuration会为配置类创建cglib动态代理（保证配置类@Bean方法调用Bean的单例），@Bean方法的调用就会通过容器.getBean进行获取 原理： 1、创建Spring上下文的时候会注册一个解析配置的处理器ConfigurationClassPostProcessor（实现BeanFactoryPostProcessor和BeanDefinitionRegistryPostProcessor) 2、在调用invokeBeanFactoryPostProcessor，就会去调用ConfigurationClassPostProcessor.postProcessBeanDefinitionRegistry进行解析配置（解析配置类说白就是去解析各种注解(@Bean @Configuration@Import @Component … 就是注册BeanDefinition) 3、ConfigurationClassPostProcessor.postProcessBeanFactory去创建cglib动态代理","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"author":"John Doe"},{"title":"对于@Bean之间的方法调用是怎么保证单例的？","slug":"对于@Bean之间的方法调用是怎么保证单例的？","date":"2022-03-09T08:44:00.000Z","updated":"2022-03-09T08:57:45.869Z","comments":true,"path":"2022/03/09/对于@Bean之间的方法调用是怎么保证单例的？/","link":"","permalink":"http://example.com/2022/03/09/%E5%AF%B9%E4%BA%8E@Bean%E4%B9%8B%E9%97%B4%E7%9A%84%E6%96%B9%E6%B3%95%E8%B0%83%E7%94%A8%E6%98%AF%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E5%8D%95%E4%BE%8B%E7%9A%84%EF%BC%9F/","excerpt":"","text":"如果希望@bean方法返回的对象是单例，需要在类上加上@Configuration注解。 原因：Spring会使用invokeBeanFactoryPostProcessor 在内置BeanFactoryPostProcessor中使用CGLib生成动态代理，当@Bean方法进行互调时， 则会通过CGLIB进行增强，通过调用的方法名作为bean的名称去ioc容器中获取，进而保证了@Bean方法的单例 。 换句话说：被@Configuration修饰的类，spring容器中会通过cglib给这个类创建一个代理，代理会拦截所有被@Bean 修饰的方法，默认情况（bean为单例）下确保这些方法只被调用一次，从而确保这些bean是同一个bean，即单例的。@Configuration修饰的类有cglib代理效果，默认添加的bean都为单例","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"author":"John Doe"},{"title":"要将一个第三方的类配成为Bean有哪些方式？","slug":"要将一个第三方的类配成为Bean有哪些方式？","date":"2022-03-09T08:39:00.000Z","updated":"2022-03-09T08:42:14.407Z","comments":true,"path":"2022/03/09/要将一个第三方的类配成为Bean有哪些方式？/","link":"","permalink":"http://example.com/2022/03/09/%E8%A6%81%E5%B0%86%E4%B8%80%E4%B8%AA%E7%AC%AC%E4%B8%89%E6%96%B9%E7%9A%84%E7%B1%BB%E9%85%8D%E6%88%90%E4%B8%BABean%E6%9C%89%E5%93%AA%E4%BA%9B%E6%96%B9%E5%BC%8F%EF%BC%9F/","excerpt":"","text":"1、通过@bean注解（搭配@Configurtion） 2、通过@import注解 3、通过Spring的拓展接口BeanDefinitionRegistryPostProcessor","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"author":"John Doe"},{"title":"为什么@ComponentScan 不设置basePackage也会扫描？","slug":"为什么-ComponentScan-不设置basePackage也会扫描？","date":"2022-03-09T08:36:00.000Z","updated":"2022-03-09T08:38:51.082Z","comments":true,"path":"2022/03/09/为什么-ComponentScan-不设置basePackage也会扫描？/","link":"","permalink":"http://example.com/2022/03/09/%E4%B8%BA%E4%BB%80%E4%B9%88-ComponentScan-%E4%B8%8D%E8%AE%BE%E7%BD%AEbasePackage%E4%B9%9F%E4%BC%9A%E6%89%AB%E6%8F%8F%EF%BC%9F/","excerpt":"","text":"@componentScan注解不设置basePackage默认会将你的类所在的包的地址作为扫描包的地址","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"}],"author":"John Doe"},{"title":"Spring是如何解决循环依赖问题的？","slug":"Spring是如何解决循环依赖问题的？","date":"2022-03-09T02:54:00.000Z","updated":"2022-03-09T03:11:12.718Z","comments":true,"path":"2022/03/09/Spring是如何解决循环依赖问题的？/","link":"","permalink":"http://example.com/2022/03/09/Spring%E6%98%AF%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96%E9%97%AE%E9%A2%98%E7%9A%84%EF%BC%9F/","excerpt":"","text":"什么是循环依赖问题？ 类与类之间的依赖关系形成了闭环，就会导致循环依赖问题的产生。（比如A类依赖了B类，B类依赖了C类，而最后C类又依赖了A类，这样就形成了循环依赖问题。） 循环依赖问题在Spring中主要有三种情况： 1、通过构造方法进行依赖注入时产生的循环依赖问题。 2、通过setter方法进行依赖注入且是在多例（原型）模式下产生的循环依赖问题。 3、通过setter方法进行依赖注入且是在单例模式下产生的循环依赖问题。 注意：在Spring中，只有【第三种方式】的循环依赖问题被解决了，其他两种方式在遇到循环依赖问题时都会产生异常。 因为第一种构造方法注入的情况下，在new对象的时候就会堵塞住了，其实也就是”先有鸡还是先有蛋“的历史难题。 第二种setter方法&amp;&amp;多例的情况下，每一次getBean()时，都会产生一个新的Bean，如此反复下去就会有无穷无尽的Bean产生了，最终就会导致OOM问题的出现。 如何解决循环依赖问题？ Spring中有三个缓存，用于存储单例的Bean实例，这三个缓存是彼此互斥的，不会针对同一个Bean的实例同时存储。 如果调用getBean，则需要从三个缓存中依次获取指定的Bean实例。读取顺序依次是一级缓存–&gt;二级缓存–&gt;三级缓存 一级缓存：Map&lt;String, Object&gt; singletonObjects第一级缓存的作用？ 用于存储单例模式下创建的Bean实例（已经创建完毕）。该缓存是对外使用的，指的就是使用Spring框架的程序员。 存储什么数据？ K：bean的名称 V：bean的实例对象（有代理对象则指的是代理对象，已经创建完毕） 第二级缓存：Map&lt;String, Object&gt; earlySingletonObjects第二级缓存的作用？ 用于存储单例模式下创建的Bean实例（该Bean被提前暴露的引用,该Bean还在创建中）。该缓存是对内使用的，指的就是Spring框架内部逻辑使用该缓存。为了解决第一个classA引用最终如何替换为代理对象的问题（如果有代理对象） 存储什么数据？ K：bean的名称 V：bean的实例对象（有代理对象则指的是代理对象，该Bean还在创建中） 第三级缓存：Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories第三级缓存的作用？ 通过ObjectFactory对象来存储单例模式下提前暴露的Bean实例的引用（正在创建中）。该缓存是对内使用的，指的就是Spring框架内部逻辑使用该缓存。此缓存是解决循环依赖最大的功臣 存储什么数据？ K：bean的名称 V：ObjectFactory，该对象持有提前暴露的bean的引用 为什么第三级缓存要使用ObjectFactory？需要提前产生代理对象。 什么时候将Bean的引用提前暴露给第三级缓存的ObjectFactory持有？时机就是在第一步实例化之后，第二步依赖注入之前，完成此操作。 总结以上就是Spring解决循环依赖的关键点！总结来说，就是要搞清楚以下几点： 搞清楚Spring三级缓存的作用？搞清楚第三级缓存中ObjectFactory的作用？搞清楚为什么需要第二级缓存？搞清楚什么时候使用三级缓存（添加和查询操作）？搞清楚什么时候使用二级缓存（添加和查询操作）？当目标对象产生代理对象时，Spring容器中（第一级缓存）到底存储的是谁？","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"循环依赖","slug":"循环依赖","permalink":"http://example.com/tags/%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96/"}],"author":"John Doe"},{"title":"Spring AOP实现机制的一个小小陷阱","slug":"Spring AOP实现机制的一个小小陷阱","date":"2022-03-08T12:36:00.000Z","updated":"2022-03-08T12:48:29.130Z","comments":true,"path":"2022/03/08/Spring AOP实现机制的一个小小陷阱/","link":"","permalink":"http://example.com/2022/03/08/Spring%20AOP%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%88%B6%E7%9A%84%E4%B8%80%E4%B8%AA%E5%B0%8F%E5%B0%8F%E9%99%B7%E9%98%B1/","excerpt":"","text":"我们知道Spring AOP采用代理模式实现，具体的横切逻辑会被添加到动态生成的代理对象中，只要调用的是目标对象的代理对象上的方法就可以保证目标对象的方法执行可以被拦截。 不过遗憾的是，代理模式的实现机制在处理方法调用的时序方面会给使用这种机制实现的AOP产品造成一个缺憾。在处理对象方法中，不管你如何添加横切逻辑，也不管添加多少横切逻辑，有一点是确定的，你最终需要调用目标对象的同一方法来执行最初所应以的方法逻辑。如果目标对象中原始方法调用依赖于其他对象，那没问题。我们可以为目标对象注入所依赖对象的代理，并且可以保证相应的Joinpoint被拦截并且织入相应横切逻辑。但是如果目标对象中的原始方法调用直接调用自身方法时，会导致出现问题 在代理对象的method1执行经历了层层拦截后，最终会将调用转向目标对象上的method1，之后调用流程全部在走targetobject之上，当method1调用method2时，它调用targetobject的method2，而不是代理对象的method2，而针对method2的横切逻辑是织入到代理对象上的，因此method1中调用的method2没有被成功拦截。 好在Spring AOP提供了AopContent来公开当前目标对象的代理对象，我们只要在目标对象中使用AopContent.currentProxy()就可以获取当前目标对象所对应的代理对象。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"AOP","slug":"AOP","permalink":"http://example.com/tags/AOP/"}],"author":"John Doe"},{"title":"AOP的常见应用案例","slug":"AOP的应用案例","date":"2022-03-08T11:26:00.000Z","updated":"2022-03-08T12:33:39.435Z","comments":true,"path":"2022/03/08/AOP的应用案例/","link":"","permalink":"http://example.com/2022/03/08/AOP%E7%9A%84%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B/","excerpt":"","text":"1、异常处理： 通常将Error和RuntimeException及其子类称作非受检异常。（编译器不会对这些类型的异常进行编译期检查），而其他的则为受检异常（编写程序期间就应进行处理）。Fault Barrier即为对非受检异常的处理。 对于这些非受检异常的处理可以归并于溢出进行处理，而不是让他们散落到系统的各处。介于此，我们可以通过实现一个Aspect来处理，让其对系统中所有可能的falut情况进行统一的处理。而这个专职于处理Falut的Aspect即为Falut Barrier。 2、安全检查： Filter是Servlet规范为我们提供的一种AOP支持。通过它我们可以为基于servlet的web应用添加相应的资源访问控制等等。但是，基于filter的web应用的资源访问控制仅仅是特定领域安全检查需求。而通过AOP，我们可以为任何类型的应用添加安全支持。（Spring Security则是基于Spring的一款强大的安全框架） 3、缓存： AOP应用的另一个主要场景则是为系统透明地添加缓存支持。缓存可以在很大程度上提升系统性能。为了避免需要添加的缓存实现逻辑影响业务逻辑的实现，我们可以让缓存的实现独立于业务对象的实现外，将系统中的缓存需求通过AOP的Aspect进行封装，只在系统中某个点确切需要缓存的情况下才进行织入。（现在已经有许多现成的Caching产品实现，入EhCache、Redis等） Spring Modules项目提供了对现有Caching产品的集成，这样可以通过外部声明的方式为系统中的Joinpoint加Caching支持。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"AOP","slug":"AOP","permalink":"http://example.com/tags/AOP/"},{"name":"应用","slug":"应用","permalink":"http://example.com/tags/%E5%BA%94%E7%94%A8/"}],"author":"John Doe"},{"title":"C语言程序经过预处理、编译、汇编和链接等各个阶段的变化情况","slug":"C语言程序经过预处理、编译、汇编和链接等各个阶段的变化情况","date":"2022-03-08T07:59:00.000Z","updated":"2022-03-08T11:26:01.536Z","comments":true,"path":"2022/03/08/C语言程序经过预处理、编译、汇编和链接等各个阶段的变化情况/","link":"","permalink":"http://example.com/2022/03/08/C%E8%AF%AD%E8%A8%80%E7%A8%8B%E5%BA%8F%E7%BB%8F%E8%BF%87%E9%A2%84%E5%A4%84%E7%90%86%E3%80%81%E7%BC%96%E8%AF%91%E3%80%81%E6%B1%87%E7%BC%96%E5%92%8C%E9%93%BE%E6%8E%A5%E7%AD%89%E5%90%84%E4%B8%AA%E9%98%B6%E6%AE%B5%E7%9A%84%E5%8F%98%E5%8C%96%E6%83%85%E5%86%B5/","excerpt":"","text":"编译过程概述：通常编译程序的过程分为词法分析、语法分析、语义分析、目目标代码生成4个阶段（如果编译器支持优化，还可以有中间代码生成和代码优化两个阶段）。 1、词法分析 此阶段的任务是从左到右一个字符一个字符地读入源程序，对构成源程序的字符进行扫描和分解，从而识别出一个个单词（逻辑上紧密相连的一组有集体含义的字符）。 2、语法分析 此阶段的任务是在词法分析的基础上将单词序列分解成各类语法短语（也称语法单位）可表示成语法树。 注：词法分析和语法分析本质上都是对源程序的结构进行分析。 3、语义分析 语义分析是审查源程序有无语义错误，为代码生成阶段收集类型信息。 4、中间代码生成 “中间代码”是一种结构简单，含义明确的记号系统，这种记号系统可以设计为多种多样的形式，重要的设计原则为两点：一是容易生成；二是容易将它翻译成目标代码。很多编译程序采用了一种近似“三地址指令”的“四元式”中间代码。这种四元式的形式为：（运算符，运算对象1，运算对象2，结果） 5、代码优化 将中间代码进行变换或进行改造，目的：使生成的目标代码更为高效，即省时间和空间 6、目标代码生成 任务是把中间代码变换成特定机器上的绝对指令代码或可重定位的指令代码或汇编指令代码。","categories":[{"name":"编译原理","slug":"编译原理","permalink":"http://example.com/categories/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"}],"tags":[{"name":"基础概念","slug":"基础概念","permalink":"http://example.com/tags/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"}],"author":"John Doe"},{"title":"Spring AOP 一世","slug":"Spring-AOP-一世","date":"2022-03-07T12:03:00.000Z","updated":"2022-03-07T13:01:58.232Z","comments":true,"path":"2022/03/07/Spring-AOP-一世/","link":"","permalink":"http://example.com/2022/03/07/Spring-AOP-%E4%B8%80%E4%B8%96/","excerpt":"","text":"AOP的Joinpoint可以有许多类型，入构造方法调用、字段的设置及获取、方法调用等。但是在Spring AOP中，仅支持方法级别的Joinpoint。更确切的说，只支持方法执行类型的Joinpoint Spring中以接口定义Pointcut作为其AOP框架中所有Pointcut的最顶级抽象，该接口定义了两个方法来捕获系统中相应的Joinpoint，并提供了一个TruePointcut类型实例。如果Pointcut类型为TruePointcut，默认会对系统中的所有对象，以及对象上所有被支持的Joinpoint进行匹配。 ClassFileter和MethodMatcher分别用于匹配将被执行织入操作的对象以及相应的方法。（重用不同级别的匹配定义，并且可以在不同或相同的级别上进行组合操作，或者强制让某个子类只覆写相应的方法） Spring中各种advice实现全部遵循AOP ALLiance规定的接口。 advice实现了将被织入到Pointcut规定的Joinpoint处的横切逻辑。在Spring中，advice按照其自身实例能否在目标对象类的所有实例中共享这一标准，可以划分为两大类（per-class和per-instance） per-class类型的advice：该类型可以在目标对象类的所有实例之间共享。这种类型的advice通常只提供方法拦截的功能。不会为目标对象类保存任何状态或添加新特性。除了上图没有列出的intriuduction类型的advice外，其余都属于pre-class。（如：BeforeAdvice、ThrowsAdvic、AfterReturningAdvice、AroundAdvice等） per-instance类型的advice：introduction是唯一的一种per-instance型advice。其可以在不改目标类定义的情况下，为目标类添加新的属性和行为。 当所有的Pointcut和advice准备好之后，就可以将其装入Aspect。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"AOP","slug":"AOP","permalink":"http://example.com/tags/AOP/"}],"author":"John Doe"},{"title":"Spring AOP的实现机制","slug":"Spring-AOP的实现机制","date":"2022-03-07T11:43:00.000Z","updated":"2022-03-07T11:57:38.005Z","comments":true,"path":"2022/03/07/Spring-AOP的实现机制/","link":"","permalink":"http://example.com/2022/03/07/Spring-AOP%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%88%B6/","excerpt":"","text":"Spring AOP采用动态代理机制（先尝试jdk动态代理，如果没有实现相应接口，则采用cglib字节码生成技术）和字节码生成技术实现（对目标对象进行继承拓展，为其生成相应的子类，子类通过重写来扩展父类的行为，只要将横切逻辑的实现放到子类中，然后让系统使用扩展后的子类即可）。与最初AspectJ采用编译器将横切逻辑织入到目标对象不同，动态代理和字节码生成都是在运行期间为目标对象生成一个代理对象，而将横切逻辑织入到这个代理对象中，系统最终使用的是织入横切逻辑的代理对象而不是真正的目标对象。 注意：动态代理需要实现统一接口，而cglib生成字节码需要方法可以重写","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"AOP","slug":"AOP","permalink":"http://example.com/tags/AOP/"}],"author":"John Doe"},{"title":"AOP国家的公民","slug":"AOP国家的公民","date":"2022-03-07T10:33:00.000Z","updated":"2022-03-07T11:39:52.542Z","comments":true,"path":"2022/03/07/AOP国家的公民/","link":"","permalink":"http://example.com/2022/03/07/AOP%E5%9B%BD%E5%AE%B6%E7%9A%84%E5%85%AC%E6%B0%91/","excerpt":"","text":"Joinpoint：在系统运行前，AOP的功能模块都需要编织入OOP的功能模块中。所以，要进行这种编织，我们需要在哪些执行点进行。这些将要在其上执行编织操作的系统执行点即称之为Joinpoint。 Pointcut：Pointcut是Joinpoint的表达方式。将横切逻辑编织入当前系统的过程中，需要参考Pointcut规定的Joinpoint信息，才可以指定应该往系统的哪些Joinpoint上编织横切逻辑。即声明了一个Pointcut就指定了系统中符合条件的一组Joinpoint。 advice：advice是单一横切关注点逻辑的载体，它代表将会编织到Joinpoint的横切逻辑。如果将Aspect比作OOP中的class，那么advice就相当于class中的method。（常见的如before advice、after advice、after returning advice、after throwing advice，after advice、around advice等） Aspect：Aspect是对系统中横切关注点进行模块化封装的AOP概念实体。通常情况下，Aspect可以含有多个Pointcut以及相关Advice定义。 织入和织入器：织入过程就是“飞架”AOP和OOP的那座桥，只有经过织入过程后，以Aspect模块化的横切关注点才会集成到OOP的现存系统中。而完成织入过程的那个人就是织入器。AspectJ有专门的编译器来完成织入操作，即ajc，所有ajc就是AspectJ完成织入的织入器；JBossAOP采用自定义的类加载器完成最终织入，那么这个类加载器就是他的织入器。SpringAOP使用一组自定义的类来完成最终的织入操作，proxyFactory类则是SpringAOP中最通用的织入器。 目标对象：符合Pointcut所指定的条件，将在织入过程中被织入横切逻辑的对象，称为目标对象。 当把所有这些概念组织到一个场景后，就有如下一副场景图：","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"AOP","slug":"AOP","permalink":"http://example.com/tags/AOP/"}],"author":"John Doe"},{"title":"为什么需要AOP？","slug":"为什么需要AOP？","date":"2022-03-07T08:29:00.000Z","updated":"2022-03-07T08:46:14.426Z","comments":true,"path":"2022/03/07/为什么需要AOP？/","link":"","permalink":"http://example.com/2022/03/07/%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81AOP%EF%BC%9F/","excerpt":"","text":"软件开发的目的最终是为了解决各种需求，包括业务需求和系统需求。使用面向对象的思想，我们可以对业务需求等普通关注点进行很好的抽象和封装，并且使之模块化。但对于系统需求一类的关注点来说，情况有所不同。 对于业务需求而言，需求与具体实现直接的关系基本上是一对一的。我们可以在系统中某一个确定的点找到针对这种需求的实现。 但是在开发测试中或者进入生产环境后需要对系统进行监控，我们需要添加日志功能，除此之外，业务方法的执行可能需要一定的权限限制。那么方法执行前需要有相应的安全检查功能。对于这些系统需求，想要以面向对象的方式实现并集成待整个系统中，不仅仅是一个需求对应一个实现那么简单。可能遍布所有的业务对象。 因此，提出了AOP（面向切面编程），我们可以对类似于logging和security等系统需求进行模块化组织，简化系统需求和实现的对比关系，进而使得整个系统的实现更具模块化。 AOP是一种理念，其实现需要一种方式。就好似OOP需要对应的语言支撑一样。AOP也需要某种语言帮助实现相应的概念实体（AOL）。 静态AOP：相应的横切关注点以Aspect形式实现后，会通过特定的编译器，将实现后的Aspect编织到系统的静态类中。 静态AOP的优点：Aspect直接以java字节码的形式编译到java类。jvm可以想运行类一样运行，不会对系统造成任何性能损失。 静态AOP的缺点：灵活性不够，如果横切关注点需要改变织入位置，需要重新修改Aspect，编织进系统。 动态AOP：AOP的各种概念实体全部都是put的java类，所有容易开发和基础。Aspect和class一样以类的身份作为系统的一员。其织入过程在运行时进行，而不是预先编译织入。而且信息可以采用外部xml等形式保存，在调制编织点时不必变更系统其他模块，甚至在系统运行时，动态更改。但其缺点也很明显，就是在运行时编织，会造成一点的运行时性能损失。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"AOP","slug":"AOP","permalink":"http://example.com/tags/AOP/"}],"author":"John Doe"},{"title":"创建者模式对比","slug":"创建者模式对比","date":"2022-03-07T07:55:00.000Z","updated":"2022-03-07T07:58:03.500Z","comments":true,"path":"2022/03/07/创建者模式对比/","link":"","permalink":"http://example.com/2022/03/07/%E5%88%9B%E5%BB%BA%E8%80%85%E6%A8%A1%E5%BC%8F%E5%AF%B9%E6%AF%94/","excerpt":"","text":"工厂方法模式vs建造者模式 工厂方法模式注重的是整体对象的创建方式；而建造者模式注重的是部件构建的过程，意在通过一步一步地精确构造创建出一个复杂的对象。 我们举个简单例子来说明两者的差异，如要制造一个超人，如果使用工厂方法模式，直接产生出来的就是一个力大无穷、能够飞翔、内裤外穿的超人；而如果使用建造者模式，则需要组装手、头、脚、躯干等部分，然后再把内裤外穿，于是一个超人就诞生了。 抽象工厂模式vs建造者模式 抽象工厂模式实现对产品家族的创建，一个产品家族是这样的一系列产品：具有不同分类维度的产品组合，采用抽象工厂模式则是不需要关心构建过程，只关心什么产品由什么工厂生产即可。 建造者模式则是要求按照指定的蓝图建造产品，它的主要目的是通过组装零配件而产生一个新产品。 如果将抽象工厂模式看成汽车配件生产工厂，生产一个产品族的产品，那么建造者模式就是一个汽车组装工厂，通过对部件的组装可以返回一辆完整的汽车。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"创建者模式","slug":"创建者模式","permalink":"http://example.com/tags/%E5%88%9B%E5%BB%BA%E8%80%85%E6%A8%A1%E5%BC%8F/"}],"author":"John Doe"},{"title":"建造者模式","slug":"建造者模式","date":"2022-03-07T07:49:00.000Z","updated":"2022-03-07T07:55:15.846Z","comments":true,"path":"2022/03/07/建造者模式/","link":"","permalink":"http://example.com/2022/03/07/%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"概述：将一个复杂对象的构建与表示分离，使得同样的构建过程可以创建不同的表示。 分离了部件的构造(由Builder来负责)和装配(由Director负责)。 从而可以构造出复杂的对象。这个模式适用于：某个对象的构建过程复杂的情况。 由于实现了构建和装配的解耦。不同的构建器，相同的装配，也可以做出不同的对象；相同的构建器，不同的装配顺序也可以做出不同的对象。也就是实现了构建算法、装配算法的解耦，实现了更好的复用。 建造者模式可以将部件和其组装过程分开，一步一步创建一个复杂的对象。用户只需要指定复杂对象的类型就可以得到该对象，而无须知道其内部的具体构造细节。 结构： 抽象建造者类（Builder）：这个接口规定要实现复杂对象的那些部分的创建，并不涉及具体的部件对象的创建。 具体建造者类（ConcreteBuilder）：实现 Builder 接口，完成复杂产品的各个部件的具体创建方法。在构造过程完成后，提供产品的实例。 产品类（Product）：要创建的复杂对象。 指挥者类（Director）：调用具体建造者来创建复杂对象的各个部分，在指导者中不涉及具体产品的信息，只负责保证对象各部分完整创建或按某种顺序创建。 优点：建造者模式的封装性很好。使用建造者模式可以有效的封装变化，在使用建造者模式的场景中，一般产品类和建造者类是比较稳定的，因此，将主要的业务逻辑封装在指挥者类中对整体而言可以取得比较好的稳定性。 在建造者模式中，客户端不必知道产品内部组成的细节，将产品本身与产品的创建过程解耦，使得相同的创建过程可以创建不同的产品对象。 可以更加精细地控制产品的创建过程 。将复杂产品的创建步骤分解在不同的方法中，使得创建过程更加清晰，也更方便使用程序来控制创建过程。 建造者模式很容易进行扩展。如果有新的需求，通过实现一个新的建造者类就可以完成，基本上不用修改之前已经测试通过的代码，因此也就不会对原有功能引入风险。符合开闭原则。 缺点：造者模式所创建的产品一般具有较多的共同点，其组成部分相似，如果产品之间的差异性很大，则不适合使用建造者模式，因此其使用范围受到一定的限制。 使用场景： 创建的对象较复杂，由多个部件构成，各部件面临着复杂的变化，但构件间的建造顺序是稳定的。 创建复杂对象的算法独立于该对象的组成部分以及它们的装配方式，即产品的构建过程和最终的表示是独立的。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"建造者模式","slug":"建造者模式","permalink":"http://example.com/tags/%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/"}],"author":"John Doe"},{"title":"原型模式","slug":"原型模式","date":"2022-03-07T07:41:00.000Z","updated":"2022-03-07T07:48:09.685Z","comments":true,"path":"2022/03/07/原型模式/","link":"","permalink":"http://example.com/2022/03/07/%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"概述：用一个已经创建的实例作为原型，通过复制该原型对象来创建一个和原型对象相同的新对象。 结构：原型模式包含如下角色：抽象原型类：规定了具体原型对象必须实现的的 clone() 方法。具体原型类：实现抽象原型类的 clone() 方法，它是可被复制的对象。访问类：使用具体原型类中的 clone() 方法来复制新的对象。 原型模式的克隆分为浅克隆和深克隆。 浅克隆：创建一个新对象，新对象的属性和原来对象完全相同，对于非基本类型属性，仍指向原有属性所指向的对象的内存地址。 深克隆：创建一个新对象，属性中引用的其他对象也会被克隆，不再指向原有对象地址。（使用对象流，先将原型对象存入file，然后从file读取，即为深克隆） Java中的Object类中提供了clone() 方法来实现浅克隆。 Cloneable 接口是上面的类图中的抽象原型类，而实现了Cloneable接口的子实现类就是具体的原型类。 使用场景：对象的创建非常复杂，可以使用原型模式快捷的创建对象。性能和安全要求比较高。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"原型模式","slug":"原型模式","permalink":"http://example.com/tags/%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/"}],"author":"John Doe"},{"title":"工厂模式","slug":"工厂模式","date":"2022-03-07T01:24:00.000Z","updated":"2022-03-07T01:39:37.035Z","comments":true,"path":"2022/03/07/工厂模式/","link":"","permalink":"http://example.com/2022/03/07/%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"在java中，万物皆对象，这些对象都需要创建，如果创建的时候直接new该对象，就会对该对象耦合严重，假如我们要更换对象，所有new对象的地方都需要修改一遍，这显然违背了软件设计的开闭原则。如果我们使用工厂来生产对象，我们就只和工厂打交道就可以了，彻底和对象解耦，如果要更换对象，直接在工厂里更换该对象即可，达到了与对象解耦的目的；所以说，工厂模式最大的优点就是：解耦。 1、简单工厂模式：简单工厂不是一种设计模式，反而比较像是一种编程习惯。 结构： 简单工厂包含如下角色： 抽象产品 ：定义了产品的规范，描述了产品的主要特性和功能。 具体产品 ：实现或者继承抽象产品的子类 具体工厂 ：提供了创建产品的方法，调用者通过该方法来获取产品。 优点：封装了创建对象的过程，可以通过参数直接获取对象。把对象的创建和业务逻辑层分开，这样以后就避免了修改客户代码，如果要实现新产品直接修改工厂类，而不需要在原代码中修改，这样就降低了客户代码修改的可能性，更加容易扩展。 缺点：增加新产品时还是需要修改工厂类的代码，违背了“开闭原则”。 拓展：静态工厂– 在开发中也有一部分人将工厂类中的创建对象的功能定义为静态的，这个就是静态工厂模式，它也不是23种设计模式中的。 2、工厂方法模式：使用工厂方法模式就可以完美的解决，完全遵循开闭原则。 概念：定义一个用于创建对象的接口，让子类决定实例化哪个产品类对象。工厂方法使一个产品类的实例化延迟到其工厂的子类。 结构： 工厂方法模式的主要角色： 抽象工厂（Abstract Factory）：提供了创建产品的接口，调用者通过它访问具体工厂的工厂 方法来创建产品。 具体工厂（ConcreteFactory）：主要是实现抽象工厂中的抽象方法，完成具体产品的创建。 抽象产品（Product）：定义了产品的规范，描述了产品的主要特性和功能。 具体产品（ConcreteProduct）：实现了抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间一一对应。 优点：用户只需要知道具体工厂的名称就可得到所要的产品，无须知道产品的具体创建过程；在系统增加新的产品时只需要添加具体产品类和对应的具体工厂类，无须对原工厂进行任何修改，满足开闭原则； 缺点：每增加一个产品就要增加一个具体产品类和一个对应的具体工厂类，这增加了系统的复杂度。 3、抽象工厂模式：是一种为访问类提供一个创建一组相关或相互依赖对象的接口，且访问类无须指定所要产品的具体类就能得到同族的不同等级的产品的模式结构。抽象工厂模式是工厂方法模式的升级版本，工厂方法模式只生产一个等级的产品，而抽象工厂模式可生产多个等级的产品。 结构： 抽象工厂模式的主要角色如下： 抽象工厂（Abstract Factory）：提供了创建产品的接口，它包含多个创建产品的方法，可以创建多个不同等级的产品。 具体工厂（Concrete Factory）：主要是实现抽象工厂中的多个抽象方法，完成具体产品的创建。 抽象产品（Product）：定义了产品的规范，描述了产品的主要特性和功能，抽象工厂模式有多个抽象产品。 具体产品（ConcreteProduct）：实现了抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间是多对一的关系。 优点：当一个产品族中的多个对象被设计成一起工作时，它能保证客户端始终只使用同一个产品族中的对象。 缺点：当产品族中需要增加一个新的产品时，所有的工厂类都需要进行修改。 使用场景： 当需要创建的对象是一系列相互关联或相互依赖的产品族时，如电器工厂中的电视机、洗衣机、空调等。 系统中有多个产品族，但每次只使用其中的某一族产品。如有人只喜欢穿某一个品牌的衣服和鞋。 系统中提供了产品的类库，且所有产品的接口相同，客户端不依赖产品实例的创建细节和内部结构。 如：输入法换皮肤，一整套一起换。生成不同操作系统的程序。 模式拓展（简单工厂+配置文件解除耦合）：可以通过工厂模式+配置文件的方式解除工厂对象和产品对象的耦合。在工厂类中加载配置文件中的全类名，并创建对象进行存储，客户端如果需要对象，直接进行获取即可。 jdk中的工厂方法使用： 1、集合-迭代器2、DateForamt类中的getInstance()方法使用的是工厂模式；3、Calendar类中的getInstance()方法使用的是工厂模式；","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"工厂模式","slug":"工厂模式","permalink":"http://example.com/tags/%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/"}],"author":"John Doe"},{"title":"elementData设置成了transient，那ArrayList是怎么把元素序列化的呢？","slug":"elementData设置成了transient，那ArrayList是怎么把元素序列化的呢？","date":"2022-03-05T05:00:00.000Z","updated":"2022-03-05T05:03:07.952Z","comments":true,"path":"2022/03/05/elementData设置成了transient，那ArrayList是怎么把元素序列化的呢？/","link":"","permalink":"http://example.com/2022/03/05/elementData%E8%AE%BE%E7%BD%AE%E6%88%90%E4%BA%86transient%EF%BC%8C%E9%82%A3ArrayList%E6%98%AF%E6%80%8E%E4%B9%88%E6%8A%8A%E5%85%83%E7%B4%A0%E5%BA%8F%E5%88%97%E5%8C%96%E7%9A%84%E5%91%A2%EF%BC%9F/","excerpt":"","text":"查看writeObject()方法可知，先调用s.defaultWriteObject()方法，再把size写入到流中，再把元素一个一个的写入到流中。 一般地，只要实现了Serializable接口即可自动序列化，writeObject()和readObject()是为了自己控制序列化的方式，这两个方法必须声明为private，在java.io.ObjectStreamClass#getPrivateMethod()方法中通过反射获取到writeObject()这个方法。 在ArrayList的writeObject()方法中先调用了s.defaultWriteObject()方法，这个方法是写入非static非transient的属性，在ArrayList中也就是size属性。同样地，在readObject()方法中先调用了s.defaultReadObject()方法解析出了size属性。 elementData定义为transient的优势，自己根据size序列化真实的元素，而不是根据数组的长度序列化元素，减少了空间占用。 源码如下： private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; // 防止序列化期间有修改 int expectedModCount = modCount; // 写出非transient非static属性（会写出size属性） s.defaultWriteObject(); // 写出元素个数 s.writeInt(size); // 依次写出元素 for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); &#125; // 如果有修改，抛出异常 if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125; &#125; private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; // 声明为空数组 elementData = EMPTY_ELEMENTDATA; // 读入非transient非static属性（会读取size属性） s.defaultReadObject(); // 读入元素个数，没什么用，只是因为写出的时候写了size属性，读的时候也要按顺序来读 s.readInt(); if (size &gt; 0) &#123; // 计算容量 int capacity = calculateCapacity(elementData, size); SharedSecrets.getJavaOISAccess().checkArray(s, Object[].class, capacity); // 检查是否需要扩容 ensureCapacityInternal(size); Object[] a = elementData; // 依次读取元素到数组中 for (int i=0; i&lt;size; i++) &#123; a[i] = s.readObject(); &#125; &#125; &#125;","categories":[{"name":"集合","slug":"集合","permalink":"http://example.com/categories/%E9%9B%86%E5%90%88/"},{"name":"ArrayList","slug":"集合/ArrayList","permalink":"http://example.com/categories/%E9%9B%86%E5%90%88/ArrayList/"}],"tags":[{"name":"transient","slug":"transient","permalink":"http://example.com/tags/transient/"}],"author":"John Doe"},{"title":"反射和反序列化对单例模式的破坏","slug":"反射和反序列化对单例模式的破坏","date":"2022-03-05T01:45:00.000Z","updated":"2022-03-05T02:44:20.102Z","comments":true,"path":"2022/03/05/反射和反序列化对单例模式的破坏/","link":"","permalink":"http://example.com/2022/03/05/%E5%8F%8D%E5%B0%84%E5%92%8C%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E5%AF%B9%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%E7%9A%84%E7%A0%B4%E5%9D%8F/","excerpt":"","text":"除了枚举单列模式外，其余的单例实现方式都有可能被反射和反序列化所破坏。那么如何解决反射和反序列化对单例模式的破坏呢？ 1、反射方式破解单例的解决方法:这种方式比较好理解。当通过反射方式调用构造方法进行创建创建时，直接抛异常。不运行此中操作。 /** * @author atao * @version 1.0.0 * @ClassName Demo7.java * @Description 懒汉式-方式3（双重检查锁）双重检查锁模式是一种非常好的单例实现模式，解决了单例、性能、线程安全问题，上面的双重检 * 测锁模式看上去完美无缺，其实是存在问题，在多线程的情况下，可能会出现空指针问题，出现问 * 题的原因是JVM在实例化对象的时候会进行优化和指令重排序操作。 * 要解决双重检查锁模式带来空指针异常的问题，只需要使用 volatile 关键字, volatile 关 * 键字可以保证可见性和有序性。 * @createTime 2022年03月05日 10:35:00 */ public class Demo7 &#123; private static volatile Demo7 singleton; private Demo7()&#123; // 解决反射对单例模式的破坏 if (singleton != null)&#123; throw new RuntimeException(); &#125; &#125; public static Demo7 getInstance()&#123; if (singleton == null)&#123; synchronized (Demo2.class)&#123; if (singleton == null)&#123; singleton = new Demo7(); &#125; &#125; &#125; return singleton; &#125; &#125; 2、在Singleton类中添加 readResolve() 方法，在反序列化时被反射调用，如果定义了这个方法，就返回这个方法的值，如果没有定义，则返回新new出来的对象。 /** * @author atao * @version 1.0.0 * @ClassName Demo8.java * @Description 懒汉式-方式4（静态内部类方式）静态内部类单例模式中实例由内部类创建，由于 JVM 在加载外部类的过程中, 是不会加载静态 * 内部类的, 只有内部类的属性/方法被调用时才会被加载, 并初始化其静态属性。静态属性由于被 * static 修饰，保证只被实例化一次，并且严格保证实例化顺序。 * @createTime 2022年03月05日 10:37:00 */ public class Demo8 &#123; private Demo8 singleton; private Demo8()&#123; &#125; private static class inner&#123; public static Demo8 singleton = new Demo8(); &#125; public static Demo8 getInstance()&#123; return Demo8.inner.singleton; &#125; /** * 解决反序列化对单例模式的破坏 * @return */ private Object readResolve()&#123; return Demo8.inner.singleton; &#125; &#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"单例模式","slug":"单例模式","permalink":"http://example.com/tags/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"name":"破坏","slug":"破坏","permalink":"http://example.com/tags/%E7%A0%B4%E5%9D%8F/"}],"author":"John Doe"},{"title":"软件设计原则","slug":"软件设计原则","date":"2022-03-04T13:57:00.000Z","updated":"2022-03-04T14:04:16.777Z","comments":true,"path":"2022/03/04/软件设计原则/","link":"","permalink":"http://example.com/2022/03/04/%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/","excerpt":"","text":"在软件开发中，为了提高软件系统的可维护性和可复用性，增加软件的可扩展性和灵活性，程序员要尽量根据6条原则来开发程序，从而提高软件开发效率、节约软件开发成本和维护成本。 1、开闭原则：对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，实现一个热插拔的效果。 简言之，是为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类。因为抽象灵活性好，适应性广，只要抽象的合理，可以基本保持软件架构的稳定。而软件中易变的细节可以从抽象派生来的实现类来进行扩展，当软件需要发生变化时，只需要根据需求重新派生一个实现类来扩展就可以了。 2、里式替换原则：里氏代换原则是面向对象设计的基本原则之一。任何基类可以出现的地方，子类一定可以出现。通俗理解：子类可以扩展父类的功能，但不能改变父类原有的功能。换句话说，子类继承父类时，除添加新的方法完成新增功能外，尽量不要重写父类的方法。如果通过重写父类的方法来完成新的功能，这样写起来虽然简单，但是整个继承体系的可复用性会比较差，特别是运用多态比较频繁时，程序运行出错的概率会非常大。 3、依赖倒转原则：高层模块不应该依赖低层模块，两者都应该依赖其抽象；抽象不应该依赖细节，细节应该依赖抽象。简单的说就是要求对抽象进行编程，不要对实现进行编程，这样就降低了客户与实现模块间的耦合。 4、接口隔离原则：客户端不应该被迫依赖于它不使用的方法；一个类对另一个类的依赖应该建立在最小的接口上。 5、迪米特法则：迪米特法则又叫最少知识原则。只和你的直接朋友交谈，不跟“陌生人”说话（Talk only to your immediate friends andnot to strangers）。其含义是：如果两个软件实体无须直接通信，那么就不应当发生直接的相互调用，可以通过第三方转发该调用。其目的是降低类之间的耦合度，提高模块的相对独立性。迪米特法则中的“朋友”是指：当前对象本身、当前对象的成员对象、当前对象所创建的对象、当前对象的方法参数等，这些对象同当前对象存在关联、聚合或组合关系，可以直接访问这些对象的方法。 6、合成复用原则：合成复用原则是指：尽量先使用组合或者聚合等关联关系来实现，其次才考虑使用继承关系来实现。通常类的复用分为继承复用和合成复用两种。继承复用虽然有简单和易实现的优点，但它也存在以下缺点： 继承复用破坏了类的封装性。因为继承会将父类的实现细节暴露给子类，父类对子类是透明的，所以这种复用又称为“白箱”复用。 子类与父类的耦合度高。父类的实现的任何改变都会导致子类的实现发生变化，这不利于类的扩展与维护。 它限制了复用的灵活性。从父类继承而来的实现是静态的，在编译时已经定义，所以在运行时不可能发生变化。采用组合或聚合复用时，可以将已有对象纳入新对象中，使之成为新对象的一部分，新对象可以调用已有对象的功能，它有以下优点： 它维持了类的封装性。因为成分对象的内部细节是新对象看不见的，所以这种复用又称为“黑箱”复用。 对象间的耦合度低。可以在类的成员位置声明抽象。 复用的灵活性高。这种复用可以在运行时动态进行，新对象可以动态地引用与成分对象类型相同的对象。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计原则","slug":"设计原则","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/"}],"author":"John Doe"},{"title":"UML类图","slug":"UML类图","date":"2022-03-04T13:51:00.000Z","updated":"2022-03-04T13:57:31.240Z","comments":true,"path":"2022/03/04/UML类图/","link":"","permalink":"http://example.com/2022/03/04/UML%E7%B1%BB%E5%9B%BE/","excerpt":"","text":"属性/方法名称前加的加号和减号表示了这个属性/方法的可见性，UML类图中表示可见性的符号有三种： +：表示public -：表示private #：表示protected 属性的完整表示方式是： 可见性 名称 ：类型 [ = 缺省值]方法的完整表示方式是： 可见性 名称(参数列表) [ ： 返回类型] 类之间关系的表示方式： 1、关联关系：关联关系是对象之间的一种引用关系，用于表示一类对象与另一类对象之间的联系，如老师和学生、师傅和徒弟、丈夫和妻子等。关联关系是类与类之间最常用的一种关系，分为一般关联关系、聚合关系和组合关系。 自关联： 2、聚合关系：聚合关系是关联关系的一种，是强关联关系，是整体和部分之间的关系。聚合关系也是通过成员对象来实现的，其中成员对象是整体对象的一部分，但是成员对象可以脱离整体对象而独立存在。 3、组合关系：组合表示类之间的整体与部分的关系，但它是一种更强烈的聚合关系。在组合关系中，整体对象可以控制部分对象的生命周期，一旦整体对象不存在，部分对象也将不存在，部分对象不能脱离整体对象而存在。 4、依赖关系：依赖关系是一种使用关系，它是对象之间耦合度最弱的一种关联方式，是临时性的关联。在代码中，某个类的方法通过局部变量、方法的参数或者对静态方法的调用来访问另一个类（被依赖类）中的某些方法来完成一些职责。 5、继承关系：继承关系是对象之间耦合度最大的一种关系，表示一般与特殊的关系，是父类与子类之间的关系，是一种继承关系。 6、实现关系：实现关系是接口与实现类之间的关系。在这种关系中，类实现了接口，类中的操作实现了接口中所声明的所有的抽象操作。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"UML","slug":"UML","permalink":"http://example.com/tags/UML/"}],"author":"John Doe"},{"title":"设计模式的分类","slug":"设计模式的分类","date":"2022-03-04T13:31:00.000Z","updated":"2022-03-04T13:34:39.072Z","comments":true,"path":"2022/03/04/设计模式的分类/","link":"","permalink":"http://example.com/2022/03/04/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%88%86%E7%B1%BB/","excerpt":"","text":"创建型模式：用于描述“怎样创建对象”，它的主要特点是“将对象的创建与使用分离”。（单例、原型、工厂方法、抽象工厂、建造者等） 结构型模式：用于描述如何将类或对象按某种布局成更大的结构。（代理、适配器、桥接、装饰、外观、享元、组合等） 行为型模式：用于描述类或对象之间怎样相互协作共同完成单个对象无法单独完成的任务，以及怎样分配职责。（模板方法、策略、命令、职责链、状态、观察者、中介者、迭代器、访问者、备忘录、解释器等）","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"分类","slug":"分类","permalink":"http://example.com/tags/%E5%88%86%E7%B1%BB/"}],"author":"John Doe"},{"title":"将革命进行得更彻底一些（classpath-scanning 功能介绍）","slug":"将革命进行得更彻底一些（classpath-scanning-功能介绍）","date":"2022-03-04T08:23:00.000Z","updated":"2022-03-04T09:01:35.117Z","comments":true,"path":"2022/03/04/将革命进行得更彻底一些（classpath-scanning-功能介绍）/","link":"","permalink":"http://example.com/2022/03/04/%E5%B0%86%E9%9D%A9%E5%91%BD%E8%BF%9B%E8%A1%8C%E5%BE%97%E6%9B%B4%E5%BD%BB%E5%BA%95%E4%B8%80%E4%BA%9B%EF%BC%88classpath-scanning-%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D%EF%BC%89/","excerpt":"","text":"到目前为止，我们还是需要将相应对象的bean定义，一个个地添加到IoC容器的配置文件中。与之前唯一的区别就是，不用在配置文件中明确指定依赖关系了（改用注解来表达了嘛）。。既然使用注解来表达对象之间的依赖注入关系，那为什么不搞的彻底一点儿，将那些几乎“光秃秃”的bean定义从配置文件中彻底消灭呢？OK，我们想到了，Spring开发团队也想到了，classpath-scanning的功能正是因此而诞生的！ 使用相应的注解对组成应用程序的相关类进行标注之后，classpath-scanning功能可以从某一顶层包（base package）开始扫描。当扫描到某个类标注了相应的注解之后，就会提取该类的相关信息，构建对应的BeanDefinition，然后把构建完的BeanDefinition注册到容器。这之后所发生的事情就不用我说了，既然相关的类已经添加到了容器，那么后面BeanPostProcessor为@Autowired或者@Resource所提供的注入肯定是有东西拿咯！ classpath-scanning功能的触发是由context:component-scan决定的。 context:component-scan默认扫描的注解类型是@Component。不过，在@Component语义基础上细化后的@Repository、@Service和@Controller也同样可以获得context:component-scan的青睐。@Component的语义更广、更宽泛，而@Repository、@Service和@Controller的语义则更具体。所以，同样对于服务层的类定义来说，使用@Service标注它，要比使用@Component更为确切。对于其他两种注解也是同样道理，我们暂且使用语义更广的@Component来标注FXNews相关类，以便摆脱每次都要向IoC容器配置添加bean定义的苦恼。 context:component-scan在扫描相关类定义并将它们添加到容器的时候，会使用一种默认的命名规则，来生成那些添加到容器的bean定义的名称（beanName）。比如DowJonesNewsPersister通过默认命名规则将获得dowJonesNewsPersister作为bean定义名称。如果想改变这一默认行为，可以指定一个自定义的名称 你或许会觉得有些诧异，因为我们并没有使用context:annotation-config甚至直接将相应的BeanPostProcessor添加到容器中，而FXNewsProvider怎么会获得相应的依赖注入呢？这个得怪context:component-scan“多管闲事”，它同时将AutowiredAnnotationBeanPostProcessor和CommonAnnotationBeanPostProcessor一并注册到了容器中，所以，依赖注入的需求得以满足。如果你不喜欢，非要自己通过 context:annotation-config 或者直接添加相关 BeanPost\u0002Processor的方式来满足@Autowired或者@Resource的需求，可以将context:component-scan的annotation-config属性值从默认的true改为false。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"注解","slug":"注解","permalink":"http://example.com/tags/%E6%B3%A8%E8%A7%A3/"},{"name":"IOC","slug":"IOC","permalink":"http://example.com/tags/IOC/"}],"author":"John Doe"},{"title":"Autowired之外的选择——使用JSR250 标注依赖注入关系","slug":"Autowired之外的选择——使用JSR250-标注依赖注入关系","date":"2022-03-04T08:20:00.000Z","updated":"2022-03-04T08:22:53.133Z","comments":true,"path":"2022/03/04/Autowired之外的选择——使用JSR250-标注依赖注入关系/","link":"","permalink":"http://example.com/2022/03/04/Autowired%E4%B9%8B%E5%A4%96%E7%9A%84%E9%80%89%E6%8B%A9%E2%80%94%E2%80%94%E4%BD%BF%E7%94%A8JSR250-%E6%A0%87%E6%B3%A8%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5%E5%85%B3%E7%B3%BB/","excerpt":"","text":"除了可以使用Spring提供的@Autowired和@Qualifier来标注相应类定义之外，还可以使用JSR250的@Resource和@PostConstruct以及@PreDestroy对相应类进行标注，这同样可以达到依赖注入的目的。 @Resource与@Autowired不同，它遵循的是byName自动绑定形式的行为准则，也就是说，IoC容器将根据@Resource所指定的名称，到容器中查找beanName与之对应的实例，然后将查找到的对象实例注入给@Resource所标注的对象。 JSR250规定，如果@Resource标注于属性域或者方法之上的话，相应的容器将负责把指定的资源注入给当前对象，所以，除了像我们这样直接在属性域上标注@Resource，还可以在构造方法或者普通方法定义上标注@Resource，这与@Autowired能够存在的地方大致相同。 确切地说， 10 @PostConstruct和@PreDestroy不是服务于依赖注入的，它们主要用于标注对象生命周期管理相关方法，这与Spring的InitializingBean和DisposableBean接口，以及配置项中的init-method和destroy-method起到类似的作用。 如果想某个方法在对象实例化之后被调用，以做某些准备工作，或者想在对象销毁之前调用某个方法清理某些资源，那么就可以像我们这样，使用@PostConstruct和@PreDestroy来标注这些方法。当然，是使用@PostConstruct和@PreDestroy，还是使用Spring的InitializingBean和Disposable-Bean接口，或者init-method和destroy-method配置项，可以根据个人的喜好自己决定。 天上永远不会掉馅饼，我们只是使用@Resource或者@PostConstruct和@PreDestroy标注了相应对象，并不能给该对象带来想要的东西。所以，就像@Autowired需要AutowiredAnnotationBean\u0002PostProcessor为它与IoC容器牵线搭桥一样，JSR250的这些注解也同样需要一个BeanPost\u0002Processor帮助它们实现自身的价值。这个BeanPostProcessor就是org.springframework.context.annotation.CommonAnnotationBeanPostProcessor，只有将CommonAnnotationBeanPostProcessor添加到容器，JSR250的相关注解才能发挥作用 既然不管是@Autowired还是@Resource都需要添加相应的BeanPostProcessor到容器，那么我们就可以在基于XSD的配置文件中使用一个context:annotation-config配置搞定以上所有的BeanPostProcessor配置 context:annotation-config 不但帮我们把 AutowiredAnnotationBeanPostProcessor 和CommonAnnotationBeanPostProcessor注册到容器，同时还会把PersistenceAnnotationBeanPost\u0002Processor和RequiredAnnotationBeanPostProcessor一并进行注册，可谓一举四得啊！","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"容器","slug":"容器","permalink":"http://example.com/tags/%E5%AE%B9%E5%99%A8/"},{"name":"注解","slug":"注解","permalink":"http://example.com/tags/%E6%B3%A8%E8%A7%A3/"}],"author":"John Doe"},{"title":"Qualifier的陪伴","slug":"Qualifier的陪伴","date":"2022-03-04T08:19:00.000Z","updated":"2022-03-04T08:20:06.461Z","comments":true,"path":"2022/03/04/Qualifier的陪伴/","link":"","permalink":"http://example.com/2022/03/04/Qualifier%E7%9A%84%E9%99%AA%E4%BC%B4/","excerpt":"","text":"@Autowired是按照类型进行匹配，如果当前@Autowired标注的依赖在容器中只能找到一个实例与之对应的话，那还好。可是，要是能够同时找到两个或者多个同一类型的对象实例，又该怎么办呢？我们自己当然知道应该把具体哪个实例注入给当前对象，可是，IoC容器并不知道，所以，得通过某种方式告诉它。这时，就可以使用@Qualifier对依赖注入的条件做进一步限定，使得容器不再迷茫。 @Qualifier实际上是byName自动绑定的注解版，既然IoC容器无法自己从多个同一类型的实例中选取我们真正想要的那个，那么我们不妨就使用@Qualifier直接点名要哪个好了。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"注解","slug":"注解","permalink":"http://example.com/tags/%E6%B3%A8%E8%A7%A3/"}],"author":"John Doe"},{"title":"Spring基于注解的依赖注入","slug":"Spring基于注解的依赖注入","date":"2022-03-04T08:14:00.000Z","updated":"2022-03-04T08:18:55.251Z","comments":true,"path":"2022/03/04/Spring基于注解的依赖注入/","link":"","permalink":"http://example.com/2022/03/04/Spring%E5%9F%BA%E4%BA%8E%E6%B3%A8%E8%A7%A3%E7%9A%84%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/","excerpt":"","text":"@Autowired是基于注解的依赖注入的核心注解，它的存在可以让容器知道需要为当前类注入哪些依赖。比如可以使用@Autowired对类进行标注，以表明要为该类注入的依赖。 @Autowired也是按照类型匹配进行依赖注入的 @Autowired可以标注于类定义的多个位置，包括如下几个。 1、域（Filed）或者说属性（Property）。不管它们声明的访问限制符是private、protected还是public，只要标注了@Autowired，它们所需要的依赖注入需求就都能够被满足。 2、构造方法定义（Constructor）。标注于类的构造方法之上的@Autowired，相当于抢夺了原有自动绑定功能中“constructor”方式的权利，它将根据构造方法参数类型，来决定将什么样的依赖对象注入给当前对象。 3、方法定义（Method）。@Autowired不仅可以标注于传统的setter方法之上，而且还可以标注于任意名称的方法定义之上，只要该方法定义了需要被注入的参数。 现在，虽然可以随意地在类定义的各种合适的地方标注@Autowired，希望这些被@Autowired标注的依赖能够被注入，但是，仅将@Autowired标注于类定义中并不能让Spring的IoC容器聪明到自己去查看这些注解，然后注入符合条件的依赖对象。容器需要某种方式来了解，哪些对象标注了@Autowired，哪些对象可以作为可供选择的依赖对象来注入给需要的对象。在考虑使用什么方式实现这一功能之前，我们先比较一下原有的自动绑定功能与使用@Autowired之后产生了哪些差别。 使用自动绑定的时候，我们将所有对象相关的bean定义追加到了容器的配置文件中，然后使用default-autowire或者autowire告知容器，依照这两种属性指定的绑定方式，将容器中各个对象绑定到一起。在使用@Autowired之后，default-autowire或者autowire的职责就转给了@Autowired，所以，现在，容器的配置文件中就只剩下了一个个孤伶伶的bean定义 为了给容器中定义的每个bean定义对应的实例注入依赖，可以遍历它们，然后通过反射，检查每个bean定义对应的类上各种可能位置上的@Autowired。如果存在的话，就可以从当前容器管理的对象中获取符合条件的对象，设置给@Autowired所标注的属性域、构造方法或者方法定义。 我们可以提供一个Spring的IoC容器使用的BeanPostProcessor自定义实现，让这个BeanPostProcessor在实例化bean定义的过程中，来检查当前对象是否有@Autowired标注的依赖需要注入。org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor就是Spring提供的用于这一目的的BeanPostProcessor实现。所以，很幸运，我们不用自己去实现它了。 相关类定义使用@Autowired标注之后，只要在IoC容器的配置文件中追加Autowired\u0002AnnotationBeanPostProcessor就可以让整个应用开始运作了","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"注解","slug":"注解","permalink":"http://example.com/tags/%E6%B3%A8%E8%A7%A3/"}],"author":"John Doe"},{"title":"ApplicationContext统一资源加载策略","slug":"ApplicationContext统一资源加载策略","date":"2022-03-04T07:24:00.000Z","updated":"2022-03-04T07:55:24.970Z","comments":true,"path":"2022/03/04/ApplicationContext统一资源加载策略/","link":"","permalink":"http://example.com/2022/03/04/ApplicationContext%E7%BB%9F%E4%B8%80%E8%B5%84%E6%BA%90%E5%8A%A0%E8%BD%BD%E7%AD%96%E7%95%A5/","excerpt":"","text":"Spring框架内部使用org.springframework.core.io.Resource接口作为所有资源的抽象和访问接口 其中ClassPathResource就是Resource的一个特定类型的实现，代表的是位于Classpath中的资源。 Resource接口可以根据资源的不同类型，或者资源所处的不同场合，给出相应的具体实现。Spring框架在这个理念的基础上，提供了一些实现类（可以在org.springframework.core.io包下找到这些实现类）。  ByteArrayResource。将字节（byte）数组提供的数据作为一种资源进行封装，如果通过InputStream形式访问该类型的资源，该实现会根据字节数组的数据，构造相应的ByteArray\u0002InputStream并返回。  ClassPathResource。该实现从Java应用程序的ClassPath中加载具体资源并进行封装，可以使用指定的类加载器（ClassLoader）或者给定的类进行资源加载。  FileSystemResource。对java.io.File类型的封装，所以，我们可以以文件或者URL的形式对该类型资源进行访问，只要能跟File打的交道，基本上跟FileSystemResource也可以。  UrlResource。通过java.net.URL进行的具体资源查找定位的实现类，内部委派URL进行具体的资源操作。  InputStreamResource。将给定的InputStream视为一种资源的Resource实现类，较为少用。可能的情况下，以ByteArrayResource以及其他形式资源实现代之。 如果给定的实现类不满足需求，还可以通过实现Resource接口自定义。 org.spr\u0002ingframework.core.io.ResourceLoader接口是资源查找定位策略的统一抽象，具体的资源查找定位策略则由相应的ResourceLoader实现类给出。 其中最主要的就是Resource getResource(String location);方法，通过它，我们就可以根据指定的资源位置，定位到具体的资源实例。 1、可用的ResourceLoader： ResourceLoader有一个默认的实现类，即org.springframework.core.io.DefaultResource\u0002Loader，该类默认的资源查找处理逻辑如下。 (1) 首先检查资源路径是否以classpath:前缀打头，如果是，则尝试构造ClassPathResource类型资源并返回。 (2) 否则，(a) 尝试通过URL，根据资源路径来定位资源，如果没有抛出MalformedURLException，有则会构造UrlResource类型的资源并返回；(b)如果还是无法根据资源路径定位指定的资源，则委派getResourceByPath(String) 方法来定位， DefaultResourceLoader 的getResourceByPath(String)方法默认实现逻辑是，构造ClassPathResource类型的资源并返回。 为了避免DefaultResourceLoader在最后getResourceByPath(String)方法上的不恰当处理，我们可以使用org.springframework.core.io.FileSystemResourceLoader，它继承自Default\u0002ResourceLoader，但覆写了getResourceByPath(String)方法，使之从文件系统加载资源并以FileSystemResource类型返回。这样，我们就可以取得预想的资源类型。 FileSystemResourceLoader在ResourceLoader家族中的兄弟FileSystemXmlApplication\u0002Context，也是覆写了getResourceByPath(String)方法的逻辑，以改变DefaultResourceLoader的默认资源加载行为，最终从文件系统中加载并返回FileSystemResource类型的资源。 2、 ResourcePatternResolver ——批量查找的ResourceLoader： ResourcePatternResolver是ResourceLoader的扩展，ResourceLoader每次只能根据资源路径返回确定的单个Resource实例，而ResourcePatternResolver则可以根据指定的资源路径匹配模式，每次返回多个Resource实例。 ResourcePatternResolver在继承ResourceLoader原有定义的基础上，又引入了Resource[]getResources(String)方法定义，以支持根据路径匹配模式返回多个Resources的功能。它同时还引入了一种新的协议前缀classpath*:，针对这一点的支持，将由相应的子类实现给出。 ResourcePatternResolver最常用的一个实现是org.springframework.core.io.support.PathMatchingResourcePatternResolver，该实现类支持ResourceLoader级别的资源加载，支持基于Ant风格的路径匹配模式（类似于**/.suffix之类的路径形式），支持ResourcePatternResolver新增加的classpath:前缀等，基本上集所有技能于一身。 在构造PathMatchingResourcePatternResolver实例的时候，可以指定一个ResourceLoader，如果不指定的话，则PathMatchingResourcePatternResolver内部会默认构造一个Default\u0002ResourceLoader实例。PathMatchingResourcePatternResolver内部会将匹配后确定的资源路径，委派给它的ResourceLoader来查找和定位资源。这样，如果不指定任何ResourceLoader的话，Path\u0002MatchingResourcePatternResolver在加载资源的行为上会与DefaultResourceLoader基本相同，只存在返回的Resource数量上的差异。 不过，可以通过传入其他类型的ResourceLoader来替换PathMatchingResourcePatternResolver内部默认使用的DefaultResourceLoader，从而改变其默认行为。 ApplicationContext继承了ResourcePatternResolver，当然就间接实现了ResourceLoader接口。所以，任何的ApplicationContext实现都可以看作是一个ResourceLoader甚至ResourcePatternResolver。而这就是ApplicationContext支持Spring内统一资源加载策略的真相。 通常，所有的ApplicationContext实现类会直接或者间接地继承org.springframework.context.support.AbstractApplicationContext，从这个类上，我们就可以看到Application\u0002Context与ResourceLoader之间的所有关系。AbstractApplicationContext继承了DefaultRe\u0002sourceLoader，那么，它的getResource(String)当然就直接用DefaultResourceLoader的了。剩下需要它“效劳”的，就是ResourcePatternResolver的Resource[]getResources (String)，当然，AbstractApplicationContext也不负众望，当即拿下。AbstractApplicationContext类的内部声明有一个resourcePatternResolver，类型是ResourcePatternResolver，对应的实例类型为PathMatchingResourcePatternResolver 。之前我们说过 PathMatchingResourcePattern\u0002Resolver构造的时候会接受一个ResourceLoader，而AbstractApplicationContext本身又继承自DefaultResourceLoader，当然就直接把自身给“贡献”了。这样，整个ApplicationContext的实现类就完全可以支持ResourceLoader或者ResourcePatternResolver接口，你能说Application\u0002Context不支持Spring的统一资源加载吗？说白了，ApplicationContext的实现类在作为Resource\u0002Loader或者ResourcePatternResolver时候的行为，完全就是委派给了PathMatchingResource\u0002PatternResolver和DefaultResourceLoader来做。 1、既然ApplicationContext可以作为ResourceLoader或者ResourcePatternResolver来使用，那么，很显然，我们可以通过ApplicationContext来加载任何Spring支持的Resource类型。与直接使用ResourceLoader来做这些事情相比，很明显，ApplicationContext的表现过于“谦虚”了。 2、ApplicationContext容器本身就是一个ResourceLoader，我们为了该类还需要单独提供一个resourceLoader实例就有些多于了，直接将当前的ApplicationContext容器作为Resource\u0002Loader注入不就行了？而ResourceLoaderAware和ApplicationContextAware接口正好可以帮助我们做到这一点，只不过现在的FooBar需要依赖于Spring的API了。不过，在我看来，这没有什么大不了，因为我们从来也没有真正逃脱过依赖（这种依赖也好，那种依赖也罢）。 3、容器可以将bean定义文件中的字符串形式表达的信息，正确地转换成具体对象定义的依赖类型。对于那些Spring容器提供的默认的PropertyEditors无法识别的对象类型，我们可以提供自定义的PropertyEditor实现并注册到容器中，以供容器做类型转换的时候使用。默认情况下，BeanFactory容器不会为org.springframework.core.io.Resource类型提供相应的Property\u0002Editor，所以，如果我们想注入Resource类型的bean定义，就需要注册自定义的PropertyEditor到BeanFactory容器。不过，对于ApplicationContext来说，我们无需这么做，因为Application\u0002Context容器可以正确识别Resource类型并转换后注入相关对象。 4、特定的 10 ApplicationContext容器实现，在作为ResourceLoader加载资源时，会有其特定的行为。我们下面主要讨论两种类型的ApplicationContext容器，即ClassPathXmlApplicationContext和FileSystemXmlApplicationContext。其他类型的ApplicationContext容器，会在稍后章节中提到。 11我们知道，对于URL所接受的资源路径来说，通常开始都会有一个协议前缀，比如file:、http:、ftp:等。既然Spring使用UrlResource对URL定位查找的资源进行了抽象，那么，同样也支持这样类型的资源路径，而且，在这个基础上，Spring还扩展了协议前缀的集合。ResourceLoader中增加了一种新的资源路径协议——classpath:，ResourcePatternResolver又增加了一种——classpath*:。这样，我们就可以通过这些资源路径协议前缀，明确地告知Spring容器要从classpath中加载资源 当ClassPathXmlApplicationContext在实例化的时候，即使没有指明classpath:或者classpath*:等前缀，它会默认从classpath中加载bean定义配置文件 而FileSystemXmlApplicationContext则有些不同，如果我们像如下代码那样指定conf/appContext.xml，它会尝试从文件系统中加载bean定义文件 不过，我们可以像如下代码所示，通过在资源路径之前增加classpath:前缀，明确指定FileSystemXmlApplicationContext从classpath中加载bean定义的配置文件","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"资源加载","slug":"资源加载","permalink":"http://example.com/tags/%E8%B5%84%E6%BA%90%E5%8A%A0%E8%BD%BD/"},{"name":"容器","slug":"容器","permalink":"http://example.com/tags/%E5%AE%B9%E5%99%A8/"}],"author":"John Doe"},{"title":"Spring IoC容器 ApplicationContext","slug":"Spring-IoC容器-ApplicationContext","date":"2022-03-04T07:21:00.000Z","updated":"2022-03-04T07:23:24.352Z","comments":true,"path":"2022/03/04/Spring-IoC容器-ApplicationContext/","link":"","permalink":"http://example.com/2022/03/04/Spring-IoC%E5%AE%B9%E5%99%A8-ApplicationContext/","excerpt":"","text":"作为Spring提供的较之BeanFactory更为先进的IoC容器实现，ApplicationContext除了拥有BeanFactory支持的所有功能之外，还进一步扩展了基本容器的功能，包括BeanFactoryPostProces\u0002sor、BeanPostProcessor以及其他特殊类型bean的自动识别、容器启动后bean实例的自动初始化、国际化的信息支持、容器内事件发布等。 常见的ApplicationContext实现类有 org.springframework.context.support.FileSystemXmlApplicationContext。在默认情况下，从文件系统加载bean定义以及相关资源的ApplicationContext实现。 org.springframework.context.support.ClassPathXmlApplicationContext。在默认情况下，从Classpath加载bean定义以及相关资源的ApplicationContext实现。 org.springframework.web.context.support.XmlWebApplicationContext。Spring提供的用于Web应用程序的ApplicationContext实现","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"容器","slug":"容器","permalink":"http://example.com/tags/%E5%AE%B9%E5%99%A8/"}],"author":"John Doe"},{"title":"了解Spring中bean的一生","slug":"了解Spring中bean的一生","date":"2022-03-04T06:10:00.000Z","updated":"2022-03-04T06:52:30.847Z","comments":true,"path":"2022/03/04/了解Spring中bean的一生/","link":"","permalink":"http://example.com/2022/03/04/%E4%BA%86%E8%A7%A3Spring%E4%B8%ADbean%E7%9A%84%E4%B8%80%E7%94%9F/","excerpt":"","text":"容器启动之后，并不会马上就实例化相应的bean定义。我们知道，容器现在仅仅拥有所有对象的BeanDefinition来保存实例化阶段将要用的必要信息。只有当请求方通过BeanFactory的getBean()方法来请求某个对象实例的时候，才有可能触发Bean实例化阶段的活动。BeanFactory的getBean（）法可以被客户端对象显式调用，也可以在容器内部隐式地被调用。隐式调用有如下两种情况。  对于BeanFactory来说，对象实例化默认采用延迟初始化。通常情况下，当对象A被请求而需要第一次实例化的时候，如果它所依赖的对象B之前同样没有被实例化，那么容器会先实例化对象A所依赖的对象。这时容器内部就会首先实例化对象B，以及对象 A依赖的其他还没有实例化的对象。这种情况是容器内部调用getBean()，对于本次请求的请求方是隐式的。  ApplicationContext启动之后会实例化所有的bean定义，但ApplicationContext在实现的过程中依然遵循Spring容器实现流程的两个阶段，只不过它会在启动阶段的活动完成之后，紧接着调用注册到该容器的所有bean定义的实例化方法getBean()。这就是为什么当你得到ApplicationContext类型的容器引用时，容器内所有对象已经被全部实例化完成。不信你查一下类org.AbstractApplicationContext的refresh()方法。 1、 Bean的实例化与 BeanWrapper：容器在内部实现的时候，采用“策略模式（Strategy Pattern）”来决定采用何种方式初始化bean实例。通常，可以通过反射或者CGLIB动态字节码生成来初始化相应的bean实例或者动态生成其子类。 org.springframework.beans.factory.support.InstantiationStrategy定义是实例化策略的抽象接口，其直接子类SimpleInstantiationStrategy实现了简单的对象实例化功能，可以通过反射来实例化对象实例，但不支持方法注入方式的对象实例化。CglibSubclassingInstantiation\u0002Strategy继承了SimpleInstantiationStrategy的以反射方式实例化对象的功能，并且通过CGLIB的动态字节码生成功能，该策略实现类可以动态生成某个类的子类，进而满足了方法注入所需的对象实例化需求。默认情况下，容器内部采用的是CglibSubclassingInstantiationStrategy。 容器只要根据相应bean定义的BeanDefintion取得实例化信息，结合CglibSubclassingIns\u0002tantiationStrategy以及不同的bean定义类型，就可以返回实例化完成的对象实例。但是，返回方式上有些“点缀”。不是直接返回构造完成的对象实例，而是以BeanWrapper对构造完成的对象实例进行包裹，返回相应的BeanWrapper实例。 BeanWrapper接口通常在Spring框架内部使用，它有一个实现类org.springframework.beans.BeanWrapperImpl。其作用是对某个bean进行“包裹”，然后对这个“包裹”的bean进行操作，比如设置或者获取bean的相应属性值。而在第一步结束后返回BeanWrapper实例而不是原先的对象实例，就是为了第二步“设置对象属性”。 BeanWrapper定义继承了org.springframework.beans.PropertyAccessor接口，可以以统一的方式对对象属性进行访问；BeanWrapper定义同时又直接或者间接继承了PropertyEditorRegistry和TypeConverter接口。不知你是否还记得CustomEditorConfigurer？当把各种PropertyEditor注册给容器时，知道后面谁用到这些PropertyEditor吗？对，就是BeanWrapper！在第一步构造完成对象之后，Spring会根据对象实例构造一个BeanWrapperImpl实例，然后将之前CustomEditor\u0002Configurer注册的PropertyEditor复制一份给BeanWrapperImpl实例（这就是BeanWrapper同时又是PropertyEditorRegistry的原因）。这样，当BeanWrapper转换类型、设置对象属性值时，就不会无从下手了。 2、 各色的Aware接口： 当对象实例化完成并且相关属性以及依赖设置完成之后，Spring容器会检查当前对象实例是否实现了一系列的以Aware命名结尾的接口定义。如果是，则将这些Aware接口定义中规定的依赖注入给当前对象实例。 3、 BeanPostProcessor BeanPostProcessor的概念容易与BeanFactoryPostProcessor的概念混淆。但只要记住Bean\u0002PostProcessor是存在于对象实例化阶段，而BeanFactoryPostProcessor则是存在于容器启动阶段，这两个概念就比较容易区分了。 与BeanFactoryPostProcessor通常会处理容器内所有符合条件的BeanDefinition类似，Bean\u0002PostProcessor会处理容器内所有符合条件的实例化后的对象实例。该接口声明了两个方法，分别在两个不同的时机执行。postProcessBeforeInitialization()方法是BeanPostProcessor前置处理这一步将会执行的方法，postProcessAfterInitialization()则是对应BeanPostProcessor后置处理那一步将会执行的方法。BeanPostProcessor的两个方法中都传入了原来的对象实例的引用，这为我们扩展容器的对象实例化过程中的行为提供了极大的便利，我们几乎可以对传入的对象实例执行任何的操作。 通常比较常见的使用BeanPostProcessor的场景，是处理标记接口实现类，或者为当前对象提供代理实现。除了检查标记接口以便应用自定义逻辑，还可以通过BeanPostProcessor对当前对象实例做更多的处理。比如替换当前对象实例或者字节码增强当前对象实例等。Spring的AOP则更多地使用BeanPostProcessor来为对象生成相应的代理对象，如org.springframework.aop.framework.autoproxy.BeanNameAutoProxyCreator。 4、 InitializingBean和init-method： org.springframework.beans.factory.InitializingBean是容器内部广泛使用的一个对象生命周期标识接口 该接口定义很简单，其作用在于，在对象实例化过程调用过“BeanPostProcessor的前置处理”之后，会接着检测当前对象是否实现了InitializingBean接口，如果是，则会调用其afterProper\u0002tiesSet()方法进一步调整对象实例的状态。比如，在有些情况下，某个业务对象实例化完成后，还不能处于可以使用状态。这个时候就可以让该业务对象实现该接口，并在方法afterPropertiesSet()中完成对该业务对象的后续处理。 虽然该接口在Spring容器内部广泛使用，但如果真的让我们的业务对象实现这个接口，则显得Spring容器比较具有侵入性。所以，Spring还提供了另一种方式来指定自定义的对象初始化操作，那就是在XML配置的时候，使用的init-method属性。 通过init-method，系统中业务对象的自定义初始化操作可以以任何方式命名，而不再受制于InitializingBean的afterPropertiesSet()。如果系统开发过程中规定：所有业务对象的自定义初始化操作都必须以init()命名，为了省去挨个的设置init-method这样的烦琐，我们还可以通过最顶层的的default-init-method统一指定这一init()方法名。 5、 DisposableBean与destroy-method： 当所有的一切，该设置的设置，该注入的注入，该调用的调用完成之后，容器将检查singleton类型的bean实例，看其是否实现了org.springframework.beans.factory.DisposableBean接口。或者其对应的bean定义是否通过的destroy-method属性指定了自定义的对象销毁方法。如果是，就会为该实例注册一个用于对象销毁的回调（Callback），以便在这些singleton类型的对象实例销毁之前，执行销毁逻辑。 与InitializingBean和init-method用于对象的自定义初始化相对应，DisposableBean和destroy-method为对象提供了执行自定义销毁逻辑的机会。 最常见到的该功能的使用场景就是在Spring容器中注册数据库连接池，在系统退出后，连接池应该关闭，以释放相应资源。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"bean","slug":"bean","permalink":"http://example.com/tags/bean/"}],"author":"John Doe"},{"title":"插手“容器的启动”","slug":"插手“容器的启动”","date":"2022-03-04T02:33:00.000Z","updated":"2022-03-04T06:09:45.853Z","comments":true,"path":"2022/03/04/插手“容器的启动”/","link":"","permalink":"http://example.com/2022/03/04/%E6%8F%92%E6%89%8B%E2%80%9C%E5%AE%B9%E5%99%A8%E7%9A%84%E5%90%AF%E5%8A%A8%E2%80%9D/","excerpt":"","text":"Spring提供了一种叫做BeanFactoryPostProcessor的容器扩展机制。该机制允许我们在容器实例化相应对象之前，对注册到容器的BeanDefinition所保存的信息做相应的修改。这就相当于在容器实现的第一阶段最后加入一道工序，让我们对最终的BeanDefinition做一些额外的操作，比如修改其中bean定义的某些属性，为bean定义增加其他信息等。 如果要自定义实现BeanFactoryPostProcessor，通常我们需要实org.springframework.beans.factory.config.BeanFactoryPostProcessor接口。同时，因为一个容器可能拥有多个Bean\u0002FactoryPostProcessor，这个时候可能需要实现类同时实现Spring的org.springframework.core.Ordered接口，以保证各个BeanFactoryPostProcessor可以按照预先设定的顺序执行（如果顺序紧要的话）。但是，因为Spring已经提供了几个现成的BeanFactoryPostProcessor实现类，所以，大多时候，我们很少自己去实现某个BeanFactoryPostProcessor。其中，org.springframework.beans.factory.config.PropertyPlaceholderConfigurer和org.springframework.beans.factory. config.Property OverrideConfigurer是两个比较常用的BeanFactoryPostProcessor。另外，为了处理配置文件中的数据类型与真正的业务对象所定义的数据类型转换，Spring还允许我们通过org.springframework.beans.factory.config.CustomEditorConfigurer来注册自定义的Pro\u0002pertyEditor以补助容器中默认的PropertyEditor。可以参考BeanFactoryPostProcessor的Javadoc来了解更多其实现子类的情况。 对于BeanFactory来说，我们需要用手动方式应用所有的BeanFactoryPostProcessor 对于ApplicationContext来说，情况看起来要好得多。因为ApplicationContext会自动识别配置文件中的BeanFactoryPostProcessor并应用它，所以，相对于BeanFactory，在ApplicationContext中加载并应用BeanFactoryPostProcessor，仅需要在XML配置文件中将这些BeanFactoryPost\u0002Processor简单配置一下即可。 1、PropertyPlaceholderConfigurer： 通常情况下，我们不想将类似于系统管理相关的信息同业务对象相关的配置信息混杂到XML配置文件中，以免部署或者维护期间因为改动繁杂的XML配置文件而出现问题。我们会将一些数据库连接信息、邮件服务器等相关信息单独配置到一个properties文件中，这样，如果因系统资源变动的话，只需要关注这些简单properties配置文件即可。PropertyPlaceholderConfigurer允许我们在XML配置文件中使用占位符（PlaceHolder），并将这些占位符所代表的资源单独配置到简单的properties文件中来加载。 基本机制就是之前所说的那样。当BeanFactory在第一阶段加载完成所有配置信息时，BeanFactory中保存的对象的属性信息还只是以占位符的形式存在，如${jdbc.url}、${jdbc.driver}。当PropertyPlaceholderConfigurer作为BeanFactoryPostProcessor被应用时，它会使用properties配置文件中的配置信息来替换相应BeanDefinition中占位符所表示的属性值。这样，当进入容器实现的第二阶段实例化bean时，bean定义中的属性值就是最终替换完成的了。PropertyPlaceholderConfigurer不单会从其配置的properties文件中加载配置项，同时还会检查Java的System类中的Properties，可以通过setSystemPropertiesMode()或者setSystemProper\u0002tiesModeName()来控制是否加载或者覆盖System相应Properties的行为。PropertyPlaceholder\u0002Configurer提供了SYSTEM_PROPERTIES_MODE_FALLBACK、SYSTEM_PROPERTIES_MODE_NEVER和SYSTEM_PROPERTIES_MODE_OVERRIDE三种模式。默认采用的是SYSTEM_PROPERTIES_ MODE_FALLBACK，即如果properties文件中找不到相应配置项，则到System的Properties中查找，我们还可以选择不检查System的Properties或者覆盖它。 2、 CustomEditorConfigurer：我们知道，不管对象是什么类型，也不管这些对象所声明的依赖对象是什么类型，通常都是通过XML（或者properties甚至其他媒介）文件格式来配置这些对象类型。但XML所记载的，都是String类型，即容器从XML格式的文件中读取的都是字符串形式，最终应用程序却是由各种类型的对象所构成。要想完成这种由字符串到具体对象的转换（不管这个转换工作最终由谁来做），都需要这种转换规则相关的信息，而CustomEditorConfigurer就是帮助我们传达类似信息的。 Spring内部通过JavaBean的PropertyEditor来帮助进行String类型到其他类型的转换工作。只要为每种对象类型提供一个 PropertyEditor ，就可以根据该对象类型取得与其相对应的PropertyEditor来做具体的类型转换。Spring容器内部在做具体的类型转换的时候，会采用JavaBean框架内默认的PropertyEditor搜寻逻辑，从而继承了对原生类型以及java.lang.String.java.awt.Color和java.awt.Font等类型的转换支持。同时，Spring框架还提供了自身实现的一些Property\u0002Editor，这些PropertyEditor大部分都位于org.springframework. beans.propertyeditors包下。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"容器","slug":"容器","permalink":"http://example.com/tags/%E5%AE%B9%E5%99%A8/"}],"author":"John Doe"},{"title":"Spring容器背后的秘密","slug":"Spring容器背后的秘密","date":"2022-03-04T02:22:00.000Z","updated":"2022-03-04T02:34:21.122Z","comments":true,"path":"2022/03/04/Spring容器背后的秘密/","link":"","permalink":"http://example.com/2022/03/04/Spring%E5%AE%B9%E5%99%A8%E8%83%8C%E5%90%8E%E7%9A%84%E7%A7%98%E5%AF%86/","excerpt":"","text":"Spring的IOC容器它会以某种方式加载Configuration Metadata（通常也就是XML格式的配置信息），然后根据这些信息绑定整个系统的对象，最终组装成一个可用的基于轻量级容器的应用系统。 Spring的IOC容器在实现上述过程中可以分为两个阶段：容器启动阶段和Bean实例化阶段。 Spring的IoC容器在实现的时候，充分运用了这两个实现阶段的不同特点，在每个阶段都加入了相应的容器扩展点，以便我们可以根据具体场景的需要加入自定义的扩展逻辑。 1、容器启动阶段：容器启动伊始，首先会通过某种途径加载Configuration MetaData。除了代码方式比较直接，在大部分情况下，容器需要依赖某些工具类（BeanDefinitionReader）对加载的Configuration MetaData进行解析和分析，并将分析后的信息编组为相应的BeanDefinition，最后把这些保存了bean定义必要信息的BeanDefinition，注册到相应的BeanDefinitionRegistry，这样容器启动工作就完成了。 总地来说，该阶段所做的工作可以认为是准备性的，重点更加侧重于对象管理信息的收集。当然，一些验证性或者辅助性的工作也可以在这个阶段完成。 2、 Bean实例化阶段：经过第一阶段，现在所有的bean定义信息都通过BeanDefinition的方式注册到了BeanDefinitionRegistry中。当某个请求方通过容器的getBean方法明确地请求某个对象，或者因依赖关系容器需要隐式地调用getBean方法时，就会触发第二阶段的活动。该阶段，容器会首先检查所请求的对象之前是否已经初始化。如果没有，则会根据注册的BeanDefinition所提供的信息实例化被请求对象，并为其注入依赖。如果该对象实现了某些回调接口，也会根据回调接口的要求来装配它。当该对象装配完毕之后，容器会立即将其返回请求方使用。 如果说第一阶段只是根据图纸装配生产线的话，那么第二阶段就是使用装配好的生产线来生产具体的产品了。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"IOC","slug":"IOC","permalink":"http://example.com/tags/IOC/"}],"author":"John Doe"},{"title":"bean的scope的使用陷阱","slug":"bean的scope的使用陷阱","date":"2022-03-03T15:48:00.000Z","updated":"2022-03-03T16:01:27.658Z","comments":true,"path":"2022/03/03/bean的scope的使用陷阱/","link":"","permalink":"http://example.com/2022/03/03/bean%E7%9A%84scope%E7%9A%84%E4%BD%BF%E7%94%A8%E9%99%B7%E9%98%B1/","excerpt":"","text":"我们知道，拥有prototype类型scope的bean，在请求方每次向容器请求该类型对象的时候，容器都会返回一个全新的该对象实例。但是对于在类A中定义成员变量类B，并且通过setter注入类B，并getter返回类B时，会存在每次返回的对象都是同一个对象。 原因在于：虽然A拥有prototype类型的scope，但当容器将一个B的实例注入A之后，A就会一直持有这个FXNewsBean实例的引用。虽然每次输出都调用了getNewsBean()方法并返回了 FXNewsBean 的实例，但实际上每次返回的都是A持有的容器第一次注入的实例。这就是问题之所在。换句话说，第一个实例注入后，A再也没有重新向容器申请新的实例。所以，容器也不会重新为其注入新的B类型的实例。 解决的方案就在于保证get方法每次从容器中取得新的B实例，而不是每次都返回其持有的单一实例。 1、方法注入：Spring容器提出了一种叫做方法注入（Method Injection）的方式，可以帮助我们解决上述问题。我们所要做的很简单，只要让getNewsBean方法声明符合规定的格式，并在配置文件中通知容器，当该方法被调用的时候，每次返回指定类型的对象实例即可。也就是说，该方法必须能够被子类实现或者覆写，因为容器会为我们要进行方法注入的对象使用Cglib动态生成一个子类实现，从而替代当前对象。 2、使用BeanFactoryAware接口：我们知道，即使没有方法注入，只要在实现 get方法的时候，能够保证每次调用BeanFactory的getBean(“newsBean”)，就同样可以每次都取得新的FXNewsBean对象实例。Spring框架提供了一个BeanFactoryAware接口，容器在实例化实现了该接口的bean定义的过程中，会自动将容器本身注入该bean。这样，该bean就持有了它所处的BeanFactory的引用 3、 使用ObjectFactoryCreatingFactoryBean：ObjectFactoryCreatingFactoryBean是Spring提供的一个FactoryBean实现，它返回一个ObjectFactory实例。从ObjectFactoryCreatingFactoryBean返回的这个ObjectFactory实例可以为我们返回容器管理的相关对象。实际上， ObjectFactoryCreatingFactoryBean 实现了BeanFactoryAware接口，它返回的ObjectFactory实例只是特定于与Spring容器进行交互的一个实现而已。使用它的好处就是，隔离了客户端对象对BeanFactory的直接引用。 4、方法替换：与方法注入只是通过相应方法为主体对象注入依赖对象不同，方法替换更多体现在方法的实现层面上，它可以灵活替换或者说以新的方法实现覆盖掉原来某个方法的实现逻辑。基本上可以认为，方法替换可以帮助我们实现简单的方法拦截功能。 首先，我们需要给出org.springframework.beans.factory.support.MethodReplacer的实现类，在这个类中实现将要替换的方法逻辑。 有了要替换的逻辑之后，我们就可以把这个逻辑通过配置到FXNewsProv\u0002ider的bean定义中，使其生效。 最后需要强调的是，这种方式刚引入的时候执行效率不是很高。而且，当你充分了解并应用SpringAOP之后，我想你也不会再回头求助这个特色功能。不过，怎么说这也是一个选择，场景合适的话，为何不用呢？哦，如果要替换的方法存在参数，或者对象存在多个重载的方法，可以在内部通过明确指定将要替换的方法参数类型。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"scop","slug":"scop","permalink":"http://example.com/tags/scop/"}],"author":"John Doe"},{"title":"工厂方法与 FactoryBean","slug":"工厂方法与-FactoryBean","date":"2022-03-03T15:05:00.000Z","updated":"2022-03-03T15:31:28.499Z","comments":true,"path":"2022/03/03/工厂方法与-FactoryBean/","link":"","permalink":"http://example.com/2022/03/03/%E5%B7%A5%E5%8E%82%E6%96%B9%E6%B3%95%E4%B8%8E-FactoryBean/","excerpt":"","text":"在强调“面向接口编程”的同时，有一点需要注意：虽然对象可以通过声明接口来避免对特定接口实现类的过度耦合，但总归需要一种方式将声明依赖接口的对象与接口实现类关联起来。否则，只依赖一个不做任何事情的接口是没有任何用处的。 如果该类是由我们设计并开发的，那么还好说，我们可以通过依赖注入，让容器帮助我们解除接口与实现类之间的耦合性。但是，有时，我们需要依赖第三方库，需要实例化并使用第三方库中的相关类，这时，接口与实现类的耦合性需要其他方式来避免。 通常的做法是通过使用工厂方法（Factory Method）模式，提供一个工厂类来实例化具体的接口实现类，这样，主体对象只需要依赖工厂类，具体使用的实现类有变更的话，只是变更工厂类，而主体对象不需要做任何变动。 针对使用工厂方法模式实例化对象的方式，Spring的IoC容器同样提供了对应的集成支持。我们所要做的，只是将工厂类所返回的具体的接口实现类注入给主体对象 1、 静态工厂方法（Static Factory Method） 2、非静态工厂方法（Instance Factory Method） 3、FactoryBean：FactoryBean是Spring容器提供的一种可以扩展容器对象实例化逻辑的接口，请不要将其与容器名称BeanFactory相混淆。FactoryBean，其主语是Bean，定语为Factory，也就是说，它本身与其他注册到容器的对象一样，只是一个Bean而已，只不过，这种类型的Bean本身就是生产对象的工厂（Factory）。 当某些对象的实例化过程过于烦琐，通过XML配置过于复杂，使我们宁愿使用Java代码来完成这个实例化过程的时候，或者，某些第三方库不能直接注册到Spring容器的时候，就可以实现org.spring.framework.beans.factory.FactoryBean接口，给出自己的对象实例化逻辑代码。当然，不使用Fac.toryBean，而像通常那样实现自定义的工厂方法类也是可以的。不过，FactoryBean可是Spring提供的对付这种情况的“制式装备”哦！ Spring容器内部许多地方了使用FactoryBean。下面是一些比较常见的FactoryBean实现，你可以参照FactoryBean的Javadoc以了解更多内容。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"FactoryBean","slug":"FactoryBean","permalink":"http://example.com/tags/FactoryBean/"}],"author":"John Doe"},{"title":"bean的scope","slug":"bean的scope","date":"2022-03-03T14:23:00.000Z","updated":"2022-03-03T15:05:21.116Z","comments":true,"path":"2022/03/03/bean的scope/","link":"","permalink":"http://example.com/2022/03/03/bean%E7%9A%84scope/","excerpt":"","text":"BeanFactory除了拥有作为IoC Service Provider的职责，作为一个轻量级容器，它还有着其他一些职责，其中就包括对象的生命周期管理。 Spring容器最初提供了两种bean的scope类型：singleton和prototype，但发布2.0之后，又引入了另外三种scope类型，即request、session和global session类型。不过这三种类型有所限制，只能在Web应用中使用。也就是说，只有在支持Web应用的ApplicationContext中使用这三个scope才是合理的。 1、singleton：标记为拥有singleton scope的对象定义，在Spring的IoC容器中只存在一个实例，所有对该对象的引用将共享这个实例。该实例从容器启动，并因为第一次被请求而初始化之后，将一直存活到容器退出，也就是说，它与IoC容器“几乎”拥有相同的“寿命”。 （注意：需要注意的一点是，不要因为名字的原因而与GoF所提出的Singleton模式相混淆，二者的语意是不同的：标记为singleton的bean是由容器来保证这种类型的bean在同一个容器中只存在一个共享实例；而Singleton模式则是保证在同一个Classloader中只存在一个这种类型的实例。） 2、 prototype：针对声明为拥有prototype scope的bean定义，容器在接到该类型对象的请求的时候，会每次都重新生成一个新的对象实例给请求方。虽然这种类型的对象的实例化以及属性设置等工作都是由容器负责的，但是只要准备完毕，并且对象实例返回给请求方之后，容器就不再拥有当前返回对象的引用，请求方需要自己负责当前返回对象的后继生命周期的管理工作，包括该对象的销毁。也就是说，容器每次返回给请求方一个新的对象实例之后，就任由这个对象实例“自生自灭”了。 3、 request：Spring容器，即XmlWebApplicationContext会为每个HTTP请求创建一个全新的Request\u0002Processor对象供当前请求使用，当请求结束后，该对象实例的生命周期即告结束。当同时有10个HTTP请求进来的时候，容器会分别针对这10个请求返回10个全新的RequestProcessor对象实例，且它们之间互不干扰。从不是很严格的意义上说，request可以看作prototype的一种特例，除了场景更加具体之外，语意上差不多。 4、session：对于Web应用来说，放到session中的最普遍的信息就是用户的登录信息，对于这种放到session中的信息，我们可使用如下形式指定其scope为session 5、global session：global session只有应用在基于portlet的Web应用程序中才有意义，它映射到portlet的global范围的 3session。如果在普通的基于servlet的Web应用中使用了这个类型的scope，容器会将其作为普通的session类型的scope对待。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"},{"name":"Spring ","slug":"Spring/Spring","permalink":"http://example.com/categories/Spring/Spring/"}],"tags":[{"name":"scope","slug":"scope","permalink":"http://example.com/tags/scope/"}],"author":"John Doe"},{"title":"Spring的IOC容器之BeanFactory","slug":"Spring的IOC容器之BeanFactory","date":"2022-03-03T13:21:00.000Z","updated":"2022-03-03T13:56:15.460Z","comments":true,"path":"2022/03/03/Spring的IOC容器之BeanFactory/","link":"","permalink":"http://example.com/2022/03/03/Spring%E7%9A%84IOC%E5%AE%B9%E5%99%A8%E4%B9%8BBeanFactory/","excerpt":"","text":"BeanFactory，顾名思义，就是生产Bean的工厂。当然，严格来说，这个“生产过程”可能不像说起来那么简单。既然Spring框架提倡使用POJO，那么把每个业务对象看作一个JavaBean对象，或许更容易理解为什么Spring的IoC基本容器会起这么一个名字。作为Spring提供的基本的IoC容器，BeanFactory可以完成作为IoC Service Provider的所有职责，包括业务对象的注册和对象间依赖关系的绑定。 BeanFactory就像一个汽车生产厂。你从其他汽车零件厂商或者自己的零件生产部门取得汽车零件送入这个汽车生产厂，最后，只需要从生产线的终点取得成品汽车就可以了。相似地，将应用所需的所有业务对象交给BeanFactory之后，剩下要做的，就是直接从BeanFactory取得最终组装完成并且可用的对象。至于这个最终业务对象如何组装，你不需要关心，BeanFactory会帮你搞定。 所以，对于客户端来说，与BeanFactory打交道其实很简单。最基本地，BeanFactory肯定会公开一个取得组装完成的对象的方法接口，就像代码清单4-1中真正的BeanFactory的定义所展示的那样。 BeanFactory就像一个汽车生产厂。你从其他汽车零件厂商或者自己的零件生产部门取得汽车零件送入这个汽车生产厂，最后，只需要从生产线的终点取得成品汽车就可以了。相似地，将应用所需的所有业务对象交给BeanFactory之后，剩下要做的，就是直接从BeanFactory取得最终组装完成并且可用的对象。至于这个最终业务对象如何组装，你不需要关心，BeanFactory会帮你搞定。 当BeanFactory说这些事情让它来做的时候，可能没有告诉你它会怎么来做这个事情。不过没关系，我们通常只需将“生产线图纸”交给BeanFactory就行了。通常情况下，它会通过常用的图纸（XML文件）来注册并管理各个业务对象之间的依赖关系。 当然BeanFactory只是一个接口，我们最终需要一个该接口的实现来进行实际的Bean的管理，DefaultListableBeanFactory就是这么一个比较通用的BeanFactory实现类。 DefaultListableBeanFactory除了间接地实现了BeanFactory接口，还实现了BeanDefinitionRegistry接口，该接口才是在BeanFactory的实现中担当Bean注册管理的角色。基本上，BeanFactory接口只定义如何访问容器内管理的Bean的方法，各个BeanFactory的具体实现类负责具体Bean的注册以及管理工作。BeanDefinitionRegistry接口定义抽象了Bean的注册逻辑。通常情况下，具体的BeanFactory实现类会实现这个接口来管理Bean的注册。 每一个受管的对象，在容器中都会有一个BeanDefinition的实例（instance）与之相对应，该BeanDefinition的实例负责保存对象的所有必要信息，包括其对应的对象的class类型、是否是抽象类、构造方法参数以及其他属性等。当客户端向BeanFactory请求相应对象的时候，BeanFactory会通过这些信息为客户端返回一个完备可用的对象实例。RootBeanDefinition和ChildBean\u0002Definition是BeanDefinition的两个主要实现类。 采用外部配置文件时，Spring的IoC容器有一个统一的处理方式。通常情况下，需要根据不同的外部配置文件格式，给出相应的BeanDefinitionReader实现类，由BeanDefinitionReader的相应实现类负责将相应的配置文件内容读取并映射到BeanDefinition，然后将映射后的BeanDefinition注册到一个BeanDefinitionRegistry，之后，BeanDefinitionRegistry即完成Bean的注册和加载。当然，大部分工作，包括解析文件格式、装配BeanDefinition之类的工作，都是由BeanDefinition\u0002Reader的相应实现类来做的，BeanDefinitionRegistry只不过负责保管而已。 与为 Properties配置文件格式提供PropertiesBeanDefinitionReader相对应，Spring同样为XML格式的配置文件提供了现成的BeanDefinitionReader实现，即XmlBeanDefinitionReader。XmlBeanDefinitionReader负责读取Spring指定格式的XML配置文件并解析，之后将解析后的文件内容映射到相应的BeanDefinition，并加载到相应的BeanDefinitionRegistry中（在这里是Default\u0002ListableBeanFactory）。这时，整个BeanFactory就可以放给客户端使用了。除了提供XmlBeanDefinitionReader用于XML格式配置文件的加载，Spring还在Default\u0002ListableBeanFactory的基础上构建了简化XML格式配置加载的XmlBeanFactory实现。 如果要通过注解标注的方式为类注入所需要的依赖，现在可以使用@Autowired以 及@Component等对相关类进行标记。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"BeanFactory","slug":"BeanFactory","permalink":"http://example.com/tags/BeanFactory/"}],"author":"John Doe"},{"title":"Spring的IOC容器","slug":"Spring的IOC容器","date":"2022-03-03T13:11:00.000Z","updated":"2022-03-03T13:20:31.109Z","comments":true,"path":"2022/03/03/Spring的IOC容器/","link":"","permalink":"http://example.com/2022/03/03/Spring%E7%9A%84IOC%E5%AE%B9%E5%99%A8/","excerpt":"","text":"Spring的IoC容器是一个IoC Service Provider，但是，这只是它被冠以IoC之名的部分原因，我们不能忽略的是“容器”。Spring的IoC容器是一个提供IoC支持的轻量级容器，除了基本的IoC支持，它作为轻量级容器还提供了IoC之外的支持。如在Spring的IoC容器之上，Spring还提供了相应的AOP框架支持、企业级服务集成等服务。Spring的IoC容器和IoCService Provider所提供的服务之间存在一定的交集。 Spring提供了两种容器类型：BeanFactoryApplicationContext  BeanFactory。基础类型IoC容器，提供完整的IoC服务支持。如果没有特殊指定，默认采用延迟初始化策略（lazy-load）。只有当客户端对象需要访问容器中的某个受管对象的时候，才对该受管对象进行初始化以及依赖注入操作。所以，相对来说，容器启动初期速度较快，所需要的资源有限。对于资源有限，并且功能要求不是很严格的场景，BeanFactory是比较合适的IoC容器选择。  ApplicationContext在BeanFactory的基础上构建，是相对比较高级的容器实现，除了拥有BeanFactory的所有支持，ApplicationContext还提供了其他高级特性，比如事件发布、国际化信息支持等。ApplicationContext所管理的对象，在该类型容器启动之后，默认全部初始化并绑定完成。所以，相对于BeanFactory来说，ApplicationContext要求更多的系统资源，同时，因为在启动时就完成所有初始化，容器启动时间较之BeanFactory也会长一些。在那些系统资源充足，并且要求更多功能的场景中，ApplicationContext类型的容器是比较合适的选择。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"IOC","slug":"IOC","permalink":"http://example.com/tags/IOC/"}],"author":"John Doe"},{"title":"—IoC Service Provider 如何管理对象间的 依赖关系","slug":"—IoC-Service-Provider-如何管理对象间的-依赖关系","date":"2022-03-03T10:30:00.000Z","updated":"2022-03-03T13:11:08.016Z","comments":true,"path":"2022/03/03/—IoC-Service-Provider-如何管理对象间的-依赖关系/","link":"","permalink":"http://example.com/2022/03/03/%E2%80%94IoC-Service-Provider-%E5%A6%82%E4%BD%95%E7%AE%A1%E7%90%86%E5%AF%B9%E8%B1%A1%E9%97%B4%E7%9A%84-%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB/","excerpt":"","text":"IoC Service Provider不是人类，也就不能像酒吧服务生那样通过大脑来记忆和存储所有的相关信息。所以，它需要寻求其他方式来记录诸多对象之间的对应关系。比如：  它可以通过最基本的文本文件来记录被注入对象和其依赖对象之间的对应关系；  它也可以通过描述性较强的XML文件格式来记录对应信息；  它还可以通过编写代码的方式来注册这些对应信息；  甚至，如果愿意，它也可以通过语音方式来记录对象间的依赖注入关系（“嗨，它要一个这种类型的对象，拿这个给它”）。 那么，实际情况下，各种具体的IoC Service Provider实现又是通过哪些方式来记录“服务信息”的呢？我们可以归纳一下，当前流行的 IoC Service Provider产品使用的注册对象管理信息的方式主要有以下几种。 1、直接编码方式 2、配置文件方式 3、元数据方式","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"IOC","slug":"IOC","permalink":"http://example.com/tags/IOC/"}],"author":"John Doe"},{"title":"IoC Service Provider 的职责","slug":"IoC-Service-Provider-的职责","date":"2022-03-03T10:27:00.000Z","updated":"2022-03-03T10:28:50.333Z","comments":true,"path":"2022/03/03/IoC-Service-Provider-的职责/","link":"","permalink":"http://example.com/2022/03/03/IoC-Service-Provider-%E7%9A%84%E8%81%8C%E8%B4%A3/","excerpt":"","text":"IoC Service Provider的职责相对来说比较简单，主要有两个：业务对象的构建管理和业务对象间的依赖绑定。  业务对象的构建管理。在IoC场景中，业务对象无需关心所依赖的对象如何构建如何取得，但这部分工作始终需要有人来做。所以，IoC Service Provider需要将对象的构建逻辑从客户端对象那里剥离出来，以免这部分逻辑污染业务对象的实现。  业务对象间的依赖绑定。对于IoC Service Provider来说，这个职责是最艰巨也是最重要的，这是它的最终使命之所在。如果不能完成这个职责，那么，无论业务对象如何的“呼喊”，也不会得到依赖对象的任何响应（最常见的倒是会收到一个NullPointerException）。IoC Service Provider通过结合之前构建和管理的所有业务对象，以及各个业务对象间可以识别的依赖关系，将这些对象所依赖的对象注入绑定，从而保证每个业务对象在使用的时候，可以处于就绪状态。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"IoC Service Provider","slug":"IoC-Service-Provider","permalink":"http://example.com/tags/IoC-Service-Provider/"}],"author":"John Doe"},{"title":"IOC的基本理念：让别人为你服务","slug":"IOC的基本理念：让别人为你服务","date":"2022-03-03T09:01:00.000Z","updated":"2022-03-03T10:25:33.400Z","comments":true,"path":"2022/03/03/IOC的基本理念：让别人为你服务/","link":"","permalink":"http://example.com/2022/03/03/IOC%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%90%86%E5%BF%B5%EF%BC%9A%E8%AE%A9%E5%88%AB%E4%BA%BA%E4%B8%BA%E4%BD%A0%E6%9C%8D%E5%8A%A1/","excerpt":"","text":"IOC即控制反转，它还有一个别名叫做依赖注入（DependencyInjection）。 在我日常开发中，经常需要一个对象依赖于另一个对象的服务。最简单而有效的方式就是直接在类的构造函数中新建相应的依赖类。这就好比要装修新房，需要用家具，这个时候，根据通常解决对象依赖关系的做法，我们就会直接打造出需要的家具来。不过，通常都是分工明确的，所以，大多数情况下，我们可以去家具广场将家具买回来，然后根据需要装修布置即可。不管是直接打造家具（通过new构造对象），还是去家具广场买家具（通过工厂设计模式），有一个共同点需要我们关注，那就是我们都是自己主动地去获取依赖的对象！可是回头想想，我们自己每次用到什么依赖对象都要主动地去获取，这是否真的必要？我们最终所要做的，其实就是直接调用依赖对象所提供的某项服务而已。只要用到这个依赖对象的时候，它能够准备就绪，我们完全可以不管这个对象是自己找来的还是别人送过来的。 实际上IOC就是为了帮助我们解决这种问题的，而提供了更加轻松简洁的方式。它的反转，就反转在让你从原来的事必躬亲，转变为现在的享受服务。 通常情况下，被注入对象会直接依赖于被依赖对象。但是，在IoC的场景中，二者之间通过IoC Service Provider来打交道，所有的被注入对象和依赖对象现在由IoC Service Provider统一管理。被注入对象需要什么，直接跟IoC Service Provider招呼一声，后者就会把相应的被依赖对象注入到被注入对象中，从而达到IoC Service Provider为被注入对象服务的目的。IoC Service Provider在这里就是通常的IoC容器所充当的角色。从被注入对象的角度看，与之前直接寻求依赖对象相比，依赖对象的取得方式发生了反转，控制也从被注入对象转到了IoC Service Provider那里。 （IoC Service Provider在这里是一个抽象出来的概念，它可以指代任何将IoC场景中的业务对象绑定到一起的实现方式。它可以是一段代码，也可以是一组相关的类，甚至可以是比较通用的IoC框架或者IoC容器实现。）","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"IOC的理解","slug":"IOC的理解","permalink":"http://example.com/tags/IOC%E7%9A%84%E7%90%86%E8%A7%A3/"}],"author":"John Doe"},{"title":"Spring三种注入方法比较","slug":"Spring三种注入方法比较","date":"2022-03-03T08:53:00.000Z","updated":"2022-03-03T08:54:49.415Z","comments":true,"path":"2022/03/03/Spring三种注入方法比较/","link":"","permalink":"http://example.com/2022/03/03/Spring%E4%B8%89%E7%A7%8D%E6%B3%A8%E5%85%A5%E6%96%B9%E6%B3%95%E6%AF%94%E8%BE%83/","excerpt":"","text":"接口注入。从注入方式的使用上来说，接口注入是现在不甚提倡的一种方式，基本处于“退役状态”。因为它强制被注入对象实现不必要的接口，带有侵入性。而构造方法注入和setter方法注入则不需要如此。 构造方法注入。这种注入方式的优点就是，对象在构造完成之后，即已进入就绪状态，可以 马上使用。缺点就是，当依赖对象比较多的时候，构造方法的参数列表会比较长。而通过反射构造对象的时候，对相同类型的参数的处理会比较困难，维护和使用上也比较麻烦。而且在Java中，构造方法无法被继承，无法设置默认值。对于非必须的依赖处理，可能需要引入多个构造方法，而参数数量的变动可能造成维护上的不便。 setter方法注入。因为方法可以命名，所以setter方法注入在描述性上要比构造方法注入好一些。 另外，setter方法可以被继承，允许设置默认值，而且有良好的IDE支持。缺点当然就是对象无法在构造完成后马上进入就绪状态。","categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"}],"tags":[{"name":"注入方式","slug":"注入方式","permalink":"http://example.com/tags/%E6%B3%A8%E5%85%A5%E6%96%B9%E5%BC%8F/"}],"author":"John Doe"},{"title":"MySQL怎么查看索引是否是高选择性？","slug":"MySQL怎么查看索引是否是高选择性？","date":"2022-03-02T11:33:00.000Z","updated":"2022-03-02T11:40:36.822Z","comments":true,"path":"2022/03/02/MySQL怎么查看索引是否是高选择性？/","link":"","permalink":"http://example.com/2022/03/02/MySQL%E6%80%8E%E4%B9%88%E6%9F%A5%E7%9C%8B%E7%B4%A2%E5%BC%95%E6%98%AF%E5%90%A6%E6%98%AF%E9%AB%98%E9%80%89%E6%8B%A9%E6%80%A7%EF%BC%9F/","excerpt":"","text":"通过show index查看结果列中的cardinality值（表示索引中不重复记录数量的预估值），在实际应用中cardinality/table.size应该尽可能接近1。 cardinality是在存储引擎层进行统计的。具体方式是通过采样的方法来完成。具体发生在insert和update操作中，策略为①表中1/16数据发生过变化②stat_modified_counter&gt;2 000 000 000（表中数据实际未增加，实际发生变化的还是这一行数据①就无法适用，则通过②的计数器stat_modirfied_counter表示发生变化次数）。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"索引","slug":"索引","permalink":"http://example.com/tags/%E7%B4%A2%E5%BC%95/"}],"author":"John Doe"},{"title":"InnoDB行溢出（Redundant）","slug":"InnoDB行溢出","date":"2022-03-02T10:45:00.000Z","updated":"2022-03-02T10:53:16.247Z","comments":true,"path":"2022/03/02/InnoDB行溢出/","link":"","permalink":"http://example.com/2022/03/02/InnoDB%E8%A1%8C%E6%BA%A2%E5%87%BA/","excerpt":"","text":"3个列长度总和是66000，innoDB存储引擎的页为16kb，16384字节，会产生行溢出，因此对于这种情况，数据不会存放于b+tree的叶子节点中，而是存入页类型为uncompress blob页中。 每页中至少存放两条行记录（否则失去了B+tree的意义，变为了链表），因此如果一页中只能放一条记录，则会将数据放到溢出页。而对于Text或BLOB的数据类型亦然。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"行结构","slug":"行结构","permalink":"http://example.com/tags/%E8%A1%8C%E7%BB%93%E6%9E%84/"}],"author":"John Doe"},{"title":"MySQL中varchar中的N","slug":"MySQL中varchar中的N","date":"2022-03-02T10:42:00.000Z","updated":"2022-03-02T10:44:12.635Z","comments":true,"path":"2022/03/02/MySQL中varchar中的N/","link":"","permalink":"http://example.com/2022/03/02/MySQL%E4%B8%ADvarchar%E4%B8%AD%E7%9A%84N/","excerpt":"","text":"varchar(N)指的是字符长度，而官方文档中指的是最大支持65535是字节","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"行","slug":"行","permalink":"http://example.com/tags/%E8%A1%8C/"}],"author":"John Doe"},{"title":"如何在 Linux 系统中查看 TCP 状态？","slug":"如何在-Linux-系统中查看-TCP-状态？","date":"2022-02-25T01:56:00.000Z","updated":"2022-02-25T01:58:06.991Z","comments":true,"path":"2022/02/25/如何在-Linux-系统中查看-TCP-状态？/","link":"","permalink":"http://example.com/2022/02/25/%E5%A6%82%E4%BD%95%E5%9C%A8-Linux-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E6%9F%A5%E7%9C%8B-TCP-%E7%8A%B6%E6%80%81%EF%BC%9F/","excerpt":"","text":"netstat-napt","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"}],"author":"John Doe"},{"title":"最左匹配原则","slug":"最左匹配原则","date":"2022-02-24T08:02:00.000Z","updated":"2022-02-24T08:04:16.906Z","comments":true,"path":"2022/02/24/最左匹配原则/","link":"","permalink":"http://example.com/2022/02/24/%E6%9C%80%E5%B7%A6%E5%8C%B9%E9%85%8D%E5%8E%9F%E5%88%99/","excerpt":"","text":"MySQL中的索引可以以一定顺序引用多列，这种索引叫作联合索引。如User表的name和city加联合索引就是(name,city)，而最左前缀原则指的是，如果查询的时候查询条件精确匹配索引的左边连续一列或几列，则此列就可以被用到。如下： select * from user where name=xx and city=xx ; ／／可以命中索引 select * from user where name=xx ; // 可以命中索引 select * from user where city=xx; // 无法命中索引 需要注意：查询的时候如果两个条件都用上了，但是顺序不同，如 city= xx and name ＝xx，那么现在的查询引擎会自动优化为匹配联合索引的顺序，这样是能够命中索引的.由于最左前缀原则，在创建联合索引时，索引字段的顺序需要考虑字段值去重之后的个数，较多的放前面。ORDERBY子句也遵循此规则。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"最左匹配原则","slug":"最左匹配原则","permalink":"http://example.com/tags/%E6%9C%80%E5%B7%A6%E5%8C%B9%E9%85%8D%E5%8E%9F%E5%88%99/"}],"author":"John Doe"},{"title":"一条SQL语句执行得很慢的原因有哪些？","slug":"一条SQL语句执行得很慢的原因有哪些？","date":"2022-02-24T07:50:00.000Z","updated":"2022-02-24T07:59:49.798Z","comments":true,"path":"2022/02/24/一条SQL语句执行得很慢的原因有哪些？/","link":"","permalink":"http://example.com/2022/02/24/%E4%B8%80%E6%9D%A1SQL%E8%AF%AD%E5%8F%A5%E6%89%A7%E8%A1%8C%E5%BE%97%E5%BE%88%E6%85%A2%E7%9A%84%E5%8E%9F%E5%9B%A0%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F/","excerpt":"","text":"一个 SQL 执行的很慢，我们要分两种情况讨论： 1、大多数情况下很正常，偶尔很慢，则有如下原因 (1)、数据库在刷新脏页，例如 redo log 页写满了需要同步到磁盘。 (2)、执行的时候，遇到锁，如表锁、行锁。 2、这条 SQL 语句一直执行的很慢，则有如下原因。 (1)、没有用上索引：例如该字段没有索引；由于对字段进行运算、函数操作导致无法用索引。 (2)、数据库选错了索引。 转载：https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&amp;mid=2247485185&amp;idx=1&amp;sn=66ef08b4ab6af5757792223a83fc0d45&amp;chksm=cea248caf9d5c1dc72ec8a281ec16aa3ec3e8066dbb252e27362438a26c33fbe842b0e0adf47&amp;token=79317275&amp;lang=zh_CN%23rd","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"性能","slug":"性能","permalink":"http://example.com/tags/%E6%80%A7%E8%83%BD/"}],"author":"John Doe"},{"title":"分库分表之后的主键处理方式","slug":"分库分表之后的主键处理方式","date":"2022-02-24T04:56:00.000Z","updated":"2022-02-24T05:00:22.852Z","comments":true,"path":"2022/02/24/分库分表之后的主键处理方式/","link":"","permalink":"http://example.com/2022/02/24/%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E4%B9%8B%E5%90%8E%E7%9A%84%E4%B8%BB%E9%94%AE%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F/","excerpt":"","text":"1、UUID：不适合作为主键，其太长了而且无序，插入效率低 2、数据库自增id：两台数据库分别设置不同步⻓，⽣成不重复ID的策略来实现⾼可⽤。这种⽅式⽣成的 id 有序，但是需要独⽴部署数据库实例，成本⾼，还会有性能瓶颈。 3、利⽤ redis ⽣成 id : 性能⽐᫾好，灵活⽅便，不依赖于数据库。但是，引⼊了新的组件造成系统更加复杂，可⽤性降低，编码更加复杂，增加了系统成本。 4、Twitter的snowflake算法： 1.第一位 占用1bit，其值始终是0，没有实际作用。 2.时间戳 占用41bit，精确到毫秒，总共可以容纳约140年的时间。 3.工作机器id 占用10bit，其中高位5bit是数据中心ID（datacenterId），低位5bit是工作节点ID（workerId），做多可以容纳1024个节点。 4.序列号 占用12bit，这个值在同一毫秒同一节点上从0开始不断累加，最多可以累加到4095。 SnowFlake算法在同一毫秒内最多可以生成多少个全局唯一ID呢？只需要做一个简单的乘法： 1024x4096 SnowFlake算法的优点： 1.生成ID时不依赖于DB，完全在内存生成，高性能高可用。 2.ID呈趋势递增，后续插入索引树的时候性能较好。 SnowFlake算法的缺点： 依赖于系统时钟的一致性。如果某台机器的系统时钟回拨，有可能造成ID冲突，或者ID乱序。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"主键","slug":"主键","permalink":"http://example.com/tags/%E4%B8%BB%E9%94%AE/"}],"author":"John Doe"},{"title":"http常见状态码","slug":"http常见状态码","date":"2022-02-23T07:48:00.000Z","updated":"2022-02-23T07:56:43.034Z","comments":true,"path":"2022/02/23/http常见状态码/","link":"","permalink":"http://example.com/2022/02/23/http%E5%B8%B8%E8%A7%81%E7%8A%B6%E6%80%81%E7%A0%81/","excerpt":"","text":"1xx：表示一种提示信息，一般是服务器的中间状态，不常用 2xx：表示服务器已经成功处理了请求 200：成功204：成功（响应头没有body数据）206：用于http分块下载或者断点续传，表示响应的body里面的数据并不完整，只是一部分 3xx：客户端请求的资源发生了变动需要重定向 301：表示永久重定向，即访问的资源永久不存在 302：临时重定向，即访问的资源还在，需要换一个url访问 301和302会在响应头使用location字段表明重定向的url进行重定向 304：不具有跳转含义，表明请求的资源未修改，重定向缓存文件。 4xx：请求的报文有误，服务器无法处理 400：请求错误，具体不清楚 403：服务器禁止访问资源 404：访问的资源在服务器找不到 5xx 类状态码表示客户端请求报⽂正确，但是服务器处理时内部发⽣了错误，属于服务器端的错误码。 「500 Internal Server Error」与 400 类型，是个笼统通⽤的错误码，服务器发⽣了什么错误，我们并不知道。 「501 Not Implemented」表示客户端请求的功能还不⽀持，类似“即将开业，敬请期待”的意思。 「502 Bad Gateway」通常是服务器作为⽹关或代理时返回的错误码，表示服务器⾃身⼯作正常，访问后端服务器发⽣了错误。 「503 Service Unavailable」表示服务器当前很忙，暂时⽆法响应服务器，类似“⽹络服务正忙，请稍后᯿试”的意思。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"http","slug":"http","permalink":"http://example.com/tags/http/"}],"author":"John Doe"},{"title":"http","slug":"http","date":"2022-02-23T07:37:47.000Z","updated":"2022-02-23T07:38:21.504Z","comments":true,"path":"2022/02/23/http/","link":"","permalink":"http://example.com/2022/02/23/http/","excerpt":"","text":"http是计算机世界里面两点之间进行文字、图片、视频等超文本数据传输的协议","categories":[],"tags":[],"author":"John Doe"},{"title":"get和post的区别","slug":"get和post的区别","date":"2022-02-18T14:38:00.000Z","updated":"2022-02-18T14:39:22.715Z","comments":true,"path":"2022/02/18/get和post的区别/","link":"","permalink":"http://example.com/2022/02/18/get%E5%92%8Cpost%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"POST和GET都是向服务器提交数据，并且都会从服务器获取数据。 区别： 1、传送方式：get通过地址栏传输，post通过报文传输。 2、传送长度：get参数有长度限制（受限于url长度），而post无限制 3、GET和POST还有一个重大区别，简单的说： GET产生一个TCP数据包；POST产生两个TCP数据包 长的说： 对于GET方式的请求，浏览器会把http header和data一并发送出去，服务器响应200（返回数据）； 而对于POST，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok（返回数据）。 也就是说，GET只需要汽车跑一趟就把货送到了，而POST得跑两趟，第一趟，先去和服务器打个招呼“嗨，我等下要送一批货来，你们打开门迎接我”，然后再回头把货送过去。 因为POST需要两步，时间上消耗的要多一点，看起来GET比POST更有效。因此Yahoo团队有推荐用GET替换POST来优化网站性能。但这是一个坑！跳入需谨慎。为什么？ GET与POST都有自己的语义，不能随便混用。 据研究，在网络环境好的情况下，发一次包的时间和发两次包的时间差别基本可以无视。而在网络环境差的情况下，两次包的TCP在验证数据包完整性上，有非常大的优点。 并不是所有浏览器都会在POST中发送两次包，Firefox就只发送一次。 建议： 1、get方式的安全性较Post方式要差些，包含机密信息的话，建议用Post数据提交方式； 2、在做数据查询时，建议用Get方式；而在做数据添加、修改或删除时，建议用Post方式； 案例：一般情况下，登录的时候都是用的POST传输，涉及到密码传输，而页面查询的时候，如文章id查询文章，用get 地址栏的链接为：article.php?id=11，用post查询地址栏链接为：article.php， 不会将传输的数据展现出来。 拓展资料： GET在浏览器回退时是无害的，而POST会再次提交请求。 GET产生的URL地址可以被Bookmark，而POST不可以。 GET请求会被浏览器主动cache，而POST不会，除非手动设置。 GET请求只能进行url编码，而POST支持多种编码方式。 GET请求参数会被完整保留在浏览器历史记录里，而POST中的参数不会被保留。 GET请求在URL中传送的参数是有长度限制的，而POST么有。 对参数的数据类型，GET只接受ASCII字符，而POST没有限制。 GET比POST更不安全，因为参数直接暴露在URL上，所以不能用来传递敏感信息。 GET参数通过URL传递，POST放在Request body中。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"get","slug":"get","permalink":"http://example.com/tags/get/"},{"name":"post","slug":"post","permalink":"http://example.com/tags/post/"}],"author":"John Doe"},{"title":"代理模式","slug":"代理模式","date":"2022-02-11T13:22:00.000Z","updated":"2022-02-11T13:38:00.109Z","comments":true,"path":"2022/02/11/代理模式/","link":"","permalink":"http://example.com/2022/02/11/%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"代理模式是指，为其他对象提供一种代理以控制对这个对象的访问。在某些情况下，一个对象不适合或者不能直接引用另一个对象，而代理对象可以在客户类和目标对象之间起到中介的作用。说直白一点就是在不修改目标对象的基础上，使用代理对象来增强目标对象的业务逻辑方法。 代理模式分为：静态代理和动态代理（jdk动态代理和cglib动态代理） 静态代理就是：代理类在程序运行前就确定好了和目标类的关系，在编译期就实现了。其中静态代理的缺点在于： 1、代码复杂，不便于管理：试想对于代理类，需要和目标类实现相同接口即每个代理类都要实现目标类的的方法，会出现代码重复，且考虑到如果接口增加一个方法，其所有实现类都要重写，维护也麻烦。 2、代理类依赖于目标类：当代理类考虑代理多个服务的时候，不便于实现 动态代理是在程序运行期间根据jvm反射机制动态生成的。 jdk动态代理：基于java反射机制实现的。具体通过使用java.lang.reflect 包提供三个类支持代理模式 Proxy, Method和 InovcationHandler。（要求：求目标对象必须实现接口） public interface UsbSell &#123; Object sell(float amount); &#125; public class UsbFactory implements UsbSell &#123; public Object sell(float amount) &#123; float price = 0; if (amount &gt; 100)&#123; price = (float) (amount * (1 + 0.2)); &#125;else &#123; price = (float) (amount * (1 + 0.5)); &#125; return price; &#125; &#125; public class ProxySeller &#123; private Object target; public ProxySeller() &#123; &#125; public ProxySeller(Object target) &#123; this.target = target; &#125; public Object getProxy()&#123; return Proxy.newProxyInstance(target.getClass().getClassLoader(), target.getClass().getInterfaces(), new InvocationHandler() &#123; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; float res = (Float) method.invoke(target,args); System.out.println(&quot;==&quot;+res); return proxy; &#125; &#125;); &#125; &#125; public class Main &#123; public static void main(String[] args) &#123; UsbFactory factory = new UsbFactory(); ProxySeller seller = new ProxySeller(factory); UsbSell proxy = (UsbSell)seller.getProxy(); UsbSell s = (UsbSell)proxy.sell(50); s.sell(50); &#125; &#125; cglib动态代理：一个开源项目。对于无接口的类，要为其创建动态代理，就要使用 CGLIB 来实现。CGLIB 代理的生成原理是生成目标类的子类，而子类是增强过的，这个子类对象就是代理对象。所以，使用CGLIB 生成动态代理，要求目标类必须能够被继承，即不能是 final 的类。 public class Saller &#123; public float sell(int amount)&#123; float price = 100; if (amount &gt; 100)&#123; price = (float) (price * (1 + 0.2)); &#125;else &#123; price = (float) (price * (1 + 0.5)); &#125; return price; &#125; &#125; public class ProxySaller implements MethodInterceptor &#123; private Object target; public ProxySaller() &#123; &#125; public ProxySaller(Object target) &#123; this.target = target; &#125; public Object getProxySaller()&#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(target.getClass()); enhancer.setCallback(this); Saller saller = (Saller) enhancer.create(); return saller; &#125; public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; Float price = (Float) methodProxy.invoke(target,objects); System.out.println(&quot;===&quot;+price); return price; &#125; &#125; public class Main &#123; public static void main(String[] args) &#123; Saller saller = new Saller(); ProxySaller proxySaller = new ProxySaller(saller); Saller proxy = (Saller) proxySaller.getProxySaller(); proxy.sell(100); &#125; &#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"代理模式","slug":"代理模式","permalink":"http://example.com/tags/%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/"}],"author":"John Doe"},{"title":"innoDB中的锁","slug":"innoDB中的锁","date":"2022-02-10T05:38:00.000Z","updated":"2022-02-10T11:19:02.156Z","comments":true,"path":"2022/02/10/innoDB中的锁/","link":"","permalink":"http://example.com/2022/02/10/innoDB%E4%B8%AD%E7%9A%84%E9%94%81/","excerpt":"","text":"innoDB实现了共享锁（读锁）和排他锁（写锁）两种行级锁。意向共享锁和意向排他锁两种表级别的锁。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"锁","slug":"锁","permalink":"http://example.com/tags/%E9%94%81/"}],"author":"John Doe"},{"title":"lock与latch","slug":"lock与latch","date":"2022-02-10T05:36:00.000Z","updated":"2022-02-10T05:38:14.315Z","comments":true,"path":"2022/02/10/lock与latch/","link":"","permalink":"http://example.com/2022/02/10/lock%E4%B8%8Elatch/","excerpt":"","text":"","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"锁","slug":"锁","permalink":"http://example.com/tags/%E9%94%81/"}],"author":"John Doe"},{"title":"MySQL分区","slug":"MySQL分区","date":"2022-02-09T12:14:49.000Z","updated":"2022-02-09T12:14:49.287Z","comments":true,"path":"2022/02/09/MySQL分区/","link":"","permalink":"http://example.com/2022/02/09/MySQL%E5%88%86%E5%8C%BA/","excerpt":"","text":"","categories":[],"tags":[],"author":"John Doe"},{"title":"MySQL表","slug":"MySQL表","date":"2022-02-09T10:47:00.000Z","updated":"2022-02-09T12:07:08.587Z","comments":true,"path":"2022/02/09/MySQL表/","link":"","permalink":"http://example.com/2022/02/09/MySQL%E8%A1%A8/","excerpt":"","text":"innoDB中，表根据主键顺序存放。每张表都有一个主键，在建表时没有显示定义主键，则innoDB会先判断表中是否有非空的唯一索引，如果有，则该索引即为主键（对于多个非空唯一索引，根据定义的顺序选择，而不是建表列的顺序选择），如果没有，则会自动创建一个6字节的指针。 innoDB中，数据被逻辑的放在一个表空间。表空间由段组成，段又由区组成，区又有页组成，页时最基本的单位。如下： innoDB默认情况下有一个共享表空间，如果用户开启参数innodb_file_per_table则每张表的数据单独放到一个表空间。（需要注意的是：单独的表空间只是存放数据、索引和插入缓冲bitmap页，对于其他数据，如回滚信息、插入缓冲索引页等仍是存放在共享表空间） 对于段由innoDB管理，数据段即为B+tree的叶子节点，索引段即为B+tree的非叶子节点，回滚段较为特殊。 区则是连续页（默认16kb/页）组成的空间（大小1mb）。一个区默认有64个连续的页。为了保证区中页的连续性，innoDB会一次从磁盘申请4-5个区。值得注意的是： innoDB常见页： 页又由行组成最多允许7992（16kb/2-200）行记录。innoDB提供了Compact和Redundant格式的行数据格式。需要注意：除了下图的信息外，还存在事务ID列（6字节）和回滚指针列（7字节），如果innoDB没有定义主键还会有一个6字节的rowid列 Compact行记录： Redundant行记录格式： 当然一般情况下innoDB的数据都是放在页类型为B+tree-node中，但是当发生行溢出，数据存放在Uncompress BLOB页中。 innoDB数据页结构：","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"表","slug":"表","permalink":"http://example.com/tags/%E8%A1%A8/"}],"author":"John Doe"},{"title":"MySQL文件","slug":"MySQL文件","date":"2022-02-09T08:05:00.000Z","updated":"2022-02-09T08:39:14.232Z","comments":true,"path":"2022/02/09/MySQL文件/","link":"","permalink":"http://example.com/2022/02/09/MySQL%E6%96%87%E4%BB%B6/","excerpt":"","text":"1、参数文件：MySQL实例启动时会读取参数文件来初始化。 2、日志文件： 错误日志（记录了MySQL执行期间的错误信息） 二进制日志（记录了对MySQL执行的写操作，默认未开启。 作用：1、恢复，可以通过binlog进行数据的恢复2、复制：通过复制和执行binlog对远程的MySQL进行实时数据同步（主从复制）3、审计：对binlog数据进行审计，看是否有对数据库进行注入的攻击 ） 慢查询日志（可以从中得到一些SQL优化信息，默认未开启） 查询日志（记录了所有对MySQL的请求信息） 3、套接字文件 4、pid文件 5、表结构定义文件（以frm为后缀名）：记录了该表的表结构定义。除此之外还用于存放视图的定义。 6、innoDB存储引擎文件： a）表空间文件（默认10mb，名为ibdata1）：可以设置基于innoDB存储的单独的。idb独立表空间文件（仅存储数据、索引等信息，其他信息还是存放于表空间文件）。 b）、redolog文件（默认会有两个名为ib_logfile0和ib_logfile1的文件）：每个innoDB至少有一个redolog组（每组至少有两个redolog文件），redolog冲缓冲区写入磁盘是按512字节，即一个扇区大小，可以保障写入必定成功（所有不需要doublewrite）","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"文件","slug":"文件","permalink":"http://example.com/tags/%E6%96%87%E4%BB%B6/"}],"author":"John Doe"},{"title":"innoDB存储引擎","slug":"innoDB存储引擎","date":"2022-02-09T03:04:00.000Z","updated":"2022-02-09T07:37:44.969Z","comments":true,"path":"2022/02/09/innoDB存储引擎/","link":"","permalink":"http://example.com/2022/02/09/innoDB%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/","excerpt":"","text":"innoDB存储引擎是开源的第一个完整支持ACID事务的MySQL存储引擎（第一个支持事务的是BDB存储引擎），其特点是行锁设计、支持MVCC、支持外键、一致性非锁定读等。被广泛的应用。 innoDB存储引擎有多个内存块，组成了一个大的内存池，每个内存块有指定的后台线程来维护其运行。 1、main线程主要负责将缓冲区的数据异步刷新到磁盘，保证数据的一致性（包括脏页的刷新、合并插入缓冲、undo页的回收等） 2、IO线程主要是负责IO请求的回调，包括read、write、insert buffer、log等，其使用了AIO机制，保证了IO性能。 3、Purge线程主要用于回收提交事务之后，undo页可能不再需要，需要对其进行回收。（1.2版本支持多个Purge线程，目的是进一步加快undo页的回收，提升性能） 4、Page cleaner线程主要用于脏页的刷新操作，减轻main线程的压力。 innoDB是基于磁盘存储的，为了权衡磁盘速度和CPU速度的差异，提供了一块缓冲池技术来提升性能。（因此可以将缓冲池区域设置大一点来进行优化操作）innoDB1.0允许多个缓冲池实例，磁盘读取的页根据哈希值均匀分配到不同缓冲池中（目的：减少数据库资源竞争，增加数据库并发处理能力） 在数据的读取中，会先去缓冲池中查看是否存在于缓冲池，如果存在直接读取，不存在则去磁盘读取，在同步到缓冲池中。而对于写操作则是先写到缓冲池，然后根据Checkpoint机制将脏数据刷新到磁盘，保证磁盘和内存数据的一致性。 缓冲池具体的数据页有： 为了管理这些数据页，innoDB使用了一个freeLIst链表来管理空闲的页内存，LRUList来管理已经分配的页内存，flushList来管理脏页。（脏页及存在于LRUList，又存在于flushList，是两者共享的） 对于LRUList管理的页，采用了LRU（最近最少使用）算法来管理（缓冲池页大小默认16kb），在LRU列表中加入了一个midpoint位置（默认是5/8位置处），midpoint位置前面的是热点数据区域，后面的是冷数据区域（设置冷、热数据区域主要是为了保证一些经常被访问的数据存在于内存中，提示效率的一种考虑）。当一个新的页被分配到LRULIst上，会先加入到midpoint位置后面（这样做是为了防止当进行全表查询的时候，多个页会覆盖调热数据区域的页，而这些查的数据页又只使用一次，后续不再使用，当后面访问热区域的页时有会从磁盘中查找，浪费性能），同时也指定了一个从冷数据区域晋升到热数据区域的参数，当到了晋升时间后，冷数据区域的页就会晋升到热数据区域。 值得注意的是：空闲页的内存freeList+以分配的页内存LRUList并不等于缓冲池的内存，因为缓冲池中包含得其他部分页（自适应哈希索引、lock信息等）不需要LRU维护，不存在于LRUList中。 另外，页是支持压缩的，16kb的页可以压缩成1kb、2kb、4kb、8kb。页的大小发生变化，所有对于压缩的页，会使用zipLRUList进行管理。（注意：LRUList包含zipLRUList中的页） 对于zipLIRList的页的分配采用伙伴算法 例如压缩后的页为4kb 1、先检查4kb的zipLRUList是否有空闲页，存在即分配 2、否则，检查8kb的zipLRUList看是否存在空闲页，存在则将8kb分为两个4kb，将4kb的页放入4kbzipLRUList，然后为其分配 3、在否则，检查16kb的freeList看是否存在空闲页，存在则将16kb分为两个4kb，一个8kb，分别放入对应zipLRUList，然后为其分配 由上图可知，innoDB除了缓冲池，还存在redolog日志缓冲和额外内存池。 其中redolog日志缓冲（默认8mb大小）是redolog文件的缓冲区（redolog文件记录了写请求的指令，对页的写指令都会记录到这个文件中，后续数据库恢复会使用到这个文件）当满足以下条件就会将缓冲区的数据刷新到文件中。 1、每个事物提交会进行刷新 2、当缓冲区小于一半，会进行刷新 3、main线程每秒会进行一次刷新 而额外的内存池则是在对于一些数据结构本身进行进行内存分配时会从额外内存储进行申请。 前面提到进行写操作入时，一般会先写到缓冲区，然后在根据checkpoint机制将脏页刷新到内存，保持内存和磁盘数据的一致性。但考虑到如果频繁发生写操作，而对脏数据刷新到磁盘不加以控制，每来一个写操作，都会进行一次刷新，那就会产生大量的io，导致整体性能下降；除此之外，在刷新的时候如果出现了宕机，数据也会丢失。因此采用提交事物前，先写redolog日志，然后在修改内存中的页，即使将脏页刷新到磁盘时出现宕机，也能够根据redolog日志进行恢复。而chenckpoint技术就是为了解决 1、缩短数据库的恢复时间（因为chenckpoint前的脏页都已经刷新到磁盘了，只需对chenckpoint之后的进行恢复）2、缓冲池不够时将脏页刷新到磁盘。（当缓冲池不够用时，会根据LRU算法将最近最少用的页淘汰，而淘汰时会检测是否为脏页，如果是则执行checkpoint，将脏页刷新到磁盘）3、redolog日志不够用时，刷新脏页（即redolog的大小是有限制的，chenckpont前的是可重用的，而chenckpoint之后的是需要的，如果redolog文件里面全部都是需要使用的，则必须进行checkpoint） 在innoDB中，使用LSN（八字节）标记版本，每个页都有自己的LSN，redolog日志中和checkpoint中也有。 在innoDB中存在两种checkpoint，即sharp checkpoint（默认）和fuzzy checkpoint。 sharp checkpoint发生在数据库关闭时，此时会将所有脏页刷新到磁盘（会发生迟钝） fuzzy checkpoint则是每次只刷新部分脏页到磁盘。以下是几种发生fuzzy checkpoint的情况： 1、main线程会每秒或10秒的速度从fulshList中刷新页到磁盘。 2、当缓冲池没有多余空闲空间，会根据LRU算法冲LRUList淘汰页，对于淘汰的页会检测是否是脏页，是则会刷新到磁盘。 3、当redolog不可用时会强制flushList中的脏页进行刷新 4、当脏页太多，也会强制进行checkpoint刷新脏页到磁盘。 innoDB主要工作都是在main线程中完成的，其内部由多个循环组成（主循环、后台循环、刷新循环、暂停循环），在多个循环中切换进行工作。 innoDB1.0 主循环： 后台循环： innoDB1.2基于上述IO限制，加入了innoDB_io_capacitiy用于表示IO（默认200），对于刷新的页用百分比来控制 另外一个参数是innoDB_max_dirty_pct（默认75），当脏页小于innoDB_max_dirty_pct也会刷新一定量的脏页（之前是不会刷新的）。 接下来说一说innoDB的插入缓冲。insert buffer和数据页一样，是物理页的一部分。 在innoDB中，主键是唯一标识，插入记录一般按主键递增顺序插入。因此，聚集索引一般是顺序的（比如自增id这种），对于顺序的一般插入操作速度很快，但对于主键是uuid之类则和辅助索引一样，是随机的。因此对于这种情况，按顺序插入则相对要慢得多。因此insert buffer的作用就是对于非聚集索引的插入或者更新操作先判断是否存在缓冲池，若存在则直接插入，不存在则放到insert buffer中，在以一定频率进行inser buffer和辅助索引子节点的合并，提高对于非聚集索引的插入性能。当然使用insert buffer需要满足 insert buffer数据实现是一颗b+树， change buffer：在1.0.x版本引入了channge buffer，可以对增删改都进行缓冲 两次写：当innoDB刷新某个页到磁盘中，但只刷新了部分，数据库就宕机了（部分写失效）。double write就是为了解决这种情况产生的。 double write由两部分组成。一部分时内存double write buffer（大小2mb），一部分是磁盘上的共享表空间中连续的128页（即两个区，大小为2mb）。在对脏页进行刷新时，并不直接写磁盘，而是通过memcpy函数将脏页先复杂到 doublewrite buffer，然后doubllewrite buffer再分两次顺序的写到共享表空间的物理磁盘上（每次写1mb）。完成之后，在将doublewrite buffer中的页写入各个表文件空间中。如果在写入表中磁盘时发生了宕机什么的。在恢复时，可以从共享表空间中找到一个备份页，将其复制到表空间。 自适应哈希：innoDB会对表上各索引页的查询监控。如果建立哈希索引会提升性能，则建立哈希索引。而条件就是：对这个页的连续访问模式要一样。 异步IO：innoDB采用AIO的方式处理磁盘操作，可以在发起一个IO请求后，立马发起另一个IO请求，当全部发送完后，等所有请求操作完。除此之外，还可以进行IO的合并操作。 刷新邻接页：即刷新一个脏页时，会检测该页周围的页是否是脏页，是则一并刷新。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"}],"tags":[{"name":"innoDB","slug":"innoDB","permalink":"http://example.com/tags/innoDB/"}],"author":"John Doe"},{"title":"jedis和redission","slug":"jedis和redission","date":"2022-02-08T12:45:00.000Z","updated":"2022-02-08T12:46:11.557Z","comments":true,"path":"2022/02/08/jedis和redission/","link":"","permalink":"http://example.com/2022/02/08/jedis%E5%92%8Credission/","excerpt":"","text":"","categories":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"}],"tags":[{"name":"框架实现","slug":"框架实现","permalink":"http://example.com/tags/%E6%A1%86%E6%9E%B6%E5%AE%9E%E7%8E%B0/"}],"author":"John Doe"},{"title":"一致性hash算法","slug":"一致性hash算法","date":"2022-02-08T12:33:00.000Z","updated":"2022-02-08T12:34:21.071Z","comments":true,"path":"2022/02/08/一致性hash算法/","link":"","permalink":"http://example.com/2022/02/08/%E4%B8%80%E8%87%B4%E6%80%A7hash%E7%AE%97%E6%B3%95/","excerpt":"","text":"","categories":[{"name":"算法","slug":"算法","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/"},{"name":"Redis","slug":"算法/Redis","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/Redis/"}],"tags":[{"name":"一致性哈希算法","slug":"一致性哈希算法","permalink":"http://example.com/tags/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95/"}],"author":"John Doe"},{"title":"Redis集群的主从复制模型","slug":"Redis集群的主从复制模型","date":"2022-02-08T12:27:00.000Z","updated":"2022-02-08T12:27:38.911Z","comments":true,"path":"2022/02/08/Redis集群的主从复制模型/","link":"","permalink":"http://example.com/2022/02/08/Redis%E9%9B%86%E7%BE%A4%E7%9A%84%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"","categories":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"}],"tags":[{"name":"主从复制模型","slug":"主从复制模型","permalink":"http://example.com/tags/%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E6%A8%A1%E5%9E%8B/"}],"author":"John Doe"},{"title":"Redis内存优化","slug":"Redis内存优化","date":"2022-02-08T12:09:00.000Z","updated":"2022-02-08T12:10:03.147Z","comments":true,"path":"2022/02/08/Redis内存优化/","link":"","permalink":"http://example.com/2022/02/08/Redis%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96/","excerpt":"","text":"","categories":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"}],"tags":[{"name":"内存优化","slug":"内存优化","permalink":"http://example.com/tags/%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96/"}],"author":"John Doe"},{"title":"Redis的回收进程如何工作","slug":"Redis的回收进程如何工作","date":"2022-02-08T09:37:00.000Z","updated":"2022-02-08T11:59:38.055Z","comments":true,"path":"2022/02/08/Redis的回收进程如何工作/","link":"","permalink":"http://example.com/2022/02/08/Redis%E7%9A%84%E5%9B%9E%E6%94%B6%E8%BF%9B%E7%A8%8B%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C/","excerpt":"","text":"","categories":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"}],"tags":[{"name":"回收进程","slug":"回收进程","permalink":"http://example.com/tags/%E5%9B%9E%E6%94%B6%E8%BF%9B%E7%A8%8B/"}],"author":"John Doe"},{"title":"一个Redis实例能存放多少key？","slug":"一个Redis实例能存放多少key？","date":"2022-02-08T09:27:00.000Z","updated":"2022-02-08T09:28:21.147Z","comments":true,"path":"2022/02/08/一个Redis实例能存放多少key？/","link":"","permalink":"http://example.com/2022/02/08/%E4%B8%80%E4%B8%AARedis%E5%AE%9E%E4%BE%8B%E8%83%BD%E5%AD%98%E6%94%BE%E5%A4%9A%E5%B0%91key%EF%BC%9F/","excerpt":"","text":"理论上 Redis 可以处理多达 2^32 的 keys，并且在实际中进行了测试，每个实例至少存放了 2 亿 5 千万的 keys。我们正在测试一些较大的值。任何 list、set、和 sorted set 都可以放 2^32 个元素。换句话说，Redis 的存储极限是系统中的可用内存值。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"}],"tags":[],"author":"John Doe"},{"title":"Redis异步队列","slug":"Redis异步队列","date":"2022-02-08T08:50:00.000Z","updated":"2022-02-08T09:02:41.782Z","comments":true,"path":"2022/02/08/Redis异步队列/","link":"","permalink":"http://example.com/2022/02/08/Redis%E5%BC%82%E6%AD%A5%E9%98%9F%E5%88%97/","excerpt":"","text":"","categories":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"}],"tags":[{"name":"异步队列","slug":"异步队列","permalink":"http://example.com/tags/%E5%BC%82%E6%AD%A5%E9%98%9F%E5%88%97/"}],"author":"John Doe"},{"title":"Redis分布式锁","slug":"Redis分布式锁","date":"2022-02-08T08:29:00.000Z","updated":"2022-02-08T08:30:14.720Z","comments":true,"path":"2022/02/08/Redis分布式锁/","link":"","permalink":"http://example.com/2022/02/08/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","excerpt":"","text":"转载自：https://www.cnblogs.com/wangyingshuo/p/14510524.html","categories":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"}],"tags":[{"name":"分布式锁","slug":"分布式锁","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/"}],"author":"John Doe"},{"title":"如何保证Redis数据都是热点数据？","slug":"如何保证Redis数据都是热点数据？","date":"2022-02-07T10:06:00.000Z","updated":"2022-02-07T10:11:22.310Z","comments":true,"path":"2022/02/07/如何保证Redis数据都是热点数据？/","link":"","permalink":"http://example.com/2022/02/07/%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81Redis%E6%95%B0%E6%8D%AE%E9%83%BD%E6%98%AF%E7%83%AD%E7%82%B9%E6%95%B0%E6%8D%AE%EF%BC%9F/","excerpt":"","text":"1.限定 Redis 占用的内存，Redis 会根据自身数据淘汰策略，加载热数据到内存。所以，计算一下 20W 数据大约占用的内存，然后设置一下 Redis 内存限制即可。 2.问题是什么数据？ 比如用户数据。数据库有2000w条。活跃用户：redis sortSet里 放两天内(为方便取一天内活跃用户)登录过的用户，登录一次ZADD一次，如set已存在则覆盖其分数（登录时间）。键：login:users，值：分数 时间戳、value userid。设置一个周期任务，比如每天03:00:00点删除sort set中前一天3点前的数据（保证set不无序增长、留近一天内活跃用户）。 取时，拿到当前时间戳（int 10位），再减1天就可按分数范围取过去24h活跃用户。 3.看你的提问,应该只是把Redis当缓存来用.提供一种简单实现缓存失效的思路: LRU(最近少用的淘汰)即redis的缓存每命中一次,就给命中的缓存增加一定ttl(过期时间)(根据具体情况来设定, 比如10分钟).一段时间后, 热数据的ttl都会较大, 不会自动失效, 而冷数据基本上过了设定的ttl就马上失效了.","categories":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"}],"tags":[{"name":"热点数据","slug":"热点数据","permalink":"http://example.com/tags/%E7%83%AD%E7%82%B9%E6%95%B0%E6%8D%AE/"}],"author":"John Doe"},{"title":"Redis共享整数字符串","slug":"Redis共享整数字符串","date":"2022-02-07T01:55:00.000Z","updated":"2022-02-07T01:55:23.165Z","comments":true,"path":"2022/02/07/Redis共享整数字符串/","link":"","permalink":"http://example.com/2022/02/07/Redis%E5%85%B1%E4%BA%AB%E6%95%B4%E6%95%B0%E5%AD%97%E7%AC%A6%E4%B8%B2/","excerpt":"","text":"","categories":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"}],"tags":[{"name":"共享","slug":"共享","permalink":"http://example.com/tags/%E5%85%B1%E4%BA%AB/"}],"author":"John Doe"},{"title":"Redis对象内存回收","slug":"Redis对象内存回收","date":"2022-02-07T01:53:00.000Z","updated":"2022-02-07T01:53:31.910Z","comments":true,"path":"2022/02/07/Redis对象内存回收/","link":"","permalink":"http://example.com/2022/02/07/Redis%E5%AF%B9%E8%B1%A1%E5%86%85%E5%AD%98%E5%9B%9E%E6%94%B6/","excerpt":"","text":"","categories":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"}],"tags":[{"name":"内存回收","slug":"内存回收","permalink":"http://example.com/tags/%E5%86%85%E5%AD%98%E5%9B%9E%E6%94%B6/"}],"author":"John Doe"},{"title":"Redis底层数据结构之对象","slug":"Redis底层数据结构之对象","date":"2022-02-07T00:49:00.000Z","updated":"2022-02-07T01:52:21.525Z","comments":true,"path":"2022/02/07/Redis底层数据结构之对象/","link":"","permalink":"http://example.com/2022/02/07/Redis%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%AF%B9%E8%B1%A1/","excerpt":"","text":"Redis对外提供了五种数据类型，分别是String、List、HashMap、HashSet以及ZSeT，其中String作为key-value键值映射的key，以及五种value之一，而其他四种则只能作为value。并且这五种数据类型在底层实现上都至少有两种数据结构实现。 在底层上，由一个type类型表明当前数据对象属于哪个类型，由encodeing表明底层具体的数据结构实现，然后由一个指针指向底层的数据结构实现。这样的好处的话主要就是在不同的应用场景选择不同的底层数据结构实现，会大大提高redis的存储性能。 1、具体的话，String类型底层实现有：int、raw、embstr三种数据结构的实现。其中int类型的底层数据结构实现主要是用于存放整数值，当我们的value是一个整数值，就可以选择用int类型的底层实现。而raw类型的底层实现则是一个动态字符串数据结构，一般当字符串大于32字节就会使用到。embstr则是当字符串小于32字节会使用到。两者的不同在于，embstr只会进行一次内存分配和释放，而raw则会进行两次内存分配和释放；而且embstr的内存时连续的，而raw不是。 需要注意的是：double这种浮点型的数据作为value存储的时候，底层使用的是str类型的数据结构实现。另外上述三种底层数据结构实现是可以相互转换的。 2、list类型底层编码可以使ziplist和linkedlist两种类型，当list满足每个节点小于64字节并且节点数小于512个就可以采用ziplist作为底层实现，否则采用linkedlist 3、hash对象底层编码可以使ziplist和hashtable两种类型 4、set集合的编码实现可以使整数集合和hashtable 5、有序集合zset的编码实现：ziplist和skiplist。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"}],"tags":[{"name":"对象","slug":"对象","permalink":"http://example.com/tags/%E5%AF%B9%E8%B1%A1/"}],"author":"John Doe"},{"title":"Redis底层数据结构之整数集合","slug":"Redis底层数据结构之整数集合","date":"2022-02-06T13:40:00.000Z","updated":"2022-02-06T13:49:01.981Z","comments":true,"path":"2022/02/06/Redis底层数据结构之整数集合/","link":"","permalink":"http://example.com/2022/02/06/Redis%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%95%B4%E6%95%B0%E9%9B%86%E5%90%88/","excerpt":"","text":"整数集合是Redis用于保存整数值得集合数据结构，可以保存int16、int32、int64de整数值，并且有序不会重复，具体由encoding决定保存是int16、32还是64. 当将一个新元素加入整数集合时，而且这个元素类型长于当前集合类型，就会先对集合升级，然后在加入新元素。 升级： 1、根据新元素类型，开辟新的数组 2、将原数组的元素转移到新数组的正确位置上，且转化为与新数组相同的类型 3、将新元素加到新数组指定的位置 好处： 1、提升灵活性，C语言是静态类型语言，为了避免错误，不会将两种类型放在一个数据结构里面，通过底层数组升级操作，不必担心不同类型的整数出现类型错误 2、节约内存，整数集合的升级操作，确保了只在需要的时候进行，尽量节约内存。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"}],"tags":[{"name":"整数集合","slug":"整数集合","permalink":"http://example.com/tags/%E6%95%B4%E6%95%B0%E9%9B%86%E5%90%88/"}],"author":"John Doe"},{"title":"Redis底层数据结构之跳表","slug":"Redis底层数据结构实现之跳表","date":"2022-02-06T13:13:00.000Z","updated":"2022-02-06T13:24:50.758Z","comments":true,"path":"2022/02/06/Redis底层数据结构实现之跳表/","link":"","permalink":"http://example.com/2022/02/06/Redis%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B7%B3%E8%A1%A8/","excerpt":"","text":"跳表支持平均o（logN）、最坏O（n）复杂度的节点查找，还可以通过顺序性操作来批量处理节点。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"}],"tags":[{"name":"跳表","slug":"跳表","permalink":"http://example.com/tags/%E8%B7%B3%E8%A1%A8/"}],"author":"John Doe"},{"title":"Redis底层数据结构之字典","slug":"Redis底层实现之字典","date":"2022-02-06T12:37:00.000Z","updated":"2022-02-06T13:24:45.912Z","comments":true,"path":"2022/02/06/Redis底层实现之字典/","link":"","permalink":"http://example.com/2022/02/06/Redis%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%AD%97%E5%85%B8/","excerpt":"","text":"字典即符号表，Redis的数据库就是通过字典作为底层实现的。而字典的底层实现主要是使用hash表。 1、hash表底层实现是通过数组加链表实现的，对于一个key值，通过计算其hashcode，然后与上hash表的掩码（数组-1），得到在数组中的下标，然后同该下标上的链表进行比较（没有链表则直接加上去），看是否是同一个值，如果是，则覆盖，不是则加到链表尾。 2、字典则是一个包含两个hash表的结构体，一般情况只使用下标为0的hash表，当对0下标的hash表进行扩容时，会使用到1下标处的hash表。即当0下标处的hash表 a）满足服务器没有执bgsave或者bgrewriteaof命令，并且hash表负载因子大于等于1 b）或者满足服务器执bgsave或者bgrewriteaof命令，并且hash表负载因子大于等于5 （因为在执bgsave或者bgrewriteaof命令时，服务器在执行备份操作，为了尽可能提高其效率，避免在此期间进行hash表扩容操作） c）负载因子小于0.1会收缩 就会发生扩容，此时会渐进的将0下标的hash表的数据转移到扩容后的1下标处。（这里之所以采取渐进式的转移，主要是考虑到当hash表里面存的数据量很大时，一次性转移会很消耗时间）","categories":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"}],"tags":[{"name":"哈希表","slug":"哈希表","permalink":"http://example.com/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"}],"author":"John Doe"},{"title":"Redis底层数据结构之链表","slug":"Redis底层数据结构之链表","date":"2022-02-06T12:28:00.000Z","updated":"2022-02-06T12:35:31.761Z","comments":true,"path":"2022/02/06/Redis底层数据结构之链表/","link":"","permalink":"http://example.com/2022/02/06/Redis%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/","excerpt":"","text":"链表作为一种常用数据结构，Redis也对其进行了实现。链表键、发布与订阅、慢查询、监视器等方面都用到了链表。其底层由node节点和list结构构成，node节点包含前驱和后继指针以及value值，而list结构则包含了指向投节点、尾结点的指针以及链表节点数、节点复制函数、释放函数等。即最终是一个双端无环链表。","categories":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"}],"tags":[{"name":"链表","slug":"链表","permalink":"http://example.com/tags/%E9%93%BE%E8%A1%A8/"}],"author":"John Doe"},{"title":"Redis底层数据结构之SDS","slug":"Redis底层数据结构之SDS","date":"2022-02-06T12:13:00.000Z","updated":"2022-02-06T12:36:06.134Z","comments":true,"path":"2022/02/06/Redis底层数据结构之SDS/","link":"","permalink":"http://example.com/2022/02/06/Redis%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8BSDS/","excerpt":"","text":"Redis是用C语言写的，底层实现了众多数据结构，其中字符串是最常用的一种，其底层实现并不是用的C语言的char[]数组，而是进行了简单的封装sds结构体，定义了一个int的len、free和char[]来实现字符串。其相较于原生的C语言字符串有如下优点： 1、可以通过len-free以常数阶获取字符串长度 2、可以通过free字段避免缓冲区出现溢出的情况 3、同时也减少字符串修改时，内存重新分配的次数，其具体实现是通过预先分配内存（即当追加字符串之后，字符串小于1MB，会多分配一倍的空间）和懒惰回收（即当字符串变短之后，不会立即回收那一部分空间，而是作为临时空间供后续字符串扩增做优化） 4、可以保存二进制安全的数据","categories":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"}],"tags":[{"name":"字符串","slug":"字符串","permalink":"http://example.com/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"}],"author":"John Doe"},{"title":"枚举单例模式如何防止反射和反序列化","slug":"枚举单例模式如何防止反射和反序列化","date":"2022-01-27T11:08:00.000Z","updated":"2022-03-05T01:43:58.546Z","comments":true,"path":"2022/01/27/枚举单例模式如何防止反射和反序列化/","link":"","permalink":"http://example.com/2022/01/27/%E6%9E%9A%E4%B8%BE%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%E5%A6%82%E4%BD%95%E9%98%B2%E6%AD%A2%E5%8F%8D%E5%B0%84%E5%92%8C%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/","excerpt":"","text":"1、枚举单例在创建时不存在并发问题： 枚举类里的各个枚举项是是通过static代码块来定义和初始化的，它们会在类被加载时完成初始化，而java类的加载由JVM保证线程安全，所以，创建一个Enum类型的枚举是线程安全的 2、反序列化： Java对枚举的序列化作了规定，在序列化时，仅将枚举对象的name属性输出到结果中，在反序列化时，就是通过java.lang.Enum的valueOf来根据名字查找对象，而不是新建一个新的对象。枚举在序列化和反序列化时，并不会调用构造方法，这就防止了反序列化导致的单例破坏的问题。 3、反射： 反射在通过newInstance创建对象时，会检查这个类是否是枚举类，如果是，会抛出异常java.lang.IllegalArgumentException: Cannot reflectively create enum objects，表示反射创建对象失败。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"单例模式","slug":"单例模式","permalink":"http://example.com/tags/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"}],"author":"John Doe"},{"title":"单利模式防止反射创建新的实例","slug":"单利模式防止反射创建新的实例","date":"2022-01-27T09:47:00.000Z","updated":"2022-01-27T09:49:12.324Z","comments":true,"path":"2022/01/27/单利模式防止反射创建新的实例/","link":"","permalink":"http://example.com/2022/01/27/%E5%8D%95%E5%88%A9%E6%A8%A1%E5%BC%8F%E9%98%B2%E6%AD%A2%E5%8F%8D%E5%B0%84%E5%88%9B%E5%BB%BA%E6%96%B0%E7%9A%84%E5%AE%9E%E4%BE%8B/","excerpt":"","text":"方法一（饿汉式）：在私有的构造器里面增加判断，如果不为空，抛出异常之类 方法二（懒汉式）：可以增加一个静态变量，然后在类初始化的时候将静态变量修改值，然后在构造器内判断静态变量的值来做相应的操作","categories":[],"tags":[{"name":"单例模式","slug":"单例模式","permalink":"http://example.com/tags/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"}],"author":"John Doe"},{"title":"如果实现了序列化接口, 还要做什么来防止反序列化破坏单例","slug":"：如果实现了序列化接口-还要做什么来防止反序列化破坏单例","date":"2022-01-27T09:40:00.000Z","updated":"2022-01-27T09:47:09.071Z","comments":true,"path":"2022/01/27/：如果实现了序列化接口-还要做什么来防止反序列化破坏单例/","link":"","permalink":"http://example.com/2022/01/27/%EF%BC%9A%E5%A6%82%E6%9E%9C%E5%AE%9E%E7%8E%B0%E4%BA%86%E5%BA%8F%E5%88%97%E5%8C%96%E6%8E%A5%E5%8F%A3-%E8%BF%98%E8%A6%81%E5%81%9A%E4%BB%80%E4%B9%88%E6%9D%A5%E9%98%B2%E6%AD%A2%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E7%A0%B4%E5%9D%8F%E5%8D%95%E4%BE%8B/","excerpt":"","text":"在类中添加如下方法","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[],"author":"John Doe"},{"title":"Redis开发运维实践指南笔记","slug":"Redis开发运维实践指南笔记","date":"2022-01-26T14:10:01.000Z","updated":"2022-01-26T14:25:32.547Z","comments":true,"path":"2022/01/26/Redis开发运维实践指南笔记/","link":"","permalink":"http://example.com/2022/01/26/Redis%E5%BC%80%E5%8F%91%E8%BF%90%E7%BB%B4%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/","excerpt":"","text":"1、Redis为一个运行在内存中的数据结构服务器（data structures server）。Redis使用的是单进程（除持久化时），所以在配置时，一个实例只会用到一个CPU。 2、 3、列出key： 渐进的遍历整个数据库：keys命令会一次性遍历整个数据库获取与之匹配的键，当数据库包含得键值越来越多，这个命令会愈来愈慢，因此，可以用scan命令渐进的，分多次遍历整个数据库 4、 5、","categories":[],"tags":[],"author":"John Doe"},{"title":"交替打印输出","slug":"交替打印输出","date":"2022-01-26T12:59:00.000Z","updated":"2022-01-26T13:00:45.447Z","comments":true,"path":"2022/01/26/交替打印输出/","link":"","permalink":"http://example.com/2022/01/26/%E4%BA%A4%E6%9B%BF%E6%89%93%E5%8D%B0%E8%BE%93%E5%87%BA/","excerpt":"","text":"三个线程交替打印输出 public class AlternateOutput &#123; public static void main(String[] args) &#123; // Test1 test1 = new Test1(); // new Thread(()-&gt;&#123; // try &#123; // test1.print(1); // &#125; catch (InterruptedException e) &#123; // e.printStackTrace(); // &#125; // &#125;).start(); // new Thread(()-&gt;&#123; // try &#123; // test1.print(2); // &#125; catch (InterruptedException e) &#123; // e.printStackTrace(); // &#125; // &#125;).start(); // new Thread(()-&gt;&#123; // try &#123; // test1.print(3); // &#125; catch (InterruptedException e) &#123; // e.printStackTrace(); // &#125; // &#125;).start(); // Test2 test2 = new Test2(); // Thread t1 = new Thread(()-&gt;&#123; // try &#123; // test2.print(&quot;a&quot;); // &#125; catch (InterruptedException e) &#123; // e.printStackTrace(); // &#125; // &#125;); // Thread t2 = new Thread(()-&gt;&#123; // try &#123; // test2.print(&quot;b&quot;); // &#125; catch (InterruptedException e) &#123; // e.printStackTrace(); // &#125; // &#125;); // Thread t3 = new Thread(()-&gt;&#123; // try &#123; // test2.print(&quot;c&quot;); // &#125; catch (InterruptedException e) &#123; // e.printStackTrace(); // &#125; // &#125;); // test2.setThreads(t1,t2,t3); // test2.start(); Test3 test3 = new Test3(); Condition condition1 = test3.newCondition(); Condition condition2 = test3.newCondition(); Condition condition3 = test3.newCondition(); new Thread(()-&gt;&#123; test3.print(&quot;a&quot;,condition1,condition2); &#125;).start(); new Thread(()-&gt;&#123; test3.print(&quot;b&quot;,condition2,condition3); &#125;).start(); new Thread(()-&gt;&#123; test3.print(&quot;c&quot;,condition3,condition1); &#125;).start(); test3.start(condition1); &#125; &#125; class Test1&#123; private Integer flag = 1; private Integer num = 10; public void print(int curFlag) throws InterruptedException &#123; for (int i=0; i&lt;num; ++i)&#123; synchronized (this)&#123; while (this.flag != curFlag)&#123; this.wait(); &#125; System.out.println(curFlag); this.flag = curFlag % 3 + 1; this.notifyAll(); &#125; &#125; &#125; &#125; class Test2&#123; private Thread[] threads; private Integer num = 10; public Test2(Thread... threads) &#123; this.threads = threads; &#125; public void setThreads(Thread... threads) &#123; this.threads = threads; &#125; public void print(String s) throws InterruptedException &#123; for (int i=0;i&lt;num;++i)&#123; LockSupport.park(); System.out.println(s); LockSupport.unpark(getNextThread()); &#125; &#125; public Thread getNextThread()&#123; int size = threads.length; Thread cur = Thread.currentThread(); for (int i=0;i&lt;size;++i)&#123; if (cur == threads[i])&#123; return threads[(i + 1) % size]; &#125; &#125; return null; &#125; public void start() &#123; for (Thread thread: threads)&#123; thread.start(); &#125; LockSupport.unpark(threads[0]); &#125; &#125; class Test3 extends ReentrantLock&#123; private int num = 10; public void start(Condition condition)&#123; this.lock(); try &#123; condition.signal(); &#125;finally &#123; this.unlock(); &#125; &#125; public void print(String s, Condition cur,Condition next)&#123; for (int i=0;i&lt;num;++i)&#123; this.lock(); try&#123; cur.await(); System.out.println(s); next.signal(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; this.unlock(); &#125; &#125; &#125; &#125;","categories":[],"tags":[],"author":"John Doe"},{"title":"synchronized锁静态变量Integer","slug":"synchronized锁静态变量Integer","date":"2022-01-25T02:18:00.000Z","updated":"2022-01-25T02:22:28.673Z","comments":true,"path":"2022/01/25/synchronized锁静态变量Integer/","link":"","permalink":"http://example.com/2022/01/25/synchronized%E9%94%81%E9%9D%99%E6%80%81%E5%8F%98%E9%87%8FInteger/","excerpt":"","text":"当我尝试用synchronized去锁一个Integer的静态变量时，在多线程下发生了线程不安全问题，原因是synchronized锁住的Integer静态变量在不断发生变化，即i++会不断创建新的Integer，然后致使多线程下锁的不是一个对象，锁无效（以下代码在-128~127之间是有效的，因为存在Integer缓存问题）。 public class Main &#123; private static Integer i = 0; public static void main(String[] args) throws InterruptedException &#123; List&lt;Thread&gt; list = new ArrayList&lt;&gt;(); for (int j = 0; j &lt; 2; j++) &#123; Thread thread = new Thread(() -&gt; &#123; for (int k = 0; k &lt; 127; k++) &#123; synchronized (i) &#123; i++; &#125; &#125; &#125;, &quot;&quot; + j); list.add(thread); &#125; list.stream().forEach(t -&gt; t.start()); list.stream().forEach(t -&gt; &#123; try &#123; t.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); System.out.println(i); &#125; &#125;","categories":[{"name":"juc","slug":"juc","permalink":"http://example.com/categories/juc/"}],"tags":[{"name":"synchronized","slug":"synchronized","permalink":"http://example.com/tags/synchronized/"}],"author":"John Doe"},{"title":"终止模式之两阶段终止模式（Interrupt）","slug":"终止模式之两阶段终止模式","date":"2022-01-24T11:10:00.000Z","updated":"2022-01-24T11:20:45.418Z","comments":true,"path":"2022/01/24/终止模式之两阶段终止模式/","link":"","permalink":"http://example.com/2022/01/24/%E7%BB%88%E6%AD%A2%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%B8%A4%E9%98%B6%E6%AE%B5%E7%BB%88%E6%AD%A2%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"Thread类中的stop（）方法可以用于终止一个线程，但这个方法要求立即终止，被终止的线程没有机会料理后事。因此，这里采用终止模式中的两阶段终止模式来优雅的结束一个线程，给被终止的线程一个料理后事的机会。（如果被打断线程正在 sleep，wait，join 会导致被打断的线程抛InterruptedException，并清除 打断标记如果打断的正在运行的线程，则会设置打断标记 ；park的线程被打断，也会设置 打断标记） public class TPTInterrupt &#123; public static void main(String[] args) throws InterruptedException &#123; TPTInterrupt tptInterrupt = new TPTInterrupt(); tptInterrupt.start(); Thread.sleep(2000); tptInterrupt.stop(); &#125; private Thread thread; public void start()&#123; thread = new Thread(()-&gt;&#123; while (true)&#123; Thread thread = Thread.currentThread(); if (thread.isInterrupted())&#123; System.out.println(&quot;料理后事...&quot;); break; &#125; try &#123; Thread.sleep(1000); System.out.println(&quot;运行中...&quot;); &#125; catch (InterruptedException e) &#123; // 标记打断 e.printStackTrace(); thread.interrupt(); &#125; &#125; &#125;); thread.start(); &#125; public void stop()&#123; thread.interrupt(); &#125; &#125;","categories":[{"name":"juc","slug":"juc","permalink":"http://example.com/categories/juc/"},{"name":"设计模式","slug":"juc/设计模式","permalink":"http://example.com/categories/juc/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"终止模式","slug":"终止模式","permalink":"http://example.com/tags/%E7%BB%88%E6%AD%A2%E6%A8%A1%E5%BC%8F/"}],"author":"John Doe"},{"title":"创建线程的方式","slug":"创建线程的方式","date":"2022-01-24T11:02:00.000Z","updated":"2022-03-08T14:03:10.109Z","comments":true,"path":"2022/01/24/创建线程的方式/","link":"","permalink":"http://example.com/2022/01/24/%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8B%E7%9A%84%E6%96%B9%E5%BC%8F/","excerpt":"","text":"1、创建Thread对象 2、使用Runnable配合Thread使用 3、FutureTask配合Callable和Thread使用 4、通过线程池创建","categories":[],"tags":[{"name":"创建线程","slug":"创建线程","permalink":"http://example.com/tags/%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8B/"}],"author":"John Doe"},{"title":"为什么要自定义类型加载器？","slug":"为什么要自定义类型加载器？","date":"2022-01-23T11:19:00.000Z","updated":"2022-01-23T11:20:52.309Z","comments":true,"path":"2022/01/23/为什么要自定义类型加载器？/","link":"","permalink":"http://example.com/2022/01/23/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E8%87%AA%E5%AE%9A%E4%B9%89%E7%B1%BB%E5%9E%8B%E5%8A%A0%E8%BD%BD%E5%99%A8%EF%BC%9F/","excerpt":"","text":"1、隔离加载类：在某些框架内进行中间件与应用的模块隔离，把类加载到不同的环境。比如：阿里内某容器框架通过自定义类加载器确保应用中依赖的 jar 包不会影响到中间件运行时使用的 jar 包。再比如：Tomcat 这类 Web 应用服务器，内部自定义了好几种类加载器，用于隔离同一个 Web 应用服务器上的不同应用程序。(类的仲裁 –&gt; 类冲突) 2、修改类加载的方式：类的加载模型并非强制，除 Bootstrap 外，其他的加载并非一定要引入，或者根据实际情况在某个时间点按需进行动态加载 3、扩展加载源：比如从数据库、网络、甚至是电视机机顶盒进行加载 4、防止源码泄露：Java 代码容易被编译和篡改，可以进行编译加密。那么类加载也需要自定义，还原加密的字节码","categories":[{"name":"jv'm","slug":"jv-m","permalink":"http://example.com/categories/jv-m/"}],"tags":[],"author":"John Doe"},{"title":"沙箱安全机制","slug":"沙箱安全机制","date":"2022-01-23T11:16:00.000Z","updated":"2022-01-23T11:19:08.964Z","comments":true,"path":"2022/01/23/沙箱安全机制/","link":"","permalink":"http://example.com/2022/01/23/%E6%B2%99%E7%AE%B1%E5%AE%89%E5%85%A8%E6%9C%BA%E5%88%B6/","excerpt":"","text":"沙箱安全机制就是将java代码限定在jvm特定的运行范围，并且严格限制代码对本地资源的访问，本地系统造成破坏。","categories":[{"name":"jv'm","slug":"jv-m","permalink":"http://example.com/categories/jv-m/"}],"tags":[],"author":"John Doe"},{"title":"判定一个类型是否属于\"不再被使用的类\"的条件","slug":"判定一个类型是否属于-不再被使用的类-的条件","date":"2022-01-23T09:49:00.000Z","updated":"2022-01-23T09:50:58.551Z","comments":true,"path":"2022/01/23/判定一个类型是否属于-不再被使用的类-的条件/","link":"","permalink":"http://example.com/2022/01/23/%E5%88%A4%E5%AE%9A%E4%B8%80%E4%B8%AA%E7%B1%BB%E5%9E%8B%E6%98%AF%E5%90%A6%E5%B1%9E%E4%BA%8E-%E4%B8%8D%E5%86%8D%E8%A2%AB%E4%BD%BF%E7%94%A8%E7%9A%84%E7%B1%BB-%E7%9A%84%E6%9D%A1%E4%BB%B6/","excerpt":"","text":"1、该类所有的实例都被回收，即java堆中不存在该类的实例或其子类实例对象 2、加载该类的类加载器被回收 3、该类对应的java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法","categories":[{"name":"jv'm","slug":"jv-m","permalink":"http://example.com/categories/jv-m/"},{"name":"jvm","slug":"jv-m/jvm","permalink":"http://example.com/categories/jv-m/jvm/"}],"tags":[],"author":"John Doe"},{"title":"类的初始化情况：主动使用vs被动使用","slug":"类的初始化情况：主动使用vs被动使用","date":"2022-01-23T09:31:00.000Z","updated":"2022-01-23T09:43:56.451Z","comments":true,"path":"2022/01/23/类的初始化情况：主动使用vs被动使用/","link":"","permalink":"http://example.com/2022/01/23/%E7%B1%BB%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96%E6%83%85%E5%86%B5%EF%BC%9A%E4%B8%BB%E5%8A%A8%E4%BD%BF%E7%94%A8vs%E8%A2%AB%E5%8A%A8%E4%BD%BF%E7%94%A8/","excerpt":"","text":"主动使用：类只有在首次主动使用时才会被加载，而在首次使用被加载时，必须进行初始化。 以下使用被认为是主动使用：1、当创建一个类的实例时，比如使用 new 关键字，或者通过反射、克隆、反序列化 2、当调用类的静态方法时，即当使用了字节码 invokestatic 指令 3、当使用类、接口的静态字段时(final 修饰特殊考虑)，比如，使用 getstatic 或者 putsttic 指令。(对应访问变量、赋值变量操作) 4、当使用 java.lang.reflect 包中的方法反射类的方法时。比如：Class.forname(“com.atguigu.java.Test”) 5、当初始化子类时，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化 6、如果一个接口定义了 default 方法，那么直接实现或者间接实现该接口的类的初始化，该接口要在其之前被初始化 7、当虚拟机启动时，用户需要指定一个要执行的主类(包含 main() 方法的那个类)，虚拟机会先初始化这个主类 8、当初次调用 MethodHandle 实例时，初始化该 MethodHandle 指向的方法所在的类。(涉及解析 REF_getStatic、REF_putStatic、REF_invokeStatic 方法句柄对应的类) 注意：1、当Java 虚拟机初始化一个类时，要求它的所有父类都已经被初始化，但是这条规则并不适用于接口在初始化一个类时，并不会先初始化它所实现的接口在初始化一个接口时，并不会先初始化它的父接口。因此，一个父接口并不会因为它的子接口或者实现类的初始化而初始化，只有当程序首次使用特定接口的静态字段时，才会导致该接口的初始化2、JVM 启动的时候通过引导类加载器加载一个初始类。这个类在调用 public static void main(String[]) 方法之前被链接和初始化。这个方法的执行将依次导致所需的类的加载、链接和初始化 被动使用：除了以上的情况属于主动使用，其他的情况均属于被动使用。被动使用不会引起类的初始化。也就是说：并不是在代码中出现的类，就一定会被加载或者初始化。如果不符合主动使用的条件，类就不会初始化。 1、当访问一个静态字段时，只有真正声明这个字段的类才会被初始化 2、当通过子类引用父类的静态变量，不会导致子类初始化 3、通过数组定义类引用，不会触发此类的初始化 4、引用变量不会触发此类或接口的初始化。因为常量在链接阶段就已经被显式赋值了 5、调用 ClassLoader 类的 loadClass() 方法加载一个类，并不是对类的主动使用，不会导致类的初始化","categories":[{"name":"jv'm","slug":"jv-m","permalink":"http://example.com/categories/jv-m/"}],"tags":[{"name":"类初始化：主动使用与被动使用","slug":"类初始化：主动使用与被动使用","permalink":"http://example.com/tags/%E7%B1%BB%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%9A%E4%B8%BB%E5%8A%A8%E4%BD%BF%E7%94%A8%E4%B8%8E%E8%A2%AB%E5%8A%A8%E4%BD%BF%E7%94%A8/"}],"author":"John Doe"},{"title":"类的加载过程","slug":"类的加载过程","date":"2022-01-23T07:55:00.000Z","updated":"2022-01-23T08:54:30.085Z","comments":true,"path":"2022/01/23/类的加载过程/","link":"","permalink":"http://example.com/2022/01/23/%E7%B1%BB%E7%9A%84%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B/","excerpt":"","text":"类的加载，我的理解就是将类的二进制字节码文件加载到内存中，并通过解析字节码中的常量池、类字段、类方法等信息，在jvm方法区中构建出该类的模板，并在堆区创建一个对象实例作为方法区这个类的各种数据访问入口，在jvm运行期间能够通过这个类的模板信息来调用类的静态变量、方法等。其加载过程的话，主要分为加载、链接（验证、准备、解析）、初始化三个步骤。 1、首先加载，就是查找类的全限定类名，将类的二进制字节码文件加载到jvm的内存中，将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构，并为之在堆区创建一个实例对象，作为方法区这个类的数据访问入口。当然，在加载前还需要进行验证操作，即检查字节码文件格式，看是否遵循jvm的规范。比如是否以魔数开头等。验证通过后，该类的二进制信息便会被加载到内存。 2、加载到方法区后需要验证，即检查类的语义、字节码验证、符号引用验证，看是否符合规范。 3、当验证完毕之后，就开始准备阶段，这一步主要是对类的静态变量分配内存并附上默认值。（注意：final修饰的静态变量在编译阶段就会分配，准备阶段是显示赋值，并且此阶段也不会为实例变量分配初始化） 4、然后便是解析阶段，即将符号引用转变为直接引用，得到类、字段、方法等在内存中的指针或者偏移量。 5、最后便是初始化，这个阶段主要是为类的静态变量进行显示赋值。即执行类构造器cinit方法。即执行类变量的赋值动作和静态语句块(static{}块)，虚期机会保证在子类的()方法执行之前, 父类的()方法已经执行完毕。如果一个类中没有静态语句块,也没有对变量的赋值操作, 那么编译器可以不为这个类生成()方法。 需要注意的是：接口与类不同的是, 执行接口的()方法不需要先执行父接口的()方法。只有当父接口中定义的变量被使用时, 父接口才会被初始化。 另外, 接口的实现类在初始化时也一样不会执行接口的()方法。另外，虚拟机会保证一个类的()方法在多线程环境中被正确地加锁和同步，如果多个线程同时去初始化一个类，那么只会有一个线程去执行这个类的()法,其他线程部需要阻塞等待，直到活动线程执行()方法完毕。如果在一个类的()方法中有耗时很长的操作, 那就可能造成多个进程阻塞, 在实际应用中这种阻塞往往是隐蔽的。","categories":[{"name":"jv'm","slug":"jv-m","permalink":"http://example.com/categories/jv-m/"}],"tags":[{"name":"类的加载过程","slug":"类的加载过程","permalink":"http://example.com/tags/%E7%B1%BB%E7%9A%84%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B/"}],"author":"John Doe"},{"title":"关于集合类中的modCount++","slug":"关于集合类中的modCount","date":"2022-01-21T06:45:00.000Z","updated":"2022-01-21T07:17:15.741Z","comments":true,"path":"2022/01/21/关于集合类中的modCount/","link":"","permalink":"http://example.com/2022/01/21/%E5%85%B3%E4%BA%8E%E9%9B%86%E5%90%88%E7%B1%BB%E4%B8%AD%E7%9A%84modCount/","excerpt":"","text":"话不多说，直接看源码注释讲解 由上图可知，该字段目的在于记录集合结构被修改的次数（增、删、改），该字段被迭代器所使用，当对集合进行迭代遍历时，防止数据发生改变引起错误。因此，当我们使用迭代器时，如果该值被改了，就会触发fast-fail机制，抛出异常ConcurrentModificationExceptions。","categories":[{"name":"集合","slug":"集合","permalink":"http://example.com/categories/%E9%9B%86%E5%90%88/"},{"name":"java","slug":"集合/java","permalink":"http://example.com/categories/%E9%9B%86%E5%90%88/java/"}],"tags":[{"name":"ArrayList","slug":"ArrayList","permalink":"http://example.com/tags/ArrayList/"}],"author":"John Doe"},{"title":"HashMap1.7","slug":"HashMap","date":"2022-01-18T11:27:00.000Z","updated":"2022-01-18T11:53:34.793Z","comments":true,"path":"2022/01/18/HashMap/","link":"","permalink":"http://example.com/2022/01/18/HashMap/","excerpt":"","text":"HashMap1.7底层由数组+链表实现，提供的无参构造方法默认数组容量是16，加载因子是0.75，临界值为16 * 0.75 = 12，当到了临界值则进行扩容。 你也可以通过有参构造方法指定容量和加载因子。 注意，不管有参无参此时都还未初始化数组，只是定义了数组容量。当我们第一次put往hashmap的放数据的时候才会初始化 而此时初始化会根据我们最初的容量大小进行初始化，大小为大于等于当前容量的2的幂。 然后便是计算hash值，根据hash值得到数组下标，根据下标到指定位置，如果发送hash冲突则通过拉链法，将冲突元素头插进链表","categories":[{"name":"java基础","slug":"java基础","permalink":"http://example.com/categories/java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"集合","slug":"集合","permalink":"http://example.com/tags/%E9%9B%86%E5%90%88/"},{"name":"HashMap","slug":"HashMap","permalink":"http://example.com/tags/HashMap/"}],"author":"John Doe"},{"title":"ArrayList","slug":"ArrayList","date":"2022-01-17T12:13:00.000Z","updated":"2022-01-17T12:20:04.535Z","comments":true,"path":"2022/01/17/ArrayList/","link":"","permalink":"http://example.com/2022/01/17/ArrayList/","excerpt":"","text":"ArrayList 1、ArrayList底层默认是用object数组实现的，因此在增删元素上需要移动元素，效率较低，但支持随机访问元素 2、ArrayList是线层不安全的，并发环境下，多个线程同时操作 ArrayList，会引发不可预知的异常或错误。 3、ArrayList的默认的大小是10。一开始是空数组，当第一次add的时候才会扩容到10，后续容器满了之后会按1.5倍进行扩容。如果一开始指定容器大小，后续则直接按1.5倍进行扩容。最大扩容不超过Integer.MAX_VALUE","categories":[{"name":"java基础","slug":"java基础","permalink":"http://example.com/categories/java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"集合","slug":"集合","permalink":"http://example.com/tags/%E9%9B%86%E5%90%88/"},{"name":"List","slug":"List","permalink":"http://example.com/tags/List/"}],"author":"John Doe"},{"title":"成员变量与局部变量","slug":"成员变量与局部变量","date":"2022-01-16T11:20:00.000Z","updated":"2022-01-16T11:22:59.161Z","comments":true,"path":"2022/01/16/成员变量与局部变量/","link":"","permalink":"http://example.com/2022/01/16/%E6%88%90%E5%91%98%E5%8F%98%E9%87%8F%E4%B8%8E%E5%B1%80%E9%83%A8%E5%8F%98%E9%87%8F/","excerpt":"","text":"从语法形式上看:成员变量是属于类的，⽽局部变量是在⽅法中定义的变量或是⽅法的参数；成员变量可以被 public , private , static 等修饰符所修饰，⽽局部变量不能被访问控制修饰符及 static 所修饰；但是，成员变量和局部变量都能被 final 所修饰。2. 从变量在内存中的存储⽅式来看:如果成员变量是使⽤ static 修饰的，那么这个成员变量是属于类的，如果没有使⽤ static 修饰，这个成员变量是属于实例的。对象存于堆内存，如果局部变量类型为基本数据类型，那么存储在栈内存，如果为引⽤数据类型，那存放的是指向堆内存对象的引⽤或者是指向常量池中的地址。3. 从变量在内存中的⽣存时间上看:成员变量是对象的⼀部分，它随着对象的创建⽽存在，⽽局部变量随着⽅法的调⽤⽽⾃动消失。4. 成员变量如果没有被赋初值:则会⾃动以类型的默认值⽽赋值（⼀种情况例外:被 final 修饰的成员变量也必须显式地赋值），⽽局部变量则不会⾃动赋值。","categories":[{"name":"Java","slug":"Java","permalink":"http://example.com/categories/Java/"}],"tags":[],"author":"John Doe"},{"title":"接口和抽象类的区别","slug":"接口和抽象类的区别","date":"2022-01-16T10:39:00.000Z","updated":"2022-01-16T11:17:43.700Z","comments":true,"path":"2022/01/16/接口和抽象类的区别/","link":"","permalink":"http://example.com/2022/01/16/%E6%8E%A5%E5%8F%A3%E5%92%8C%E6%8A%BD%E8%B1%A1%E7%B1%BB%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"接⼝的⽅法默认是 public ，所有⽅法在接⼝中不能有实现(Java 8 开始接⼝⽅法可以有默认实现），⽽抽象类可以有⾮抽象的⽅法。 接⼝中除了 static 、 final 变量，不能有其他变量，⽽抽象类中则不⼀定。 ⼀个类可以实现多个接⼝，但只能实现⼀个抽象类。接⼝⾃⼰本身可以通过 extends 关键字扩展多个接⼝。 接⼝⽅法默认修饰符是 public ，抽象⽅法可以有 public 、 protected 和 default 这些修饰符（抽象⽅法就是为了被重写所以不能使⽤ private 关键字修饰！）。 从设计层⾯来说，抽象是对类的抽象，是⼀种模板设计，⽽接⼝是对⾏为的抽象，是⼀种⾏为的规范。备注： 在 JDK8 中，接⼝也可以定义静态⽅法，可以直接⽤接⼝名调⽤。实现类和实现是不可以调⽤的。如果同时实现两个接⼝，接⼝中定义了⼀样的默认⽅法，则必须重写，不然会报错。 jdk9 的接⼝被允许定义私有⽅法 。总结⼀下 jdk7~jdk9 Java 中接⼝概念的变化： 在 jdk 7 或更早版本中，接⼝⾥⾯只能有常量变量和抽象⽅法。这些接⼝⽅法必须由选择实现接⼝的类实现。 jdk 8 的时候接⼝可以有默认⽅法和静态⽅法功能。 Jdk 9 在接⼝中引⼊了私有⽅法和私有静态⽅法。","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://example.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"接口","slug":"接口","permalink":"http://example.com/tags/%E6%8E%A5%E5%8F%A3/"}],"author":"John Doe"},{"title":"在 Java 中定义⼀个不做事且没有参数的构造⽅法的作⽤","slug":"在-Java-中定义⼀个不做事且没有参数的构造⽅法的作⽤","date":"2022-01-16T10:34:00.000Z","updated":"2022-01-16T10:34:43.521Z","comments":true,"path":"2022/01/16/在-Java-中定义⼀个不做事且没有参数的构造⽅法的作⽤/","link":"","permalink":"http://example.com/2022/01/16/%E5%9C%A8-Java-%E4%B8%AD%E5%AE%9A%E4%B9%89%E2%BC%80%E4%B8%AA%E4%B8%8D%E5%81%9A%E4%BA%8B%E4%B8%94%E6%B2%A1%E6%9C%89%E5%8F%82%E6%95%B0%E7%9A%84%E6%9E%84%E9%80%A0%E2%BD%85%E6%B3%95%E7%9A%84%E4%BD%9C%E2%BD%A4/","excerpt":"","text":"Java 程序在执⾏⼦类的构造⽅法之前，如果没有⽤ super() 来调⽤⽗类特定的构造⽅法，则会调⽤⽗类中“没有参数的构造⽅法”。因此，如果⽗类中只定义了有参数的构造⽅法，⽽在⼦类的构造⽅法中⼜没有⽤ super() 来调⽤⽗类中特定的构造⽅法，则编译时将发⽣错误，因为 Java 程序在⽗类中找不到没有参数的构造⽅法可供执⾏。解决办法是在⽗类⾥加上⼀个不做事且没有参数的构造⽅法。","categories":[{"name":"Java","slug":"Java","permalink":"http://example.com/categories/Java/"}],"tags":[],"author":"John Doe"},{"title":"重载和重写的区别","slug":"重载和重写的区别","date":"2022-01-16T10:05:00.000Z","updated":"2022-01-16T10:25:19.715Z","comments":true,"path":"2022/01/16/重载和重写的区别/","link":"","permalink":"http://example.com/2022/01/16/%E9%87%8D%E8%BD%BD%E5%92%8C%E9%87%8D%E5%86%99%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"重载：在一个java类中，有多个方法同名，签名不同（即不同的参数数量、顺序以及类型）的函数，就发生了重载。 重写：子类继承了父类然后重写了父类中的方法（保持和父类方法的返回值类型、函数名、签名等都不变，只是在函数体中的代码实现逻辑改变了） 注意：重写时，子类抛出的异常范围应小于等于父类，访问修饰变量范围应大于等于父类。如果父类方法被final、static、private修饰则子类不能重写父类方法，但被static修饰的方法可以再次声明。","categories":[{"name":"Java","slug":"Java","permalink":"http://example.com/categories/Java/"}],"tags":[{"name":"重写","slug":"重写","permalink":"http://example.com/tags/%E9%87%8D%E5%86%99/"},{"name":"重载","slug":"重载","permalink":"http://example.com/tags/%E9%87%8D%E8%BD%BD/"}],"author":"ATAO"},{"title":"Java中的基本数据类型","slug":"Java中的基本数据类型","date":"2022-01-15T10:31:00.000Z","updated":"2022-01-15T10:35:39.104Z","comments":true,"path":"2022/01/15/Java中的基本数据类型/","link":"","permalink":"http://example.com/2022/01/15/Java%E4%B8%AD%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","excerpt":"","text":"java中的基本数据类型有byte（1字节）、short（2字节）、int（四字节）、long（8字节）、float（4字节）、double（8字节）、boolean（1位）、char（2字节）","categories":[{"name":"Java","slug":"Java","permalink":"http://example.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}],"author":"ATAO"},{"title":"final、finalize()、finally","slug":"final、finalize-、finally","date":"2022-01-11T01:48:00.000Z","updated":"2022-01-11T02:27:34.968Z","comments":true,"path":"2022/01/11/final、finalize-、finally/","link":"","permalink":"http://example.com/2022/01/11/final%E3%80%81finalize-%E3%80%81finally/","excerpt":"","text":"1、final：在java中，final主要用于修饰类、方法和变量。 1.1 修饰类：用final修饰类时，表明这个类不能被其他类所继承。 注意：当用final对类进行修饰的时候，类中所有成员方法默认是final方法。 1.2 修饰方法：用fianl修饰方法时，表明这个方法不能被其子类重写。 1.3 修饰变量：用final修饰变量的话表明这个变量是一个常量，只能被赋值一次，赋值后其值不能够修改。 当final修饰一个基本数据类型时，表示该基本数据类型的值一旦在初始化后便不能发生变化（）；如果final修饰一个引用类型时，则在对其初始化之后便不能再让其指向其他对象了，但该引用所指向的对象的内容是可以发生变化的。本质上是一回事，因为引用的值是一个地址，final要求值，即地址的值不发生变化。 final修饰一个成员变量（属性），必须要显示初始化。这里有两种初始化方式，一种是在变量声明的时候初始化；第二种方法是在声明变量的时候不赋初值，但是要在这个变量所在的类的所有的构造函数中对这个变量赋初值。 扩展：在java中，String被设计成final类，那为什么平时使用时，String的值可以被改变呢？ 字符串常量池是java堆内存中一个特殊的存储区域，当我们建立一个String对象时，假设常量池不存在该字符串，则创建一个，若存在则直接引用已经存在的字符串。当我们对String对象值改变的时候，例如 String a=&quot;A&quot;; a=&quot;B&quot; 。a是String对象的一个引用（我们这里所说的String对象其实是指字符串常量），当a=“B”执行时，并不是原本String对象(&quot;A&quot;)发生改变，而是创建一个新的对象(&quot;B&quot;)，令a引用它。 2、finally：finally作为异常处理的一部分，它用在try/catch语句中，经常被用在需要释放资源的情况下。 注意： 1、只有与finally对应的try语句块得到执行的情况下，finally语句块才会执行。 2、在 try 语句块中执行了 System.exit (0) 语句，终止了 Java 虚拟机的运行或者在执行 try 语句块或者 catch 语句块时被打断（interrupted）或者被终止（killed）等情况finally也可能不会执行。 易错点： 答案：4 4 4 原因：finally语句在return之前执行。 3、finalize：finalize()是在java.lang.Object里定义的，每一个对象都有这么个方法。这个方法在gc启动，该对象被回收的时候被调用。 注意：一个对象的finalize()方法只会被调用一次，而且finalize()被调用不意味着gc会立即回收该对象，所以有可能调用finalize()后，该对象又不需要被回收了，然后到了真正要被回收的时候，因为前面调用过一次，所以不会调用finalize()，产生问题。 所以，推荐不要使用finalize()方法，它跟析构函数不一样。","categories":[{"name":"java","slug":"java","permalink":"http://example.com/categories/java/"}],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"},{"name":"final、finally、fi'nalize","slug":"final、finally、fi-nalize","permalink":"http://example.com/tags/final%E3%80%81finally%E3%80%81fi-nalize/"}],"author":"ATAo"},{"title":"为什么Integer 100==100 为true，而Integer 1000==1000为false？","slug":"为什么Integer-100-100-为true，而Integer-1000-1000为false？","date":"2022-01-08T13:39:00.000Z","updated":"2022-01-08T14:19:54.922Z","comments":true,"path":"2022/01/08/为什么Integer-100-100-为true，而Integer-1000-1000为false？/","link":"","permalink":"http://example.com/2022/01/08/%E4%B8%BA%E4%BB%80%E4%B9%88Integer-100-100-%E4%B8%BAtrue%EF%BC%8C%E8%80%8CInteger-1000-1000%E4%B8%BAfalse%EF%BC%9F/","excerpt":"","text":"在java的Integer包装类中为什么Integer a = 100, b = 100,c = 1000,d =1000，令a==b为true，而c==d为false呢？ 首先在上面的代码中，Integer a = 100会调用Integer的Integer.valueOf(int i)这个方法，而这个方法的源代码如下： 我们会发现在将int类型装箱时做了一个判断语句 if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) 这句话是什么意思呢？通过注释可以知道，在将一个int类型装箱为Integer类型时，总会优先调用此方法。其存在一个-128到127范围的缓存，如果int类型时该范围内，则直接返回缓存中的值，不需要额外创建Integer类型，这样可以产生显著更好的空间和时间性能。而且其范围也是可以设置。 所有我们可以知道，在执行Integer a = 100，b = 100时，走了缓存，因此b的地址同a应该一样，而c = 1000, d = 1000并为走缓存，而是走的new Interger(i)，因此创建了两个对象。 而我们知道，==比较走的是比较对象的地址。因此才会有为什么Integer 100==100 为true，而Integer 1000==1000为false。 debug如下：","categories":[{"name":"java基础","slug":"java基础","permalink":"http://example.com/categories/java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Integer","slug":"Integer","permalink":"http://example.com/tags/Integer/"}],"author":"ATAO"},{"title":"快速选择","slug":"快速选择","date":"2022-01-08T06:04:00.000Z","updated":"2022-01-08T06:08:50.171Z","comments":true,"path":"2022/01/08/快速选择/","link":"","permalink":"http://example.com/2022/01/08/%E5%BF%AB%E9%80%9F%E9%80%89%E6%8B%A9/","excerpt":"","text":"快速选择，适用于寻找无序数组中第k大（小）或者前k大（小）这种情况，即TopK问题，是快速排序的一种变化。 class Solution &#123; public static int[] getLeastNumbers(int[] arr, int k) &#123; return quickSelect(arr,k,0,arr.length-1); &#125; private static int[] quickSelect(int[] arr,int k,int l, int r)&#123; if (arr.length &lt;= k)&#123; return arr; &#125; int index = quickSort(arr,l,r); if (index == k)&#123; return Arrays.copyOf(arr,index); &#125;else if (index &lt; k)&#123; return quickSelect(arr,k,index+1,r); &#125;else &#123; return quickSelect(arr,k,l,index-1); &#125; &#125; private static int quickSort(int[] arr, int l, int r) &#123; int mid = l ,i = l, j = r; while (i &lt; j)&#123; while (i&lt;j &amp;&amp; arr[j] &gt;= arr[mid])&#123; --j; &#125; if(i&lt;j &amp;&amp; arr[j] &lt; arr[mid])&#123; swap(arr,j,mid); mid = j; &#125; // p(arr); while (i&lt;j &amp;&amp; arr[i] &lt;= arr[mid])&#123; ++i; &#125; if(i&lt;j &amp;&amp; arr[i] &gt; arr[mid])&#123; swap(arr,i,mid); mid = i; &#125; // p(arr); &#125; swap(arr,i,mid); return mid; &#125; private static void swap(int[] arr, int i, int j) &#123; int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp; &#125; public static void p(int[] arr)&#123; for (int n:arr)&#123; System.out.print(n+&quot; &quot;); &#125; System.out.println(); &#125; &#125;","categories":[{"name":"算法","slug":"算法","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"快速选择","slug":"快速选择","permalink":"http://example.com/tags/%E5%BF%AB%E9%80%9F%E9%80%89%E6%8B%A9/"},{"name":"Top","slug":"Top","permalink":"http://example.com/tags/Top/"}],"author":"ATAO"},{"title":"快速排序","slug":"快速排序","date":"2022-01-08T05:41:00.000Z","updated":"2022-01-08T05:48:02.025Z","comments":true,"path":"2022/01/08/快速排序/","link":"","permalink":"http://example.com/2022/01/08/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/","excerpt":"","text":"原理： 设要排序的数组是A[0]……A[N-1]，首先任意选取一个数据（通常选用数组的第一个数）作为关键数据，然后将所有比它小的数都放到它左边，所有比它大的数都放到它右边，这个过程称为一趟快速排序。值得注意的是，快速排序不是一种稳定的排序算法，也就是说，多个相同的值的相对位置也许会在算法结束时产生变动。 一趟快速排序的算法是： 1）设置两个变量i、j，排序开始的时候：i=0，j=N-1； 2）以第一个数组元素作为关键数据，赋值给key，即key=A[0]； 3）从j开始向前搜索，即由后开始向前搜索(j--)，找到第一个小于key的值A[j]，将A[j]和A[i]的值交换； 4）从i开始向后搜索，即由前开始向后搜索(i++)，找到第一个大于key的A[i]，将A[i]和A[j]的值交换； 5）重复第3、4步，直到i==j； (3,4步中，没找到符合条件的值，即3中A[j]不小于key,4中A[i]不大于key的时候改变j、i的值，使得j=j-1，i=i+1，直至找到为止。找到符合条件的值，进行交换的时候i， j指针位置不变。另外，i==j这一过程一定正好是i+或j-完成的时候，此时令循环结束）。 代码： public class QuickSort &#123; public static void main(String[] args) &#123; int k = 3; int[] arr = &#123;5,3,7,6,4,1,0,2,9,10,8&#125;; quickSort(arr,0,arr.length-1); p(arr); &#125; private static int[] quickSort(int[] arr,int l, int r) &#123; if (l&gt;r) return null; int mid = l ,i = l, j = r; while (i &lt; j)&#123; while (i&lt;j &amp;&amp; arr[j] &gt;= arr[mid])&#123; --j; &#125; if(i&lt;j &amp;&amp; arr[j] &lt; arr[mid])&#123; swap(arr,j,mid); mid = j; &#125; p(arr); while (i&lt;j &amp;&amp; arr[i] &lt;= arr[mid])&#123; ++i; &#125; if(i&lt;j &amp;&amp; arr[i] &gt; arr[mid])&#123; swap(arr,i,mid); mid = i; &#125; p(arr); &#125; System.out.println(&quot;=========&quot;+i); swap(arr,i,mid); mid = i; quickSort(arr,mid+1,r); quickSort(arr,l,mid-1); return arr; &#125; private static void swap(int[] arr, int i, int j) &#123; int tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp; &#125; public static void p(int[] arr)&#123; for (int n:arr)&#123; System.out.print(n+&quot; &quot;); &#125; System.out.println(); &#125; }","categories":[{"name":"算法","slug":"算法","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"快速排序","slug":"快速排序","permalink":"http://example.com/tags/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/"}],"author":"John Doe"},{"title":"进程通信的方式","slug":"有了for循环，为什么还需要forEach？","date":"2022-01-07T12:48:00.000Z","updated":"2022-01-07T12:59:11.982Z","comments":true,"path":"2022/01/07/有了for循环，为什么还需要forEach？/","link":"","permalink":"http://example.com/2022/01/07/%E6%9C%89%E4%BA%86for%E5%BE%AA%E7%8E%AF%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E9%9C%80%E8%A6%81forEach%EF%BC%9F/","excerpt":"","text":"在操作系统中，进程是资源分配的基本单位，进程间如果要实现通信，有共享存储、消息传递、管道以及socket这几种方式。 共享存储： 1、基于数据结构的共享：比如共享空间里只能放 一个长度为10的数组。这种共享方式速度慢、 限制多，是一种低级通信方式。 2、基于存储区的共享：在内存中画出一块共享存 储区，数据的形式、存放位置都由进程控制， 而不是操作系统。相比之下，这种共享方式速 度更快，是一种高级通信方式。 管道通信： 1. 管道只能采用半双工通信，某一时间段内只能实现单向的传输。如果要实现双向同时通信，则需要设置 两个管道。 2. 各进程要互斥地访问管道。 3. 数据以字符流的形式写入管道，当管道写满时，写进程的write()系统调用将被阻塞，等待读进程将数据 取走。当读进程将数据全部取走后，管道变空，此时读进程的read()系统调用将被阻塞。 4. 如果没写满，就不允许读。如果没读空，就不允许写。 5. 数据一旦被读出，就从管道中被抛弃，这就意味着读进程最多只能有一个，否则可能会有读错数据的情 况。 消息传递： 1、直接传递：发送到接收方的接受缓冲队列上 2、间接传递：设置一个接受中转信箱，发送方发送的消息发送到信箱，接收方接受消息从信箱中取 socket：借助于网络通信，从一个主机的进程发送消息到另一个主机上的进程。","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"计算机网络","slug":"计算机基础/计算机网络","permalink":"http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"计算机基础","slug":"计算机基础","permalink":"http://example.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"计算机网络","slug":"计算机网络","permalink":"http://example.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"author":"ATAO"},{"title":"单例模式","slug":"单例模式","date":"2022-01-05T08:04:00.000Z","updated":"2022-03-05T01:43:05.127Z","comments":true,"path":"2022/01/05/单例模式/","link":"","permalink":"http://example.com/2022/01/05/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"单例模式，即java类不对外提供构造方法，在类加载的时候创建一个实例化的对象，或者提供一个方法，在方法中作出限制，保证例化对象的创建全局只有唯一一个。其好处在于可以节约系统资源，在资源共享的情况下，避免由于资源操作时导致的性能或损耗等。如日志文件，应用配置。在控制资源的情况下，方便资源之间的互相通信。如线程池等。一般可以用于网站的计数器、web应用的日志、配置对象的读取、打印机、任务管理器、数据库连接池。其具体代码实现可以分为饿汉式和懒汉式。 /** 饿汉式 /public class Singleton01 { public static final Singleton01 instance = new Singleton01(); private Singleton01(){ }}/** 饿汉式 /public class Singleton02 { public static final Singleton02 instance; static { instance = new Singleton02(); } private Singleton02(){ } }/** 饿汉式 /public enum Singleton03 { INSTANCE}/** 懒汉式: 存在线程安全问题 /public class Singleton04 { private static Singleton04 instance; private Singleton04(){ } public static Singleton04 getInstance(){ if (instance == null)&#123; instance = new Singleton04(); &#125; return instance; }}/** 懒汉式: 解决线程安全问题 /public class Singleton05 { private volatile static Singleton05 instance; private Singleton05(){ } public static Singleton05 getInstance(){ if (instance == null)&#123; synchronized (Singleton05.class)&#123; instance = new Singleton05(); &#125; &#125; return instance; }}/** 懒汉式: 内部类 /public class Singleton06 { private Singleton06(){ } private static class Inner{ private static final Singleton06 instance = new Singleton06(); } public static Singleton06 getInstance(){ return Inner.instance; }} 总结： 添加 volatile 关键字之后的双重检查锁模式是一种比较好的单例实现模式，能够保证在多线程的情况下线程安全也不会有性能问题。 静态内部类单例模式中实例由内部类创建，由于 JVM 在加载外部类的过程中, 是不会加载静态内部类的, 只有内部类的属性/方法被调用时才会被加载, 并初始化其静态属性。静态属性由于被static 修饰，保证只被实例化一次，并且严格保证实例化顺序。 静态内部类单例模式是一种优秀的单例模式，是开源项目中比较常用的一种单例模式。在没有加任何锁的情况下，保证了多线程下的安全，并且没有任何性能影响和空间的浪费。 枚举类实现单例模式是极力推荐的单例实现模式，因为枚举类型是线程安全的，并且只会装载一次，设计者充分的利用了枚举的这个特性来实现单例模式，枚举的写法非常简单，而且枚举类型是所用单例实现中唯一一种不会被破坏的单例实现模式。","categories":[{"name":"java","slug":"java","permalink":"http://example.com/categories/java/"},{"name":"设计模式","slug":"java/设计模式","permalink":"http://example.com/categories/java/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"},{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"author":"John Doe"},{"title":"Hello World","slug":"hello-world","date":"2022-01-05T06:09:04.194Z","updated":"2022-01-05T06:09:04.194Z","comments":true,"path":"2022/01/05/hello-world/","link":"","permalink":"http://example.com/2022/01/05/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/categories/Spring/"},{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/categories/MySQL/"},{"name":"分布式","slug":"分布式","permalink":"http://example.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"Zookeeper","slug":"Zookeeper","permalink":"http://example.com/categories/Zookeeper/"},{"name":"Java","slug":"Java","permalink":"http://example.com/categories/Java/"},{"name":"并发编程","slug":"并发编程","permalink":"http://example.com/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"JMM","slug":"JMM","permalink":"http://example.com/categories/JMM/"},{"name":"并发编程","slug":"JMM/并发编程","permalink":"http://example.com/categories/JMM/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"编译原理","slug":"编译原理","permalink":"http://example.com/categories/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"},{"name":"LSM","slug":"LSM","permalink":"http://example.com/categories/LSM/"},{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"},{"name":"集合","slug":"Java/集合","permalink":"http://example.com/categories/Java/%E9%9B%86%E5%90%88/"},{"name":"集合","slug":"集合","permalink":"http://example.com/categories/%E9%9B%86%E5%90%88/"},{"name":"Java","slug":"集合/Java","permalink":"http://example.com/categories/%E9%9B%86%E5%90%88/Java/"},{"name":"JMM","slug":"Java/JMM","permalink":"http://example.com/categories/Java/JMM/"},{"name":"主从复制","slug":"MySQL/主从复制","permalink":"http://example.com/categories/MySQL/%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/"},{"name":"String","slug":"String","permalink":"http://example.com/categories/String/"},{"name":"字符串","slug":"String/字符串","permalink":"http://example.com/categories/String/%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"name":"JVM","slug":"JVM","permalink":"http://example.com/categories/JVM/"},{"name":"解释器","slug":"解释器","permalink":"http://example.com/categories/%E8%A7%A3%E9%87%8A%E5%99%A8/"},{"name":"JVM","slug":"解释器/JVM","permalink":"http://example.com/categories/%E8%A7%A3%E9%87%8A%E5%99%A8/JVM/"},{"name":"多线程","slug":"多线程","permalink":"http://example.com/categories/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"ThreadLocal","slug":"多线程/ThreadLocal","permalink":"http://example.com/categories/%E5%A4%9A%E7%BA%BF%E7%A8%8B/ThreadLocal/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/categories/Elasticsearch/"},{"name":"算法","slug":"算法","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/"},{"name":"Kafka","slug":"算法/Kafka","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/Kafka/"},{"name":"I/O多路复用","slug":"I-O多路复用","permalink":"http://example.com/categories/I-O%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/"},{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/categories/Kafka/"},{"name":"Zookeeper","slug":"Kafka/Zookeeper","permalink":"http://example.com/categories/Kafka/Zookeeper/"},{"name":"消息队列","slug":"Kafka/消息队列","permalink":"http://example.com/categories/Kafka/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"Spring Security","slug":"Spring-Security","permalink":"http://example.com/categories/Spring-Security/"},{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"ArrayList","slug":"集合/ArrayList","permalink":"http://example.com/categories/%E9%9B%86%E5%90%88/ArrayList/"},{"name":"Spring ","slug":"Spring/Spring","permalink":"http://example.com/categories/Spring/Spring/"},{"name":"计算机网络","slug":"计算机网络","permalink":"http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"Redis","slug":"算法/Redis","permalink":"http://example.com/categories/%E7%AE%97%E6%B3%95/Redis/"},{"name":"juc","slug":"juc","permalink":"http://example.com/categories/juc/"},{"name":"设计模式","slug":"juc/设计模式","permalink":"http://example.com/categories/juc/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"jv'm","slug":"jv-m","permalink":"http://example.com/categories/jv-m/"},{"name":"jvm","slug":"jv-m/jvm","permalink":"http://example.com/categories/jv-m/jvm/"},{"name":"java","slug":"集合/java","permalink":"http://example.com/categories/%E9%9B%86%E5%90%88/java/"},{"name":"java基础","slug":"java基础","permalink":"http://example.com/categories/java%E5%9F%BA%E7%A1%80/"},{"name":"Java基础","slug":"Java基础","permalink":"http://example.com/categories/Java%E5%9F%BA%E7%A1%80/"},{"name":"java","slug":"java","permalink":"http://example.com/categories/java/"},{"name":"计算机基础","slug":"计算机基础","permalink":"http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"计算机网络","slug":"计算机基础/计算机网络","permalink":"http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"设计模式","slug":"java/设计模式","permalink":"http://example.com/categories/java/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"事务","slug":"事务","permalink":"http://example.com/tags/%E4%BA%8B%E5%8A%A1/"},{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"},{"name":"MySQL","slug":"MySQL","permalink":"http://example.com/tags/MySQL/"},{"name":"ZAB协议","slug":"ZAB协议","permalink":"http://example.com/tags/ZAB%E5%8D%8F%E8%AE%AE/"},{"name":"数据一致性","slug":"数据一致性","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7/"},{"name":"Paxos","slug":"Paxos","permalink":"http://example.com/tags/Paxos/"},{"name":"一致性算法","slug":"一致性算法","permalink":"http://example.com/tags/%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/"},{"name":"Zookeeper","slug":"Zookeeper","permalink":"http://example.com/tags/Zookeeper/"},{"name":"选举机制","slug":"选举机制","permalink":"http://example.com/tags/%E9%80%89%E4%B8%BE%E6%9C%BA%E5%88%B6/"},{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"泛型","slug":"泛型","permalink":"http://example.com/tags/%E6%B3%9B%E5%9E%8B/"},{"name":"lambda表达式","slug":"lambda表达式","permalink":"http://example.com/tags/lambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/"},{"name":"日志","slug":"日志","permalink":"http://example.com/tags/%E6%97%A5%E5%BF%97/"},{"name":"SL4J","slug":"SL4J","permalink":"http://example.com/tags/SL4J/"},{"name":"SPI","slug":"SPI","permalink":"http://example.com/tags/SPI/"},{"name":"查询","slug":"查询","permalink":"http://example.com/tags/%E6%9F%A5%E8%AF%A2/"},{"name":"行列转换","slug":"行列转换","permalink":"http://example.com/tags/%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2/"},{"name":"JMM","slug":"JMM","permalink":"http://example.com/tags/JMM/"},{"name":"并发编程","slug":"并发编程","permalink":"http://example.com/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"重排序","slug":"重排序","permalink":"http://example.com/tags/%E9%87%8D%E6%8E%92%E5%BA%8F/"},{"name":"上下文无关文法","slug":"上下文无关文法","permalink":"http://example.com/tags/%E4%B8%8A%E4%B8%8B%E6%96%87%E6%97%A0%E5%85%B3%E6%96%87%E6%B3%95/"},{"name":"First","slug":"First","permalink":"http://example.com/tags/First/"},{"name":"Follow","slug":"Follow","permalink":"http://example.com/tags/Follow/"},{"name":"LSM","slug":"LSM","permalink":"http://example.com/tags/LSM/"},{"name":"Explain","slug":"Explain","permalink":"http://example.com/tags/Explain/"},{"name":"自增id","slug":"自增id","permalink":"http://example.com/tags/%E8%87%AA%E5%A2%9Eid/"},{"name":"流","slug":"流","permalink":"http://example.com/tags/%E6%B5%81/"},{"name":"next-key lock","slug":"next-key-lock","permalink":"http://example.com/tags/next-key-lock/"},{"name":"读已提交","slug":"读已提交","permalink":"http://example.com/tags/%E8%AF%BB%E5%B7%B2%E6%8F%90%E4%BA%A4/"},{"name":"binlog","slug":"binlog","permalink":"http://example.com/tags/binlog/"},{"name":"自增主键","slug":"自增主键","permalink":"http://example.com/tags/%E8%87%AA%E5%A2%9E%E4%B8%BB%E9%94%AE/"},{"name":"锁","slug":"锁","permalink":"http://example.com/tags/%E9%94%81/"},{"name":"RESP","slug":"RESP","permalink":"http://example.com/tags/RESP/"},{"name":"通信协议","slug":"通信协议","permalink":"http://example.com/tags/%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/"},{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"},{"name":"集合","slug":"集合","permalink":"http://example.com/tags/%E9%9B%86%E5%90%88/"},{"name":"Queue","slug":"Queue","permalink":"http://example.com/tags/Queue/"},{"name":"fail-safe","slug":"fail-safe","permalink":"http://example.com/tags/fail-safe/"},{"name":"其它","slug":"其它","permalink":"http://example.com/tags/%E5%85%B6%E5%AE%83/"},{"name":"SQL优化","slug":"SQL优化","permalink":"http://example.com/tags/SQL%E4%BC%98%E5%8C%96/"},{"name":"change buffer","slug":"change-buffer","permalink":"http://example.com/tags/change-buffer/"},{"name":"索引下推","slug":"索引下推","permalink":"http://example.com/tags/%E7%B4%A2%E5%BC%95%E4%B8%8B%E6%8E%A8/"},{"name":"分页查询","slug":"分页查询","permalink":"http://example.com/tags/%E5%88%86%E9%A1%B5%E6%9F%A5%E8%AF%A2/"},{"name":"GROUP BY优化","slug":"GROUP-BY优化","permalink":"http://example.com/tags/GROUP-BY%E4%BC%98%E5%8C%96/"},{"name":"排序优化","slug":"排序优化","permalink":"http://example.com/tags/%E6%8E%92%E5%BA%8F%E4%BC%98%E5%8C%96/"},{"name":"子查询优化","slug":"子查询优化","permalink":"http://example.com/tags/%E5%AD%90%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96/"},{"name":"关联查询","slug":"关联查询","permalink":"http://example.com/tags/%E5%85%B3%E8%81%94%E6%9F%A5%E8%AF%A2/"},{"name":"索引失效","slug":"索引失效","permalink":"http://example.com/tags/%E7%B4%A2%E5%BC%95%E5%A4%B1%E6%95%88/"},{"name":"索引","slug":"索引","permalink":"http://example.com/tags/%E7%B4%A2%E5%BC%95/"},{"name":"设计原则","slug":"设计原则","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/"},{"name":"原则","slug":"原则","permalink":"http://example.com/tags/%E5%8E%9F%E5%88%99/"},{"name":"新特性","slug":"新特性","permalink":"http://example.com/tags/%E6%96%B0%E7%89%B9%E6%80%A7/"},{"name":"MySQL8.0","slug":"MySQL8-0","permalink":"http://example.com/tags/MySQL8-0/"},{"name":"缓存","slug":"缓存","permalink":"http://example.com/tags/%E7%BC%93%E5%AD%98/"},{"name":"两阶段提交","slug":"两阶段提交","permalink":"http://example.com/tags/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4/"},{"name":"Binlog","slug":"Binlog","permalink":"http://example.com/tags/Binlog/"},{"name":"主从复制","slug":"主从复制","permalink":"http://example.com/tags/%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/"},{"name":"故障","slug":"故障","permalink":"http://example.com/tags/%E6%95%85%E9%9A%9C/"},{"name":"Optional","slug":"Optional","permalink":"http://example.com/tags/Optional/"},{"name":"空指针","slug":"空指针","permalink":"http://example.com/tags/%E7%A9%BA%E6%8C%87%E9%92%88/"},{"name":"String","slug":"String","permalink":"http://example.com/tags/String/"},{"name":"G1","slug":"G1","permalink":"http://example.com/tags/G1/"},{"name":"intern","slug":"intern","permalink":"http://example.com/tags/intern/"},{"name":"JIT","slug":"JIT","permalink":"http://example.com/tags/JIT/"},{"name":"C1","slug":"C1","permalink":"http://example.com/tags/C1/"},{"name":"C2","slug":"C2","permalink":"http://example.com/tags/C2/"},{"name":"热点探测","slug":"热点探测","permalink":"http://example.com/tags/%E7%83%AD%E7%82%B9%E6%8E%A2%E6%B5%8B/"},{"name":"即时编译","slug":"即时编译","permalink":"http://example.com/tags/%E5%8D%B3%E6%97%B6%E7%BC%96%E8%AF%91/"},{"name":"方法计数器","slug":"方法计数器","permalink":"http://example.com/tags/%E6%96%B9%E6%B3%95%E8%AE%A1%E6%95%B0%E5%99%A8/"},{"name":"回边计数器","slug":"回边计数器","permalink":"http://example.com/tags/%E5%9B%9E%E8%BE%B9%E8%AE%A1%E6%95%B0%E5%99%A8/"},{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"},{"name":"解释器分类","slug":"解释器分类","permalink":"http://example.com/tags/%E8%A7%A3%E9%87%8A%E5%99%A8%E5%88%86%E7%B1%BB/"},{"name":"直接内存","slug":"直接内存","permalink":"http://example.com/tags/%E7%9B%B4%E6%8E%A5%E5%86%85%E5%AD%98/"},{"name":"ThreadLocal","slug":"ThreadLocal","permalink":"http://example.com/tags/ThreadLocal/"},{"name":"多线程","slug":"多线程","permalink":"http://example.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"堆","slug":"堆","permalink":"http://example.com/tags/%E5%A0%86/"},{"name":"对象内存分配","slug":"对象内存分配","permalink":"http://example.com/tags/%E5%AF%B9%E8%B1%A1%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/"},{"name":"分片","slug":"分片","permalink":"http://example.com/tags/%E5%88%86%E7%89%87/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://example.com/tags/Elasticsearch/"},{"name":"routing","slug":"routing","permalink":"http://example.com/tags/routing/"},{"name":"基础概念","slug":"基础概念","permalink":"http://example.com/tags/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"},{"name":"算法","slug":"算法","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"时间轮","slug":"时间轮","permalink":"http://example.com/tags/%E6%97%B6%E9%97%B4%E8%BD%AE/"},{"name":"DelayQueue","slug":"DelayQueue","permalink":"http://example.com/tags/DelayQueue/"},{"name":"队列","slug":"队列","permalink":"http://example.com/tags/%E9%98%9F%E5%88%97/"},{"name":"ScheduledThreadPoolExecutor","slug":"ScheduledThreadPoolExecutor","permalink":"http://example.com/tags/ScheduledThreadPoolExecutor/"},{"name":"Timer","slug":"Timer","permalink":"http://example.com/tags/Timer/"},{"name":"JDK","slug":"JDK","permalink":"http://example.com/tags/JDK/"},{"name":"Linux","slug":"Linux","permalink":"http://example.com/tags/Linux/"},{"name":"IO","slug":"IO","permalink":"http://example.com/tags/IO/"},{"name":"I/O多路复用","slug":"I-O多路复用","permalink":"http://example.com/tags/I-O%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/"},{"name":"select/epoll/poll","slug":"select-epoll-poll","permalink":"http://example.com/tags/select-epoll-poll/"},{"name":"Kraft模式","slug":"Kraft模式","permalink":"http://example.com/tags/Kraft%E6%A8%A1%E5%BC%8F/"},{"name":"数据积压","slug":"数据积压","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E7%A7%AF%E5%8E%8B/"},{"name":"消费者","slug":"消费者","permalink":"http://example.com/tags/%E6%B6%88%E8%B4%B9%E8%80%85/"},{"name":"漏消费","slug":"漏消费","permalink":"http://example.com/tags/%E6%BC%8F%E6%B6%88%E8%B4%B9/"},{"name":"重复消费","slug":"重复消费","permalink":"http://example.com/tags/%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9/"},{"name":"offset","slug":"offset","permalink":"http://example.com/tags/offset/"},{"name":"分区分配策略","slug":"分区分配策略","permalink":"http://example.com/tags/%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5/"},{"name":"高效读写","slug":"高效读写","permalink":"http://example.com/tags/%E9%AB%98%E6%95%88%E8%AF%BB%E5%86%99/"},{"name":"清楚策略","slug":"清楚策略","permalink":"http://example.com/tags/%E6%B8%85%E6%A5%9A%E7%AD%96%E7%95%A5/"},{"name":"文件存储","slug":"文件存储","permalink":"http://example.com/tags/%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"},{"name":"Leader","slug":"Leader","permalink":"http://example.com/tags/Leader/"},{"name":"Partition","slug":"Partition","permalink":"http://example.com/tags/Partition/"},{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"},{"name":"Leader和Follower故障","slug":"Leader和Follower故障","permalink":"http://example.com/tags/Leader%E5%92%8CFollower%E6%95%85%E9%9A%9C/"},{"name":"工作流程","slug":"工作流程","permalink":"http://example.com/tags/%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/"},{"name":"数据有序","slug":"数据有序","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E6%9C%89%E5%BA%8F/"},{"name":"数据","slug":"数据","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE/"},{"name":"认证","slug":"认证","permalink":"http://example.com/tags/%E8%AE%A4%E8%AF%81/"},{"name":"AOP","slug":"AOP","permalink":"http://example.com/tags/AOP/"},{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"name":"循环依赖","slug":"循环依赖","permalink":"http://example.com/tags/%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96/"},{"name":"应用","slug":"应用","permalink":"http://example.com/tags/%E5%BA%94%E7%94%A8/"},{"name":"创建者模式","slug":"创建者模式","permalink":"http://example.com/tags/%E5%88%9B%E5%BB%BA%E8%80%85%E6%A8%A1%E5%BC%8F/"},{"name":"建造者模式","slug":"建造者模式","permalink":"http://example.com/tags/%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/"},{"name":"原型模式","slug":"原型模式","permalink":"http://example.com/tags/%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/"},{"name":"工厂模式","slug":"工厂模式","permalink":"http://example.com/tags/%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/"},{"name":"transient","slug":"transient","permalink":"http://example.com/tags/transient/"},{"name":"单例模式","slug":"单例模式","permalink":"http://example.com/tags/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"name":"破坏","slug":"破坏","permalink":"http://example.com/tags/%E7%A0%B4%E5%9D%8F/"},{"name":"UML","slug":"UML","permalink":"http://example.com/tags/UML/"},{"name":"分类","slug":"分类","permalink":"http://example.com/tags/%E5%88%86%E7%B1%BB/"},{"name":"注解","slug":"注解","permalink":"http://example.com/tags/%E6%B3%A8%E8%A7%A3/"},{"name":"IOC","slug":"IOC","permalink":"http://example.com/tags/IOC/"},{"name":"容器","slug":"容器","permalink":"http://example.com/tags/%E5%AE%B9%E5%99%A8/"},{"name":"资源加载","slug":"资源加载","permalink":"http://example.com/tags/%E8%B5%84%E6%BA%90%E5%8A%A0%E8%BD%BD/"},{"name":"bean","slug":"bean","permalink":"http://example.com/tags/bean/"},{"name":"scop","slug":"scop","permalink":"http://example.com/tags/scop/"},{"name":"FactoryBean","slug":"FactoryBean","permalink":"http://example.com/tags/FactoryBean/"},{"name":"scope","slug":"scope","permalink":"http://example.com/tags/scope/"},{"name":"BeanFactory","slug":"BeanFactory","permalink":"http://example.com/tags/BeanFactory/"},{"name":"IoC Service Provider","slug":"IoC-Service-Provider","permalink":"http://example.com/tags/IoC-Service-Provider/"},{"name":"IOC的理解","slug":"IOC的理解","permalink":"http://example.com/tags/IOC%E7%9A%84%E7%90%86%E8%A7%A3/"},{"name":"注入方式","slug":"注入方式","permalink":"http://example.com/tags/%E6%B3%A8%E5%85%A5%E6%96%B9%E5%BC%8F/"},{"name":"行结构","slug":"行结构","permalink":"http://example.com/tags/%E8%A1%8C%E7%BB%93%E6%9E%84/"},{"name":"行","slug":"行","permalink":"http://example.com/tags/%E8%A1%8C/"},{"name":"TCP","slug":"TCP","permalink":"http://example.com/tags/TCP/"},{"name":"最左匹配原则","slug":"最左匹配原则","permalink":"http://example.com/tags/%E6%9C%80%E5%B7%A6%E5%8C%B9%E9%85%8D%E5%8E%9F%E5%88%99/"},{"name":"性能","slug":"性能","permalink":"http://example.com/tags/%E6%80%A7%E8%83%BD/"},{"name":"主键","slug":"主键","permalink":"http://example.com/tags/%E4%B8%BB%E9%94%AE/"},{"name":"http","slug":"http","permalink":"http://example.com/tags/http/"},{"name":"get","slug":"get","permalink":"http://example.com/tags/get/"},{"name":"post","slug":"post","permalink":"http://example.com/tags/post/"},{"name":"代理模式","slug":"代理模式","permalink":"http://example.com/tags/%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/"},{"name":"表","slug":"表","permalink":"http://example.com/tags/%E8%A1%A8/"},{"name":"文件","slug":"文件","permalink":"http://example.com/tags/%E6%96%87%E4%BB%B6/"},{"name":"innoDB","slug":"innoDB","permalink":"http://example.com/tags/innoDB/"},{"name":"框架实现","slug":"框架实现","permalink":"http://example.com/tags/%E6%A1%86%E6%9E%B6%E5%AE%9E%E7%8E%B0/"},{"name":"一致性哈希算法","slug":"一致性哈希算法","permalink":"http://example.com/tags/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95/"},{"name":"主从复制模型","slug":"主从复制模型","permalink":"http://example.com/tags/%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E6%A8%A1%E5%9E%8B/"},{"name":"内存优化","slug":"内存优化","permalink":"http://example.com/tags/%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96/"},{"name":"回收进程","slug":"回收进程","permalink":"http://example.com/tags/%E5%9B%9E%E6%94%B6%E8%BF%9B%E7%A8%8B/"},{"name":"异步队列","slug":"异步队列","permalink":"http://example.com/tags/%E5%BC%82%E6%AD%A5%E9%98%9F%E5%88%97/"},{"name":"分布式锁","slug":"分布式锁","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/"},{"name":"热点数据","slug":"热点数据","permalink":"http://example.com/tags/%E7%83%AD%E7%82%B9%E6%95%B0%E6%8D%AE/"},{"name":"共享","slug":"共享","permalink":"http://example.com/tags/%E5%85%B1%E4%BA%AB/"},{"name":"内存回收","slug":"内存回收","permalink":"http://example.com/tags/%E5%86%85%E5%AD%98%E5%9B%9E%E6%94%B6/"},{"name":"对象","slug":"对象","permalink":"http://example.com/tags/%E5%AF%B9%E8%B1%A1/"},{"name":"整数集合","slug":"整数集合","permalink":"http://example.com/tags/%E6%95%B4%E6%95%B0%E9%9B%86%E5%90%88/"},{"name":"跳表","slug":"跳表","permalink":"http://example.com/tags/%E8%B7%B3%E8%A1%A8/"},{"name":"哈希表","slug":"哈希表","permalink":"http://example.com/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"name":"链表","slug":"链表","permalink":"http://example.com/tags/%E9%93%BE%E8%A1%A8/"},{"name":"字符串","slug":"字符串","permalink":"http://example.com/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"name":"synchronized","slug":"synchronized","permalink":"http://example.com/tags/synchronized/"},{"name":"终止模式","slug":"终止模式","permalink":"http://example.com/tags/%E7%BB%88%E6%AD%A2%E6%A8%A1%E5%BC%8F/"},{"name":"创建线程","slug":"创建线程","permalink":"http://example.com/tags/%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8B/"},{"name":"类初始化：主动使用与被动使用","slug":"类初始化：主动使用与被动使用","permalink":"http://example.com/tags/%E7%B1%BB%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%9A%E4%B8%BB%E5%8A%A8%E4%BD%BF%E7%94%A8%E4%B8%8E%E8%A2%AB%E5%8A%A8%E4%BD%BF%E7%94%A8/"},{"name":"类的加载过程","slug":"类的加载过程","permalink":"http://example.com/tags/%E7%B1%BB%E7%9A%84%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B/"},{"name":"ArrayList","slug":"ArrayList","permalink":"http://example.com/tags/ArrayList/"},{"name":"HashMap","slug":"HashMap","permalink":"http://example.com/tags/HashMap/"},{"name":"List","slug":"List","permalink":"http://example.com/tags/List/"},{"name":"接口","slug":"接口","permalink":"http://example.com/tags/%E6%8E%A5%E5%8F%A3/"},{"name":"重写","slug":"重写","permalink":"http://example.com/tags/%E9%87%8D%E5%86%99/"},{"name":"重载","slug":"重载","permalink":"http://example.com/tags/%E9%87%8D%E8%BD%BD/"},{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"},{"name":"final、finally、fi'nalize","slug":"final、finally、fi-nalize","permalink":"http://example.com/tags/final%E3%80%81finally%E3%80%81fi-nalize/"},{"name":"Integer","slug":"Integer","permalink":"http://example.com/tags/Integer/"},{"name":"快速选择","slug":"快速选择","permalink":"http://example.com/tags/%E5%BF%AB%E9%80%9F%E9%80%89%E6%8B%A9/"},{"name":"Top","slug":"Top","permalink":"http://example.com/tags/Top/"},{"name":"快速排序","slug":"快速排序","permalink":"http://example.com/tags/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/"},{"name":"计算机基础","slug":"计算机基础","permalink":"http://example.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"计算机网络","slug":"计算机网络","permalink":"http://example.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]}